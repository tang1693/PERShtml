<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2026/00000092/00000001/art00007;jsessionid=20in16ui51h6u.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            RSIDetNet: An Efficient Oriented Small Object Detection Model for Remote Sensing Images Based on Cross-Scale Feature Fusion and Large Kernel Decomposition
        </a>
    </h3>
    <div style="font-style: italic;">January 2026, pp. 23-34(12)</div>
    <div>Authors: Kang, Zizhuang; Han, Yihui; He, Bing; Jia, Mingquan; Luo, Wen; Fu, Ying; He, Wei</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Small object detection in remote sensing images is crucial for maximizing data utility, but small objects face challenges due to their limited pixel coverage, low resolution, and susceptibility to background noise. This paper proposes an orientated small object detection model for remote sensing images based on cross-scale feature fusion and large kernel decomposition. The model consists of four main components: the image feature extraction module, the multi-scale feature fusion module, the cross-fusion region proposal network for generating candidate regions, and the dual detection head for predicting target categories and rotating bounding boxes. Experiments are conducted on two datasets, SODA‐A and HRSC‐2016, and the results show that the proposed model improves the mean average precision (mAP) by at least 6.3% over classical 1‐stage models and by at least 2.6% over classical 2‐stage model. In particular, when detecting very small objects (area less than 144 pixels), the mAP value is as high as 17.2%, which is a significant improvement compared with other models, indicating that it is very effective in dealing with the difficult task of small object detection.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2026/00000092/00000001/art00008;jsessionid=20in16ui51h6u.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Comparison of Intertidal Terrain Extraction Methods Based on ICESat‐2 and Tidal Data
        </a>
    </h3>
    <div style="font-style: italic;">January 2026, pp. 35-47(13)</div>
    <div>Authors: Chen, Deliang; Lu, Zixuan; ZhuangJianbo Xiao, Qizhi; Xiao, Jianbo; Chen, Song; Cheng, Liang</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The intertidal zone is a transitional area between land and sea, characterized by both marine and terrestrial features, with rich resources in mudflats. Accurately mapping the intertidal topography and understanding its dynamic characteristics are of great significance. In this study, Sentinel-2 imagery was used, combined with tidal data and ICESat‐2 data, respectively. Four methods–the waterline method, the inundation frequency method, random forest regression, and the long short-term memory (LSTM) model–were applied to extract intertidal topography in the large radial sand ridges along the Jiangsu coast. When validated with ICESat‐2 data and unmanned aerial vehicle (UAV) data, the root mean square error (RMSE) values of all four methods combined with ICESat‐2 data were lower than those combined with tidal data. Using ICESat‐2 data for validation, the waterline method combined with ICESat‐2 data achieved the lowest RMSE of 0.218 m. When validated with UAV data, the inundation frequency method combined with ICESat‐2 data yielded the lowest RMSE of 0.864 m. From 2020 to 2024, the intertidal zone in this region was initially dominated by erosion, followed by deposition, ultimately reaching a dynamic equilibrium. This study achieved two objectives: (1) under identical area conditions using the same image data and two elevation data, four different methods were validated to compare their topographic extraction performance and identify the optimal approach; and (2) the optimal method was applied to generate multi-temporal topographic results of a local intertidal zone along the Jiangsu coast, analyzing terrain change in the region.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2026/00000092/00000001/art00010;jsessionid=20in16ui51h6u.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Optimized 3D Building Mapping and Reconstruction via Cross-View Collaboration in Densely Built-Up Areas
        </a>
    </h3>
    <div style="font-style: italic;">January 2026, pp. 49-63(15)</div>
    <div>Authors: Tang, Shengjun; Chen, Yujie; Yu, Tian; Li, You; Xie, Linfu; Wang, Weixi; Guo, Renzhong</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            High-precision urban three-dimensional (3D) point clouds can be effectively generated using airborne oblique photogrammetry and lidar scanning. However, due to occlusions from dense building layouts and vegetation, existing airborne acquisition methods often struggle to capture complete building geometries. This incompleteness poses serious challenges for applications such as urban planning, smart city management, and autonomous navigation, which require structurally complete and accurate 3D data. To address this limitation, this paper proposes a novel cross-view collaborative surveying framework that integrates aerial and ground-based data collection for precise and efficient 3D reconstruction of urban buildings. The framework begins by performing automated building completeness detection on aerial point clouds using a multi-layer slice projection algorithm, which enables accurate identification of missing regions in both point-wise and surface-wise forms. These detected deficiencies are then used to guide the generation of optimized ground-based supplementary acquisition routes, incorporating a global-local planning mechanism and a multi-objective technologies for autonomous robot exploration (TARE) strategy to enable autonomous and adaptive data collection. Comprehensive experiments were conducted in both simulated and real-world urban environments. Evaluation metrics focused on point cloud completeness and 3D reconstruction accuracy. The results demonstrate that the proposed method significantly enhances the completeness of building point clouds, achieving an average detection accuracy above 90%, while also reducing reconstruction error by up to 65% in complex urban scenarios. The proposed method provides a valuable tool for urban mapping professionals, autonomous systems, and digital city infrastructure developers who depend on high-quality 3D building models.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000012/art00015;jsessionid=2onsdfraphhf3.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Evaluating Three-Dimensional Elevation Program Lidar Consistency and Accuracy at Scale Using Cloud-Native, Open-Source Methods
        </a>
    </h3>
    <div style="font-style: italic;">December 2025, pp. 777-785(9)</div>
    <div>Authors: Sampath, Aparajithan; Irwin, Jeffrey R.; Stoker, Jason M.</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The U.S. Geological Survey three-dimensional elevation program (3DEP) has significantly expanded national lidar coverage, necessitating scalable, reproducible methods for assessing data quality across diverse terrains and acquisition conditions. This study introduces a cloud-native, open-source workflow designed to evaluate the geometric accuracy and consistency of 3DEP lidar data sets at a national scale. Leveraging tools such as the Point Data Abstraction Library, Open3D, and Amazon Web Services infrastructure, the workflow integrates global navigation satellite system‐surveyed ground control points and terrestrial laser scanning data to validate airborne lidar collections. Two case studies demonstrate the application of this process. In Puerto Rico, the process identified vertical biases and inconsistencies in vegetated areas, while in Iowa and Arizona, the process confirmed high vertical accuracy with minimal bias. The results underscore the effectiveness of combining cloud computing with open-source tools to perform large-scale lidar data quality assessments. This process offers a reproducible, efficient solution for nationwide validation of 3DEP data sets, supporting enhanced decision-making in geospatial applications.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000012/art00019;jsessionid=2onsdfraphhf3.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Assessing the Accuracy of a 2021 Kansas‐Oklahoma Tallgrass Ecosystem Burn Area Map
        </a>
    </h3>
    <div style="font-style: italic;">December 2025, pp. 799-807(9)</div>
    <div>Authors: Iiames, John; Congalton, Russell G.; Avey, Lance; Miller, Steven; Ebert, Don; Grier, Gina</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Systematic spring season burning of the tallgrass prairie in eastern Kansas and northeastern Oklahoma has been used by ranchers and ecologists for over 150 years to encourage forage production and reduce invasive species propagation and woody species encroachment in the Kansas Flint Hills and Oklahoma Osage Tallgrass ecoregion. On average, 8100 km2is burned yearly during the March to April burn window. Despite the beneficial effects resulting from spring burning, air resource managers must also account for the particulates emitted from the combustion of these fuels. The Kansas Department of Health and Environment (KDHE) has mapped spring burn area over the Flint Hills (Kansas) and Osage (Oklahoma) regions since 2011 by processing satellite data from the National Aeronautics and Space Administration (NASA) Moderate Resolution Imaging Spectroradiometer (MODIS) sensor onboard both the Aqua and Terra satellite platforms. Understanding the accuracy of these maps is useful for understanding PM2.5 impacts to the local communities and the surrounding states. We conducted both a sample unit–based and an area‐based assessment of the 4 April 2021 burn area map to determine map accuracy and identified potential sources of error, both “commission” (i.e., mapped a burn area where there was no burn) and “omission” (i.e., did not map a burn area where a burn actually occurred). The results indicate that the KDHE 2021 map achieved high overall accuracies (>94%), yet these accuracies were not consistent over the entire area. Omission and commission errors were three to four times larger in areas that had higher burn area percentages, partially due to increased burn edge, promoting larger numbers of mixed 250-m cells. Another source of error was from the prior year (2020) fall burn scars that were occasionally identified as spring burns over the 2021 burn season. The area-based assessment method provided additional information from the sample unit–based method and was useful in identifying anomalous assessment results dependent on the geographical location within the tallgrass system.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000011/art00008;jsessionid=2a4rbjrb9p6t.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Efficient Irregular Texture Nesting Method via Hybrid NFP-SADE with Adaptive Container Resizing
        </a>
    </h3>
    <div style="font-style: italic;">November 2025, pp. 681-691(11)</div>
    <div>Authors: Lou, Liyuan; Li, Wanyun; Yu, Jingle; Wang, Xin; Zhan, Zongqian</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Efficient irregular texture nesting, which is necessary for improving the efficiency of texture mapping and 3D model rendering, especially for large-scale 3D reconstruction tasks, has emerged as a critical research topic in the fields of photogrammetry, computer graphics, and computer vision. However, persistent inefficiencies and high computational costs in existing texture nesting algorithms pose significant challenges when dealing with vast quantities of irregularly shaped texture patches. To solve this problem, this work presents an efficient and well structured texture nesting for reorganizing irregular textures in a space efficient and time efficient way. More specifically, a hybrid optimization approach that integrates an enhanced no fit polygon (NFP) method with an improved simplified atavistic differential evolution (SADE) algorithm is proposed. The canonical SADE is reformulated, tailored for texture nesting optimization, and a novel self-adaptive container resizing strategy is used to surpass traditional NFP approaches in polygon processing efficiency. The experimental results demonstrate that the proposed method significantly improves irregular texture nesting efficiency, achieving speed improvements of up to 5.44 times compared with the common genetic algorithm–based method and 5.21 times over the simulated annealing–based method. Furthermore, it consistently improves space use by approximately 6.56%, indicating a more effective layout strategy and optimized resource use. Code is available at https:// github. com/louliyuan/NFP-SADE-With-Adaptive-Container-Resizing.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000011/art00012;jsessionid=2a4rbjrb9p6t.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Landsat-Derived Rainfed and Irrigated-Area Product for Conterminous United States for the Year 2020 (LRIP30 CONUS 2020) Using Supervised and Unsupervised Machine Learning on the Cloud
        </a>
    </h3>
    <div style="font-style: italic;">November 2025, pp. 703-714(12)</div>
    <div>Authors: Teluguntla, Pardhasaradhi; Thenkabail, Prasad S.; Oliphant, Adam; Aneece, Itiya; Biggs, Trent; Gumma, Murali Krishna; Foley, Daniel; McCormick, Richard; Neelam, Rohitha; Long, Emerson; Lawton, Jake</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Accurate maps of irrigated and rainfed croplands are crucial for assessing global food and water security. Irrigated croplands yield two to four times more grain and biomass than rainfed croplands. To meet rising food demand, the proportion of cropland that is irrigated must be increased globally. Because agriculture uses 80% to 90% of global fresh water, understanding changes in cropland extent, crop type, and irrigation is critical for meeting nutritional needs sustainably. The United States has one of the most productive rainfed and irrigated croplands in the world and is a leading producer and exporter of agricultural crops. Precise maps of irrigated and rainfed croplands in the United States are crucial for assessing the current and the future agricultural production capacity in supporting food security. We developed a 30-m resolution rainfed and irrigated area map for the conterminous United States derived from 2019 to 2021 multi-date Landsat-8 data (LRIP30 CONUS 2020). A total of 96 harmonized spectral bands comprising monthly median value composites of eight bands (blue, green, red, NIR, SWIR1, SWIR2, TIR, and enhanced vegetation index [EVI]) were used. A cropland mask was then applied, and reference data were sourced from various sources. A pixel based supervised random forest classifier, and pixel based unsupervised ISODATA clustering classifier were implemented on Google Earth Engine and the ERDAS Imagine workstation to classify, identify, map, and assess accuracies of irrigated and rainfed cropland areas. The LRIP30 CONUS 2020 product achieved an overall accuracy of 93.9%. The irrigated and rainfed classes had producer's accuracies of 90.2% and 95.7%, respectively, and user's accuracies of 90.8% and 95.4%, respectively. The total net cropland area was estimated at 139.4 million hectares (Mha), of which 94.9 Mha (68%) was classified as rainfed and 44.5 Mha (32%) was classified as irrigated. State level summaries highlight regional differences and their implications for national and global food and water security.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000010/art00010;jsessionid=11u1ggiec04i4.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            MRTD-Based Effective Range Analysis of Airborne Infrared Imaging Systems for Ship Detection: Optimization of the Calculation for Operational Range Through an Improved MRTD Model
        </a>
    </h3>
    <div style="font-style: italic;">October 2025, pp. 623-630(8)</div>
    <div>Authors: Yan, Peng; Tian, Yuyang; Ling, Xiao; Zhu, Kaikai; Sheng, Qinghong; Wang, Bo; Li, Jun; Liu, Xiang; Xu, Xiao</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            When the airborne infrared imaging system detects ships, significant variations in environmental temperature are often observed. In the existing calculation models for the operating range, the factor of environmental temperature has not been taken into account. However, when the environmental temperature changes, it will affect the variation of the minimum resolvable temperature difference (MRTD) of the system, resulting in a relatively large deviation in the prediction of the operating range of the airborne infrared imaging system. To address this crucial technical challenge, this study systematically established the relationship formula of the MRTD under different temperatures. By integrating with the improved theoretical model of MRTD, a calculation method for the operating range that takes environmental temperature into consideration was developed to accurately determine the operating range of the airborne infrared imaging system. Comparative experimental studies focusing on ships show that, compared with traditional methods, the prediction deviation of the proposed method is significantly reduced, with an average reduction of 10.1%.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000010/art00011;jsessionid=11u1ggiec04i4.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Convolutional Neural Networks for Land Use and Land Cover Multi-class Maps from Historical Aerial Photographs
        </a>
    </h3>
    <div style="font-style: italic;">October 2025, pp. 631-645(15)</div>
    <div>Authors: Kostrzewa, Adam</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Historical maps that describe past land use and land cover (LULC) forms can be a precious source of information in many scientific fields studying long-term spatial and temporal changes in the landscape. Such repositories were created manually in small areas in the past, which was a time-consuming and labor-intensive task. Recently, there has been a growing tendency to use machine learning models for this purpose, along with deep learning methods. However, having a massive amount of labeled data is necessary for these methods to train the networks. Training data are often manually labeled, posing a significant challenge and limiting the automation of these methods. This article presents a method that uses topographic databases to extract complex multi-class maps representing LULC from historical aerial photographs, eliminating the time-consuming data labeling step. The method uses transfer learning with a pretrained model on 2020 and 2014 data and attempts to reconstruct LULC types with the same convolutional neural network (CNN) network on archived images from 2006. The experiment covered 488 km2and included seven LULC classes. The method was tested using different CNN architectures (U-Net, Pyramid Scene Parsing Network [PSPNet], and LinkNet) with backbones (ResNeXt+SE, EfficientNet, and Inception). The PSPNet‐EfficientNet‐b7 network model achieved the best results, with 90% overall accuracy for predicting LULC classes based on the 2006 archived aerial images.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000010/art00013;jsessionid=11u1ggiec04i4.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Thirty Years of the U.S. National Land Cover Database: Impacts and Future Direction
        </a>
    </h3>
    <div style="font-style: italic;">October 2025, pp. 647-659(13)</div>
    <div>Authors: Sohl, Terry; Jin, Suming; Dewitz, Jon; Wickham, James; Brown, Jesslyn; Stehman, Stephen; Herold, Nathaniel; Schleeweis, Karen; Tollerud, Heather; Deering, Carol</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The National Land Cover Database (NLCD), developed through the Multi-Resolution Land Characteristics Consortium, was initiated 30 years ago and has continually provided critical, Landsat-based landcover and land-change information for the United States. Originally launched to address the lack of national-scale, moderate-resolution land-cover data, NLCD has evolved from the pioneering 1992 dataset into a comprehensive, annually updated product suite. Key innovations include the introduction of impervious surface mapping, forest canopy mapping, standardized Landsat mosaics, national-scale accuracy assessments, continual evolution of deep learning and artificial intelligence methodologies, and a transition toward operational, change-focused monitoring. The NLCD has become an essential resource for scientific research, land management, and policy development, with extensive adoption across federal, state, and local agencies; academia; and the private sector. The NLCD data underpin a wide array of applications, including biodiversity conservation, urban planning, hydrology, human health studies, and natural hazard assessment. As new global and high-resolution commercial land-cover products emerge, the NLCD continues to distinguish itself through its temporal depth, federal backing, and thematic consistency. Moving forward, the NLCD will maintain its niche as the leading, moderate-resolution, long-term land-cover and land-change dataset for the United States, ensuring continued support for broad national applications while complementing higher-resolution and global-mapping efforts.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000008/art00013;jsessionid=uw86g81w98nl.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Efficient Coral Survey Using Aerial Remote Sensing and Multi-modal Segmentation for Large-Scale Ecological Assessment
        </a>
    </h3>
    <div style="font-style: italic;">August 2025, pp. 509-516(8)</div>
    <div>Authors: Qin, Jiangying; Li, Ming; Armin, Gruen; Gong, Jianya; Zhong, Jiageng; Liao, Xuan</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Due to the ecological pressures of global warming and human activities in coastal regions, coral reef ecosystems, predominantly located in shallow marine areas, are facing severe threats to their survival. Scientists and governmental managers are eager to leverage novel aerial remote sensing technologies to address the challenges of acquiring accurate, comprehensive, and timely data on coral reef health, structural complexity, and spatial distribution. This study aims to tackle these challenges by using precise and accurate aerial remote sensing data to support the restoration and sustainable prosperity of coral reef systems. Specifically, this study develops and applies an efficient coral survey method based on aerial remote sensing. The method integrates aerial imagery and bathymetric lidar (light detection and ranging) point cloud data and uses advanced photogrammetric computer vision and deep learning algorithms. Using a state-of-the-art multi-modal neural network segmentation technique, the proposed method enables high-precision and intelligent identification of coral reefs, facilitating detailed habitat mapping. Furthermore, by accurately delineating the habitat range and geometric structures of reefs, this approach allows for precise measurements of coral biomass production and skeletal calcification. These metrics help assess coral reef structural complexity and their adaptability to environmental stressors, providing robust scientific data for conservation strategies and policy making. The use of advanced multi-modal aerial remote sensing data not only enhances monitoring reliability and accuracy but also offers a cost-effective and flexible tool for coral reef ecological mapping. This approach effectively addresses challenges encountered in coastal ecological surveys, particularly in areas where direct human access or boat entry is difficult.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000008/art00014;jsessionid=uw86g81w98nl.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            On the Transferability of Semantic Segmentation for Very-High-Resolution Remote Sensing Data of Multi-City Environments
        </a>
    </h3>
    <div style="font-style: italic;">August 2025, pp. 517-528(12)</div>
    <div>Authors: Qin, Rongjun; Zhang, Guixiang; Tang, Yang</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Semantic segmentation of very-high-resolution remote sensing (RS) data is foundational for many RS applications in urban environments. Images from sources like Worldview-3/4 and Pleiades-Neo offer resolutions as high as 0.3 m, enabling a fine-grained understanding of urban structures. Recent deep learning‐based methods have significantly outperformed traditional approaches in RS semantic segmentation/classification tasks. However, they require large training data sets and often lack transferability due to highly disparate RS image content across different geographical regions. However, no comprehensive analysis exists on their transferability—i.e., to what extent a model trained on a source domain can be applied to a target domain in urban areas. This paper investigates the raw transferability of traditional and deep-learning models and the effectiveness of domain adaptation approaches in enhancing deep-learning model transferability (adapted transferability). Using five highly diverse RS data sets from different cities (6792 patches of 1024 × 1024 pixels each), we trained six models with and without three domain adaptation approaches to quantitatively analyze transferability between data sets. To facilitate easy assessment of model transferability, we developed a simple method to quantify transferability using spectral indices as a medium, demonstrating its effectiveness in evaluating model transferability at the target domain when labels are unavailable. Our experiments yield several important but underreported observations on raw and adapted transferability. Moreover, our proposed label-free transferability assessment method outperforms posterior model confidence and can guide model selection for urban studies globally. The models and datasets are publicly available on GitHub at: https://github.com/GDAOSU/Transferability-Remote-Sensing.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000007/art00010;jsessionid=uk38v2ur8f6d.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Combined Use of Satellite Observations and the RIM for Assessing Recovery from Natural Disasters
        </a>
    </h3>
    <div style="font-style: italic;">July 2025, pp. 407-417(11)</div>
    <div>Authors: Cao, Changyong; Wang, Wenhui; Bai, Yan; Shao, Xi; Uprety, Sirish; Qiu, Hong-Lie</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            In this study, we explore using satellite observations to assess community recovery from natural disasters such as fires and hurricanes, supplementing the Resilience Inference Model (RIM). The RIM model has been successfully used to quantify recoveries from hurricanes along the Gulf Coast, but it relies on long-term population changes over years or decades. Our approach integrates satellite observations to enhance recovery assessment with a shorter latency of weeks or months. Using fire, vegetation, and night light data from the Visible Infrared Imaging Radiometer Suite (VIIRS) with daily global observations, Sentinel-2, Landsat-8, and Geostationary Operational Environmental Satellite/Advanced Baseline Imager, we quantitatively evaluate fire intensity, light outage, and urban greenness changes, along with subsequent recovery, focusing on the 2023 Maui fire and selected hurricane cases along the Gulf Coast. This approach complements the RIM model by introducing quantifiable physical parameters with shorter latency, particularly beneficial in areas where census data are either unavailable or unreliable.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000007/art00017;jsessionid=uk38v2ur8f6d.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatiotemporal Continuous Shallow Water Bathymetry from a Kriged Kalman Filter
        </a>
    </h3>
    <div style="font-style: italic;">July 2025, pp. 463-471(9)</div>
    <div>Authors: Wang, Lei; Liu, Hongxing; Kang, Lei; Su, Haibin; Shu, Song; Wang, Jun</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            In GIScience, problems of missing data in space or time are nontrivial. We implemented a Kriged Kalman filter (KKF)‐based data interpolation and assimilation technique and tested it for mapping bathymetry at unsampled locations and times. This technique integrates the Kriging and Kalman filter computation frameworks to perform spatiotemporal data assimilation, which can produce spatially and temporally continuous bathymetric fields from samples that are scarce in space and time. The spatiotemporal bathymetric field over the estuary of the Yangtze River was mapped based on the four boat-based depth echo-sounding surveys conducted in 1982, 1997, 2002, and 2010. Our validation and verification analyses showed that the KKF assimilation model can predict bathymetry accurately and reliably at unsampled locations and times. This paper demonstrates that KKF is superior to traditional spatial interpolation methods because it informs the interpolator with the temporal component that also extends the prediction to the time domain. The experiments indicate that greater time intervals in conducting bathymetric surveys result in a more pronounced influence on the performance of KKF than the spatial sparsity of depth samples. The ability of space-time prediction of bathymetry allows underwater depth measurements to be accurately aligned with satellite images, which is essential for improving multispectral image inversion in bathymetry studies.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000006/art00014;jsessionid=1xsloaxr9urwi.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A High-Quality Underwater 3D Reconstruction Solution for Coral Reef Environments Leveraging Advanced Photogrammetric Computer Vision Techniques
        </a>
    </h3>
    <div style="font-style: italic;">June 2025, pp. 361-370(10)</div>
    <div>Authors: Zhong, Jiageng; Li, Ming; Gruen, Armin; Liao, Xuan; Qin, Jiangying; Wang, Bing</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Coral reefs, with their intricate 3D structures, are vital marine ecosystems increasingly threatened by environmental stressors. Accurate 3D reconstruction of these underwater structures is essential for scientific research and conservation management. While underwater photogrammetry has emerged as a promising tool for this purpose, technical challenges persist in capturing fine-scale features under underwater conditions, particularly the intricate morphology of coral reefs and the highly complex textures that hinder high-fidelity reconstruction. Recent breakthroughs in computer vision and deep learning have introduced new opportunities for underwater photogrammetry. This study begins by outlining a photogrammetric workflow that can integrate current advanced technologies, followed by summarizing cutting-edge methods used in the key stages, i. e., sparse and dense reconstruction. Building on previous research, we propose a hierarchical reconstruction strategy for accurate and efficient dense modeling. Our approach first performs an efficient global coarse-grained reconstruction to capture the overall scene structure, followed by fine-scale modeling in key regions of interest. Using image data collected from a coral reef site at Moorea Island, we compare and evaluate various techniques, analyzing their respective strengths and limitations. In sparse reconstruction, the classical feature method scale-invariant feature transform demonstrates competitive performance. Deep learning–based methods, such as ALIKED feature and the SuperGlue matching network, achieve superior results on certain metrics. For dense reconstruction, Neural Radiance Fields and 3D Gaussian Splatting–based methods yield high-quality reconstructions but are computationally intensive. In contrast, the deep learning–based multi-view stereo approach achieves comparable reconstruction quality with greater efficiency. Experimental results on reconstruction result fusion further validate that our approach offers a scalable and practical solution for coral reef monitoring, advancing conservation science and ecosystem management practices.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000005/art00013;jsessionid=2hhkmhekdaqu.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Improving Crop Classification by Integrating Phenology Using Random Forests: Assessing the Role of Feature Selection
        </a>
    </h3>
    <div style="font-style: italic;">May 2025, pp. 307-318(12)</div>
    <div>Authors: Jiangwei, Hu; Jingye, Shi; Jiqiang, Liu; Guolong, Guo; Jiadong, Jin</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The increase of satellite images with high spatial resolution, short revisit period, and wide spatial coverage has brought an enormous amount of data; however, limited efforts have been made in feature selection for crop classification. Furthermore, different crop types have unique spectral, spatial, and phenological characteristics that have not been well understood and fully used for the expected results. This study established a crop-classification framework using the random forest model by integrating four types of features (i. e., spectral reflectance features, vegetation index features, spatial texture features, and crop phenological features) over 15 scenarios generated from Sentinel-2 images. The random forest model performed best (overall accuracy = 92.86%; Kappa coefficient = 0.8995) by integrating the four types of features. We systematically assessed the contribution of feature combinations on individual crop classification. Specifically, different feature combinations can effectively improve the recognition accuracy of different crop types. Our findings can provide great potential in choosing optimal features for crop classifications and benefit the application of machine learning in remote sensing–based crop mapping.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000004/art00009;jsessionid=19m2fwh144ij7.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Sat2building: Lod-2 Building Reconstruction from Satellite Imagery Using Spatial Embeddings
        </a>
    </h3>
    <div style="font-style: italic;">April 2025, pp. 203-212(10)</div>
    <div>Authors: Schuegraf, Philipp; Gui, Shengxi; Qin, Rongjun; Fraundorfer, Friedrich; Bittner, Ksenia</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The reconstruction of buildings in level of detail–2, according to the CityGML standard, is an essential feature in applications such as urban planning, environmental simulations, and virtual reality. Existing methods work primarily only on aerial data, depend on an external digital terrain model, or do not accurately separate individual buildings. In this work, we present SAT2BUILDING, a method that predicts roof planes, building sections, and building heights in a single, fully convolutional neural network. The network relies on only orthorectified panchromatic imagery and a photogrammetric digital surface model. The three outputs are jointly processed in a level of detail–2 reconstruction pipeline that generates building models that are seamlessly connected, geometrically accurate and complete, and topologically correct. We use spatial embeddings that enable accurate segmentation of building sections and roof planes from satellite imagery. The model generalizes to data from Bonn, Germany, and Lyon, France, after being trained on data from Berlin, Germany. The training and test data differ in lighting conditions, architectural styles, and ground sampling distances. Thorough comparative evaluation shows the superiority of SAT2BUILDING over three baseline methods.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000003/art00011;jsessionid=7tic6vbu3nnb.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Comparative Study of Deep Learning Methods for Automated Road Network Extraction from High-Spatial-Resolution Remotely Sensed Imagery
        </a>
    </h3>
    <div style="font-style: italic;">March 2025, pp. 163-174(12)</div>
    <div>Authors: Zhou, Haochen; He, Hongjie; Xu, Linlin; Ma, Lingfei; Zhang, Dedong; Chen, Nan; Chapman, Michael A.; Li, Jonathan</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Road network data are crucial for various applications, such as road network planning, traffic control, map navigation, autonomous driving, and smart city construction. Automated road network extraction from high-spatial-resolution remotely sensed imagery has shown promise in road network data construction. In recent years, the advent of deep learning algorithms has pushed road network extraction towards auto - mation, achieving very high accuracy. However, the latest deep learning models are often less applied in the field of road network extraction and lack comparative experiments for guidance. Therefore, this research selected three recent deep learning algorithms, including dense prediction transformer (DPT), SegFormer, SEgmentation TRansformer (SETR), and the classic model fully convolutional network-8s (FCN-8s) for a comparative study. Additionally, this research paper compares three different decoder structures within the SETR model (SETR_naive, SETR_mla, SETR_pup) to investigate the effect of different decoders on the road network extraction task. The experiment is conducted on three commonly used datasets: the DeepGlobe Dataset, the Massachusetts Dataset, and Road Datasets in Complex Mountain Environments (RDCME). The DPT model outperforms other models on the Massachusetts dataset with superior reliability, achieving a high accuracy of 96.31% and excelling with a precision of 81.78% and recall of 32.50%, leading to an F1 score of 46.51%. While SegFormer has a slightly higher F1 score, DPT's precision is particularly valuable for minimizing false positives, making it the most balanced and reliable choice. Similarly, for the DeepGlobe Dataset, DPT achieves an accuracy of 96.76%, precision of 66.12%, recall of 41.37%, and F1 score of 50.89%, and for RDCME, DPT achieves an accuracy of 98.94%, precision of 99.07%, recall of 99.84%, and F1 score of 99.46%, confirming its consistent performance across datasets. This paper provides valuable guidance for future studies on road network extraction techniques using deep learning algorithms.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000002/art00010;jsessionid=7bb4imua1a6ot.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Artificial Neural Network Multi-layer Perceptron Models to Classify California's Crops using Harmonized Landsat Sentinel (HLS) Data
        </a>
    </h3>
    <div style="font-style: italic;">February 2025, pp. 91-100(10)</div>
    <div>Authors: McCormick, Richard; Thenkabail, Prasad S.; Aneece, Itiya; Teluguntla, Pardhasaradhi; Oliphant, Adam J.; Foley, Daniel</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Advances in remote sensing and machine learning are enhancing cropland classification, vital for global food and water security. We used multispectral Harmonized Landsat 8 Sentinel-2 (HLS) 30-m data in an artificial neural network (ANN) multi-layer perceptron (MLP) model to classify five crop classes (cotton, alfalfa, tree crops, grapes, and others) in California's Central Valley. The ANN MLP model, trained on 2021 data from the United States Department of Agriculture's Cropland Data Layer, was validated by classifying crops for an independent year, 2022. Across the five crop classes, the overall accuracy was 74%. Producer's and user's accuracies ranged from 65% to 87%, with cotton achieving the highest accuracies. The study highlights the potential of using deep learning with HLS time series data for accurate global crop classification.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000012/art00010;jsessionid=1gwjqzhhk8piu.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Monitoring LULC Changes in Babil Province for Sustainable Development Purposes Within the Period 2004–2023
        </a>
    </h3>
    <div style="font-style: italic;">December 2024, pp. 745-753(9)</div>
    <div>Authors: Jassoom, Hayder Hameed; Abdoon, Rabab Saadoon</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Land use and land cover studies are fundamental to achieving sustainable development goals by providing the information necessary for informed decision-making in social and economic development and natural resource management. This study relied on remote sensing data to analyze and assess land use and land cover changes in Babil Province, Iraq, over the past two decades. The study focused on identifying the patterns and factors influencing these changes, using Landsat satellite imagery to create digital maps classifying land into four main categories: urban lands, bare soil lands, water bodies, and vegetation lands. The results showed a noticeable expansion of urban lands at the expense of bare soil lands, primarily attributed to population growth, economic development, and improved security conditions. This study underscores the importance of sustainable land management and urban planning in Babil Province. These results highlight the importance of sustainable land management in Babil Province, including sound urban planning considering urban expansion's environmental, social, and economic effects. The classification was implemented using a maximum likelihood classifier, and the accuracy assessment yielded satisfactory results with an overall accuracy of 93.5517%. This study encourages using artificial intelligence to track and analyze land use changes in Babil.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000012/art00012;jsessionid=1gwjqzhhk8piu.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Image Fusion in Remote Sensing: An Overview and Meta-Analysis
        </a>
    </h3>
    <div style="font-style: italic;">December 2024, pp. 755-775(21)</div>
    <div>Authors: Albanwan, Hessah; Qin, Rongjun; Tang, Yang</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Remote sensing image fusion is consistently used to turn raw images of different resolutions, sources, and modalities into accurate, complete, and spatiotemporally coherent images. It facilitates downstream applications such as pan sharpening, change detection, and classification. However, image fusion solutions are highly disparate to various remote sensing problems and are often narrowly defined in existing reviews as topical applications (e. g., pan sharpening). Theoretically, image fusion can be applied to any gridded data through pixel-level operations; thus, we expand its scope by comprehensively surveying relevant works. We develop a simple taxonomy for many-to-one and many-to-many image fusion, defining it as a mapping problem turning one or multiple images into another set based on desired coherence. Furthermore, we provide a meta-analysis to cover 10,420 peer-reviewed papers from the 1980s to 2023 studying various types of image fusion and their applications. Finally, we discuss image fusion's benefits and emerging challenges to provide open research directions.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000011/art00012;jsessionid=wo3vdpacs6r9.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Machine Learning and New-Generation Spaceborne Hyperspectral Data Advance Crop Type Mapping
        </a>
    </h3>
    <div style="font-style: italic;">November 2024, pp. 687-698(12)</div>
    <div>Authors: Aneece, Itiya; Thenkabail, Prasad S.; McCormick, Richard; Alifu, Haireti; Foley, Daniel; Oliphant, Adam J.; Teluguntla, Pardhasaradhi</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Hyperspectral sensors provide near-continuous spectral data that can facilitate advancements in agricultural crop classification and characterization, which are important for addressing global food and water security issues. We investigated two new-generation hyperspectral sensors, Germany’s Deutsches Zentrum für Luft‐ und Raumfahrt Earth Sensing Imaging Spectrometer (DESIS) and Italy’s PRecursore IperSpettrale della Missione Applicativa (PRISMA), within California's Central Valley in August 2021 focusing on five irrigated agricultural crops (alfalfa, almonds, corn, grapes, and pistachios). With reference data from the U.S. Department of Agriculture Cropland Data Layer, we developed a spectral library of the crops and classified them using three machine learning algorithms (support vector machines [SVM], random forest [RF], and spectral angle mapper [SAM]) and two philosophies: 1. Full spectral analysis (FSA) and 2. Optimal hyperspectral narrowband (OHNB) analysis. For FSA, we used 59 DESIS four-bin product bands and 207 of 238 PRISMA bands. For OHNB analysis, 9 DESIS and 16 PRISMA nonredundant OHNBs for studying crops were selected. FSA achieved only 1% to 3% higher accuracies relative to OHNB analysis in most cases. SVM provided the best results, closely followed by RF. Using both DESIS and PRISMA image OHNBs in SVM for classification led to higher accuracy than using either image alone, with an overall accuracy of 99%, producer’s accuracies of 94% to 100%, and user???s accuracies of 95% to 100%.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000010/art00013;jsessionid=16mf6lifehhl7.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Bank Line Extraction by Integration of Orthoimages and Lidar Digital Elevation Model Using Principal Component Analysis and Alpha Matting
        </a>
    </h3>
    <div style="font-style: italic;">October 2024, pp. 631-638(8)</div>
    <div>Authors: Deshpande, Sagar S.</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Riverbank lines change over time, causing loss of land and property. Accurate mapping of riverbank lines is essential for restoration and preservation. This paper presents a method to map riverbank lines by combining georeferenced orthoimages and lidar digital elevation model (DEM). This method uses the properties that lidar can provide elevations under trees and open water edges are visible in orthoimages to extract the planimetric locations of bank lines. The orthoimage pixels with less than 0.15% slope on the DEM were replaced by water pixels. Principal component analysis (PCA) was conducted using DEM, slope, and orthoimage bands. Training data of river body and the background were identified manually on the first three component images. An alpha matting–based method was implemented using the training data to extract the bank lines. Bankline using α value of 50% probability were statistically and visually better when compared to the manual bank lines.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000008/art00007;jsessionid=3672g7ms4fes8.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Research Progress of Optical Satellite Remote Sensing Monitoring Asphalt Pavement Aging
        </a>
    </h3>
    <div style="font-style: italic;">August 2024, pp. 471-482(12)</div>
    <div>Authors: Wang, Jingwen; Yang, Dayong; Xie, Zhiwei; Wang, Han; Hao, Zhigang; Zhou, Fanyu; Wang, Xiaona</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The aging condition of asphalt pavement is an invaluable basis for traffic infrastructure evaluation. Due to the amount of time and high cost of monitoring and identifying asphalt pavement aging, many current studies focus on satellite remote sensing methods. In this paper, some methods and technologies for monitoring asphalt pavement degradation by optical satellite remote sensing are introduced as a literature review. Many researchers have developed spectrum libraries based on the actual aging of asphalt pavements, and it is possible to construct pavement health indices based on spectrum changes. Some indexes can extract different aging degrees of asphalt pavement from optical satellite images. Of course, current research can only preliminarily reflect the aging phenomenon of asphalt pavement and cannot accurately describe the distress characteristics of asphalt pavement. Future research needs to further strengthen mechanism research, develop higher resolution images, improve image processing technology, and adopt multi-means fusion analysis methods.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000008/art00008;jsessionid=3672g7ms4fes8.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Mapping Winter Wheat Using Ensemble‐Based Positive Unlabeled Learning Approach
        </a>
    </h3>
    <div style="font-style: italic;">August 2024, pp. 483-491(9)</div>
    <div>Authors: Wang, Hanxiang; Yu, Fan; Xie, Junwei; Wan, Huawei; Zheng, Haotian</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            High‐resolution remote sensing images can support machine learning methods to achieve remarkable results in agricultural monitoring. However, traditional supervised learning methods require pre-labeled training data and are unsuitable for non-fully labeled areas. Positive and Unlabeled Learning (PUL), can deal with unlabeled data. A loss function PU-Loss was proposed in this study to directly optimize the PUL evaluation metric and to address the data imbalance problem caused by unlabeled positive samples. Moreover, a hybrid normalization module Batch Instance-Layer Normalization was proposed to perform multiple normalization methods based on the resolution size and to improve the model performance further. A real‐world positive and unlabeled winter wheat data set was used to evaluate the proposed method, which outperformed widely used models such as U‐Net, DeepLabv3+, and DA‐Net. The results demonstrated the potential of PUL for winter wheat identification in remote sensing images.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000008/art00010;jsessionid=3672g7ms4fes8.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Building Shadow Detection Based on Improved Quick Shift Algorithm in GF‐2 Images
        </a>
    </h3>
    <div style="font-style: italic;">August 2024, pp. 493-502(10)</div>
    <div>Authors: Chen, Yunzhi; Wang, Chao; Wang, Wei; Zhang, Xiang; Chen, Nengcheng</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Shadows in remote sensing images contain crucial information about various features on the ground. In this study, a method for detecting building shadows in GF‐2 images based on improved quick shift was proposed. First, six feature variables were constructed: first principal component (PC1), brightness component (I), normalized difference shadow index (NDSI), morphological shadow index (MSI), normalized difference water index (NDWI), and normalized difference vegetation index (NDVI). Then, the image was segmented to obtain homogeneous objects, which were then classified using a random forest model. Two improvements were added to the quick shift algorithm: using PC1, I, and MSI as input data instead of RGB images; and adding Canny edge constraints. Validation in six research areas yields Kappa coefficients of 0.928, 0.896, 0.89, 0.913, 0.879, and 0.909, confirming method feasibility. In addition, comparative experiments demonstrate its effectiveness and robustness across different land cover types while mitigating the segmentation scale effect.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000008/art00011;jsessionid=3672g7ms4fes8.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Hyperspectral Reflectance Assessment for Preliminary Identification of Degraded Soil Zones in Industrial Sites, India
        </a>
    </h3>
    <div style="font-style: italic;">August 2024, pp. 503-509(7)</div>
    <div>Authors: Dutta, Amitava; Tyagi, Rashi; Sharma, Shilpi; Datta, Manoj</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The study explores the potential of next-generation satellite hyperspectral imaging systems for screening and predicting surface‐soil contamination and degradation by exploiting various spectral indices and signature‐matching techniques at a heavily industrialized area in India. The soil moisture content, desertification and salinity status, clay or fine material content, heavy metal content, vegetation health status, and stress levels were assessed from continuum-removed spectral reflectance values. Results indicated the presence of water in two tailings ponds, high salinity, and desertification values in most of the tailings ponds and dump sites, clay boundary liner along four ponds, high heavy metal indices along three ponds and all dump sites, highly stressed vegetation near all tailings ponds and coal dump sites, and pollutants in nearby water channels. The results suggest a strategy for the initial identification of priority areas for ground-based investigations and an alternative rapid methodology to monitor large industrial hubs in India.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000008/art00014;jsessionid=3672g7ms4fes8.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            One-Dimensional-Mixed Convolution Neural Network and Covariance Pooling Model for Mineral Mapping of Porphyry Copper Deposit Using PRISMA Hyperspectral Data
        </a>
    </h3>
    <div style="font-style: italic;">August 2024, pp. 511-522(12)</div>
    <div>Authors: Peyghambari, Sima; Zhang, Yun; Heidarian, Hassan; Sekandari, Milad</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Mapping distribution of alterations around porphyry copper deposits (PCDs) greatly affects mineral exploration. Diverse geological processes generate irregular alteration patterns with diverse spectral characteristics in mineral deposits. Applying remotely sensed hyperspectral images (HSIs) is an appealing technology for geologic surveyors to generate alteration maps. Conventional methods mainly use shallow spectral absorption features to discriminate minerals and cannot extract their important spectral information. Deep neural networks with nonlinear layers can evoke the deep spectral and spatial information of HSIs. Deep learning-based methods include fully connected neural networks, convolutional neural networks, and hybrid convolutional networks like mixed convolution neural network and covariance pooling (MCNN‐CP) algorithms. However, each has its advantages and limitations. To significantly avoid losing important spectral features, we proposed a new method by fusing a one‐dimensional convolutional neural network (1D‐CNN) with MCNN‐CP (1D‐MCNN‐CP), achieving an overall accuracy (97.44%) of mineral mapping from PRISMA HSIs. This research deduced that 1D‐MCNN‐CP improved performance and reduced misclassification errors among minerals sharing similar spectral features.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000007/art00007;jsessionid=6gon1chdu52hj.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            ReLAP-Net: Residual Learning and Attention Based Parallel Network for Hyperspectral and Multispectral Image Fusion
        </a>
    </h3>
    <div style="font-style: italic;">July 2024, pp. 395-403(9)</div>
    <div>Authors: Agrawal, Aditya; SourajaKundu; Ahmad, Touseef; Bhatt, Manish</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Remote sensing applications require high-resolution images to obtain precise information about the Earth???s surface. Multispectral images have high spatial resolution but low spectral resolution. Hyperspectral images have high spectral resolution but low spatial resolution. This study proposes a residual learning and attention-based parallel network based on residual network and channel attention. The network performs image fusion of a high spatial resolution multispectral image and a low spatial resolution hyperspectral image. The network training and fusion experiments are conducted on four public benchmark data sets to show the effectiveness of the proposed model. The fusion performance is compared with classical signal processing-based image fusion techniques. Four image metrics are used for the quantitative evaluation of the fused images. The proposed network improved fusion ability by reducing the root mean square error and relative dimensionless global error in synthesis and increased the peak signal-to-noise ratio when compared to other state-of-the-art models.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000007/art00009;jsessionid=6gon1chdu52hj.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Assessing the Utility of Uncrewed Aerial System Photogrammetrically Derived Point Clouds for Land Cover Classification in the Alaska North Slope
        </a>
    </h3>
    <div style="font-style: italic;">July 2024, pp. 405-414(10)</div>
    <div>Authors: Liu, Jung-kuan; Qin, Rongjun; Arundel, Samantha T.</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Uncrewed aerial systems (UASs) have been used to collect “pseudo field plot” data in the form of large-scale stereo imagery to supplement and bolster direct field observations to monitor areas in Alaska. These data supplement field data that is difficult to collect in such a vast landscape with a relatively short field season. Dense photogrammetrically derived point clouds are created and are facilitated to extract land cover data using a support vector machine (SVM) classifier in this study. We test our approach using point clouds derived from 1-cm stereo imagery of plots in the Alaska North Slope region and compare the results to field observations. The results show that the overall accuracy of six land cover classes (bare soil, shrub, grass, forb/herb, rock, and litter) is 96.8% from classified patches. Shrub had the highest accuracy (>99%) and forb/herb achieved the lowest (<48%). This study reveals that the approach could be used as reference data to check field observations in remote areas.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000007/art00010;jsessionid=6gon1chdu52hj.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Enhancing Forest‐Steppe Ecotone Mapping Accuracy through Synthetic ApertureRadar‐Optical Remote Sensing Data Fusion and Object-based Analysis
        </a>
    </h3>
    <div style="font-style: italic;">July 2024, pp. 415-426(12)</div>
    <div>Authors: Wang, Ruilin; Wang, Meng; Sun, Xiaofang; Wang, Junbang; Li, Guicai</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            In ecologically vulnerable regions with intricate land use dynamics, such as ecotones, frequent and intense land use transitions unfold. Therefore, the precise and timely mapping of land use becomes imperative. With that goal, by using principal component analysis, we integrated Sentinel-1 and Sentinel-2 data, using an object-oriented methodology to craft a 10-meter-resolution land use map for the forest‐grassland ecological zone of the Greater Khingan Mountains spanning the years 2019 to 2021. Our research reveals a substantial enhancement in classification accuracy achieved through the integration of synthetic aperture radar‐optical remote sensing data. Notably, our products outperformed other land use/land cover data sets, excelling particularly in delineating intricate riverine wetlands. The 10-meter land use product stands as a pivotal guide, offering indispensable support for sustainable development, ecological assessment, and conservation endeavors in the Greater Khingan Mountains region.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000007/art00011;jsessionid=6gon1chdu52hj.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Dynamic Monitoring of Ecological Quality in Eastern Ukraine Amidst the Russia‐Ukraine Conflict
        </a>
    </h3>
    <div style="font-style: italic;">July 2024, pp. 427-435(9)</div>
    <div>Authors: Zhang, Chaofei; Xu, Zhanghua; Yang, Yuanyao; Sun, Lei; Li, Haitao</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            To evaluate the spatiotemporal changes in the ecological environment of eastern Ukraine since the Russia-Ukraine conflict, this study used MODIS images from March to September 2020 and 2022 to calculate the Remote Sensing-Based Ecological Index. In 2022, compared with 2020, conflict zones exhibited reduced improvement and increased slight degradation, whereas nonconflict areas showed marginal enhancement. Through propensity score matching, the research confirmed the causal relationship between conflict and ecological trends. Pathway analysis revealed that the conflict contributed to 0.016 units increase in ecological quality while reducing the improvement rate by 0.042 units. This study provides empirical support for understanding the correlation between conflicts and specific environmental factors, offering technical references for ecological quality assessments in other conflict areas and future evaluations by the Ukrainian government.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000007/art00013;jsessionid=6gon1chdu52hj.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Surface Water Extraction Method Integrating Spectral and Temporal Characteristics
        </a>
    </h3>
    <div style="font-style: italic;">July 2024, pp. 437-450(14)</div>
    <div>Authors: Zou, Yebin</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Remote sensing has been applied to observe large areas of surface water to obtain higher-resolution and long-term continuous observation records of surface water. However, limitations remain in the detection of large-scale and multi-temporal surface water mainly due to the high variability in water surface signatures in space and time. In this study, we developed a surface water remote sensing information extraction model that integrates spectral and temporal characteristics to extract surface water from multi-dimensional data of long-term Landsat scenes to explore the spatiotemporal changes in surface water over decades. The goal is to extract open water in vegetation, clouds, terrain shadows, and other land cover backgrounds from medium-resolution remote sensing images. The average overall accuracy and average kappa coefficient of the classification were verified to be 0.91 and 0.81, respectively. Experiments applied to China’s inland arid area have shown that the method is effective under complex surface environmental conditions.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000006/art00011;jsessionid=1puu3o152mh4a.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Monitoring an Ecosystem in Crisis: Measuring Seagrass Meadow Loss Using Deep Learning in Mosquito Lagoon, Florida
        </a>
    </h3>
    <div style="font-style: italic;">June 2024, pp. 363-370(8)</div>
    <div>Authors: Insalaco, Stephanie A.; Herrero, Hannah V.; Limber, Russ; Oliver, Clancy; Wolfson, William B.</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The ecosystem of Mosquito Lagoon, Florida, has been rapidly deteriorating since the 2010s, with a notable decline in keystone seagrass species. Seagrass is vital for many species in the lagoon, but nutrient overloading, algal blooms, boating, manatee grazing, and other factors have led to its loss. To understand this decline, a deep neural network analyzed Landsat imagery from 2000 to 2020. Results showed significant seagrass loss post-2013, coinciding with the 2011–2013 super algal bloom. Seagrass abundance varied annually, with the model performing best in years with higher seagrass coverage. While the deep learning method successfully identified seagrass, it also revealed that recent seagrass coverage is almost non-existent. This monitoring approach could aid in ecosystem recovery if coupled with appropriate policies for Mosquito Lagoon's restoration.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000006/art00012;jsessionid=1puu3o152mh4a.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Real-Time Cross-View Image Matching and Camera Pose Determination for Unmanned Aerial Vehicles
        </a>
    </h3>
    <div style="font-style: italic;">June 2024, pp. 371-381(11)</div>
    <div>Authors: Chen, Long; Wu, Bo; Duan, Ran; Chen, Zeyu</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            In global navigation satellite systems (GNSS)-denied environments, vision-based methods are commonly used for the positioning and navigation of aerial robots. However, traditional methods often suffer from accumulative estimation errors over time, leading to trajectory drift and lack real-time performance, particularly in large-scale scenarios. This article presents novel approaches, including feature-based cross-view image matching and the integration of visual odometry and photogrammetric space resection for camera pose determination in real-time. Experimental evaluation with real UAV datasets demonstrated that the proposed method reliably matches features in cross-view images with large differences in spatial resolution, coverage, and perspective views, achieving a root-mean-square error of 4.7 m for absolute position error and 0.33° for rotation error, and delivering real-time performance of 12 frames per second (FPS) when implemented in a lightweight edge device onboard UAV. This approach offters potential for diverse intelligent UAV applications in GNSS-denied environments based on real-time feedback control.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000004/art00011;jsessionid=11kwpkbxmfvw6.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Investigation of Underwater Photogrammetry Method with Cost-Effective Action Cameras and Comparative Analysis between Reconstructed 3D Point Clouds
        </a>
    </h3>
    <div style="font-style: italic;">April 2024, pp. 251-259(9)</div>
    <div>Authors: Hamal, Seda Nur Gamze; Ulvi, Ali</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Currently, digital cameras and equipment used underwater are often inaccessible to the general public due to their professional-grade quality and high cost. Therefore alternative solutions have been sought that are both cost-effective and suitable for nonprofessional use. A review of the literature shows that researchers primarily use GoPro action cameras, while other action cameras with similar capabilities are rarely used. This study thus examines underwater photogrammetry methods using a widely recognized action camera as a reference and compares it with another camera of similar characteristics as a potential alternative. For a comprehensive temporal analysis in underwater studies, both cameras were used to capture photographic and video imagery, and the resulting 3D point clouds were compared. Comparison criteria included data collection and processing times, point cloud densities, cloud-to-cloud analysis, and assessments of surface density and roughness. Having analysed, the study concluded that the proposed alternative action camera can feasibly be used in underwater photogrammetry.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000003/art00007;jsessionid=3bhf6q783oo37.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Assessment, Specification, and Validation of a Geolocation System's Accuracy and Predicted Accuracy
        </a>
    </h3>
    <div style="font-style: italic;">March 2024, pp. 157-168(12)</div>
    <div>Authors: Dolloff, John; Theiss, Henry; Bollin, Brian</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            This article presents recommendations and corresponding detailed procedures for the assessment of a geolocation system's accuracy, as well as the specification of accuracy requirements and their subsequent validation when they are available. Applicable metrics and related processing are based on samples of corresponding geolocation errors. This article also presents similar recommendations for the predicted accuracy of a geolocation system, based on samples of geolocation error, as well as corresponding predicted error covariance matrices associated with the geolocations. Reliable error covariance matrices enable optimal use of a geolocation system's products, such as the optimal fusion of multiple geolocations or multiple products for higher confidence and increased accuracy. The recommendations presented in this article enable reliable estimates of accuracy and reliable predicted accuracies, both of which are critical to many geolocation-based applications. The recommendations associated with predicted accuracy are also relatively new and innovative.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000002/art00009;jsessionid=18dj5u3l1hy3.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            The Sight-Aesthetic Value of the Underwater Landscapes of Lakes in the Context of Exploration Tourism
        </a>
    </h3>
    <div style="font-style: italic;">February 2024, pp. 89-97(9)</div>
    <div>Authors: Dynowski, Piotr; Źróbek-Sokolnik, Anna; Czaplicka, Marta; Senetra, Adam</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The aim of the study is to identify factors affecting the sight-aesthetic value of the underwater landscapes of lakes for the purposes of exploration tourism. The reason for undertaking this topic is the lack of such studies for inland water bodies. The results will contribute to expanding and supplementing the knowledge on the assessment of the sight-aesthetic attractiveness of landscapes and fill gaps in knowledge about the underwater landscapes of lakes. The questionnaire survey implemented the direct comparison method described by Kendall (Kendall, M. G. 1970.Rank Correlation Methods. Charles Griffin and Co: Glasgow, Scotland). According to respondents, animals and submerged anthropogenic elements are the most visually attractive in an aquatic environment The results obtained are the reason for conducting further research and developing the methodology for the assessment of the sight-aesthetic value of inland bodies of water based on the experience of terrestrial landscape researchers.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000002/art00010;jsessionid=18dj5u3l1hy3.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Crop Monitoring System Using MODIS Time-Series Data for Within-Season Prediction of Yield and Production of US Corn and Soybeans
        </a>
    </h3>
    <div style="font-style: italic;">February 2024, pp. 99-119(21)</div>
    <div>Authors: Sakamoto, Toshihiro</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            In terms of contribution to global food security, this study aimed to build a crop monitoring system for within-season yield prediction of US corn and soybeans by using the Moderate Resolution Imaging Spectroradiometer (time-series data, which consists of three essential core algorithms (crop phenology detection, early crop classification, and crop yield prediction methods)). Within-season predictions for 2018–2022 were then made to evaluate the perfor- mance of the proposed system by comparing it with the United States Department of Agriculture's (USDA's) monthly forecasts and the fixed statistical data. The absolute percentage errors of the proposed system for predicting national-level yield and production were less than 5% for all simulation years as of day of year (DOY) 279. The prediction accuracy as of DOY 247 and DOY 279 were comparable to the USDA's forecasts. The proposed system would enable us to make a comprehensive understanding about overview of US corn and soybean crop condition by visualizing detail spatial pattern of good- or poor harvest regions on a within-season basis.
        </details>
    </div>
</article>
