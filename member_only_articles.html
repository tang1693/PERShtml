<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000011/art00010;jsessionid=f2j9va0266v8.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Morphology-Based Feature Extraction Network for Arbitrary-Oriented SAR Vehicle Detection
        </a>
    </h3>
    <div style="font-style: italic;">November 2024, pp. 665-673(9)</div>
    <div>Authors: Chen, Ting; Huang, Xiaohong</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            In recent years, synthetic aperture radar (SAR) vehicle detection has become a research hotspot. However, algorithms using horizontal bounding boxes can lead to redundant detection areas due to the varying aspect ratio and arbitrary orientation of vehicle targets. This paper proposes a morphology-based feature extraction network (MFE-Net), which fully uses the prior shape knowledge of the vehicle targets. Specifically, we adopt rotatable bounding boxes to predict the targets, and a novel rectangular rotation-invariant coordinate convolution (RRICC) is proposed to extract the feature, which can determine more accurately the convolutional sampling location of the vehicles. The adaptive thresholding denoising module (ATDM) is designed to suppress background clutter. Furthermore, inspired by the convolutional neural networks (CNNs) and self-attention, we propose the hybrid representation enhancement module (HREM) to highlight the vehicle target features. The experiment results show that the proposed model obtains an average precision (AP) of 93.1% on the SAR vehicle detection data set (SVDD).
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000011/art00011;jsessionid=f2j9va0266v8.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatial-Spectral Middle Cross-Attention Fusion Network for Hyperspectral Image Superresolution
        </a>
    </h3>
    <div style="font-style: italic;">November 2024, pp. 675-686(12)</div>
    <div>Authors: Lang, Xiujuan; Lu, Tao; Zhang, Yanduo; Jiang, Junjun; Xiong, Zixiang</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The spatial and spectral features of hyperspectral images exhibit complementarity, and neglecting them prevents the full exploitation of useful information for superresolution. This article proposes a spatial-spectral middle cross-attention fusion network to explore the spatial-spectral structure correlation. Initially, we learn spatial and spectral features through spatial and spectral branches instead of single ones to reduce information compression. Then, a novel middle-cross attention fusion block that includes middle features fusion strategy and cross-attention is proposed to fuse spatial-spectral features to enhance their mutual effects, which aims to explore the spatial-spectral structural correlations. Finally, we propose a spectral feature compensation mechanism to provide complementary information for adjacent band groups. The experimental results show that the proposed method outperforms state-of-the-art algorithms in object values and visual quality.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000011/art00014;jsessionid=f2j9va0266v8.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Variable-Iterative Fully Convolutional Neural Network for Sparse Unmixing
        </a>
    </h3>
    <div style="font-style: italic;">November 2024, pp. 699-706(8)</div>
    <div>Authors: Kong, Fanqiang; Lv, Zhijie; Wang, Kun; Fang, Xu; Zheng, Yuhan; Yu, Shengjie</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Neural networks have greatly promoted the development of hyperspectral unmixing (HU). Most data-driven deep networks extract features of hyperspectral images (HSIs) by stacking convolutional layers to achieve endmember extraction and abundance estimation. Some model-driven networks have strong interpretability but fail to mine the deep feature. We propose a variable-iterative fully convolutional neural network (VIFCNN) for sparse unmixing, combining the characteristics of these two networks. Under the model-driven iterative framework guided by sparse unmixing by variable splitting and augmented lagrangian (SUnSAL), a data-driven spatial-spectral feature learning module and a spatial information updating module are introduced to enhance the learning of data information. Experimental results on synthetic and real datasets show that VIFCNN significantly outperforms several traditional unmixing methods and two deep learning???based methods. On real datasets, our method improves signal-to-reconstruction error by 17.38%, reduces abundance root-mean-square error by 25.24%, and reduces abundance spectral angle distance by 31.40% compared with U-ADMM-ÃŸUNet.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000010/art00008;jsessionid=17jacolb65jc1.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Attention Heat Map-Based Black-Box Local Adversarial Attack for Synthetic Aperture Radar Target Recognition
        </a>
    </h3>
    <div style="font-style: italic;">October 2024, pp. 601-609(9)</div>
    <div>Authors: Wan, Xuanshen; Liu, Wei; Niu, Chaoyang; Lu, Wanjie</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Synthetic aperture radar (SAR) automatic target recognition (ATR) models based on deep neural networks (DNNs) are susceptible to adversarial attacks. In this study, we proposed an SAR black-box local adversarial attack algorithm named attention heat map- based black-box local adversarial attack (AH-BLAA). First, we designed an attention heat map extraction module combined with the layer-wise relevance propagation (LRP) algorithm to obtain the high concerning areas of the SAR-ATR models. Then, to gener- ate SAR adversarial attack examples, we designed a perturbation generator module, introducing the structural dissimilarity (DSSIM) metric in the loss function to limit image distortion and the dif- ferential evolution (DE) algorithm to search for optimal perturba- tions. Experimental results on the MSTAR and FUSAR-Ship datasets showed that compared with existing adversarial attack algorithms, the attack success rate of the AH-BLAA algorithm increased by 0.63% to 33.59% and 1.05% to 17.65%, respectively. Moreover, the low- est perturbation ratios reached 0.23% and 0.13%, respectively.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000010/art00010;jsessionid=17jacolb65jc1.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Exploring the Potential of the Hyperspectral Remote Sensing Data China OrbitaZhuhai-1in Land Cover Classification
        </a>
    </h3>
    <div style="font-style: italic;">October 2024, pp. 611-619(9)</div>
    <div>Authors: Li, Caixia; Xiong, Xiaoyan; Wang, Lin; Li, Yunfan; Wang, Jiaqi; Zhang, Xiaoli</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Responding to the shortcomings of China's civil remote sensing data in land cover classification, such as the difficulty of data acquisition and the low utilization rate, we used Landsat-8, China Orbita Zhuhai-1 hyperspectral remote sensing (OHS) data, and Landsat-8 + OHS data combined with band (red, green, and blue) and vegetation index features to classify land cover using maximum likelihood (ML), Mahalanobis distance (MD), and support vector machine (SVM). The results show that Landsat-8 + OHS data have the highest classification accuracy in SVM, with an overall accuracy of 83.52% and a kappa coefficient of 0.71, and this result is higher than that of Landsat-8 images and OHS images separately. In addition, the classification accuracy of OHS images was higher than that of Landsat-8 images. The results of the study provide a reference for the use of civil satellite remote sensing data in China.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
        
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000010/art00012;jsessionid=17jacolb65jc1.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Teacher-Student Prototype Enhancement Network for a Few-Shot Remote Sensing Scene Classification
        </a>
    </h3>
    <div style="font-style: italic;">October 2024, pp. 621-630(10)</div>
    <div>Authors: Zhu, Ye; Yang, Shanying; Yu, Yang</div>
    <div>
        Abstract: 
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Few-shot remote sensing scene classification identifies new classes from limited labeled samples where the great challenges are intraclass diversity, interclass similarity, and limited supervision. To alleviate these problems, a teacher-student prototype enhancement network is proposed for a few-shot remote sensing scene classification. Instead of introducing an attentional mechanism in mainstream studies, a prototype enhancement module is recommended to adaptively select high-confidence query samples, which can enhance the support prototype representations to emphasize intraclass and interclass relationships. The construction of a few-shot teacher model generates more discriminative predictive representations with inputs from many labeled samples, thus providing a strong supervisory signal to the student model and encouraging the network to achieve accurate classification with a limited number of labeled samples. Extensive experiments of four public datasets, including NWPU-remote sens ing image scene classification (NWPU-RESISC45), aerial image dataset (AID), UC Merced, and WHU-RS19, demonstrate that this method achieves superior competitive performance than the state-of-the-art methods on five-way, one-shot, and five-shot classifications.
        </details>
    </div>
</article>
