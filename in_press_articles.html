<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00007" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Efficient Coral Survey Using Aerial Remote Sensing and Multi-modal Segmentation for Large-Scale Ecological Assessment
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: April 1, 2025</div>
    <div>Authors: Qin, Jiangying; 1; Li, Ming; 2; Armin, Gruen; 3; Gong, Jianya; 4; Zhong, Jiageng; 2; Liao, Xuan; 5</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Due to the ecological pressures of global warming and human activities in coastal regions, coral reef ecosystems, predominantly located in shallow marine areas, are facing severe threats to their survival. Scientists and governmental managers are eager to leverage novel aerial remote sensing technologies to address the challenges of acquiring accurate, comprehensive, and timely data on coral reef health, structural complexity, and spatial distribution. This study aims to tackle these challenges by using precise and accurate aerial remote sensing data to support the restoration and sustainable prosperity of coral reef systems. Specifically, this study develops and applies an efficient coral survey method based on aerial remote sensing. The method integrates aerial imagery and bathymetric lidar (light detection and ranging) point cloud data and uses advanced photogrammetric computer vision and deep learning algorithms. Using a state-of-the-art multi-modal neural network segmentation technique, the proposed method enables high-precision and intelligent identification of coral reefs, facilitating detailed habitat mapping. Furthermore, by accurately delineating the habitat range and geometric structures of reefs, this approach allows for precise measurements of coral biomass production and skeletal calcification. These metrics help assess coral reef structural complexity and their adaptability to environmental stressors, providing robust scientific data for conservation strategies and policy making. The use of advanced multi-modal aerial remote sensing data not only enhances monitoring reliability and accuracy but also offers a cost-effective and flexible tool for coral reef ecological mapping. This approach effectively addresses challenges encountered in coastal ecological surveys, particularly in areas where direct human access or boat entry is difficult.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00049" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Analysis of Landslide Susceptibility of the Darjeeling District Using a Frequency Ratio Model and Geographical Weighted Regression
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: April 30, 2025</div>
    <div>Authors: Sen, Suhel; 1; Sarif, Md. Omar; 2; 3</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            India yearly experiences natural disasters such as floods, droughts, cyclones, and landslides, which not only disrupt but also significantly affect the socioeconomic activities of its people. Darjeeling, one of the most valuable districts of West Bengal, India, is located in the foothills of the mighty Himalayas. Landslides in Darjeeling, particularly during the monsoon season, severely affect the socioeconomic conditions of the local population. This research creates a landslide susceptibility map of Darjeeling using the frequency ratio model (FRM) and geographical weighted regression (GWR). The study considers 16 landslide-conditioning factors and 743 past landslide events, of which 70% are used for modeling and the remaining 30% are used for validation. The generated landslide susceptibility map categorizes the area into very low, low, moderate, high, and very high susceptibility zones. Results show that most of the study area falls within the high susceptibility zone, covering approximately 28.45% of the region. The study also reveals that the northern hilly regions of the district are more prone to landslides compared with the southern part, based on FRM and GWR. The area under curve indicates that the model achieved a 74.7% success rate. The highest value of the landslide density index was found in areas classified as having very high landslide susceptibility. Therefore, considering the landslide susceptibility, implementation of effective management strategies is essential for ensuring the future development of the district.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00101" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Phase Center Extraction of Corner Reflector Points in Single-Look Complex Image
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: April 8, 2025</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The geometric calibration accuracy of the Synthetic Aperture Radar (SAR) system is affected by the extraction accuracy of Corner Reflector (CR) points in a Single-Look Complex (SLC) image. There are some problems in traditional artificial extraction methods for CR points, such as the difficulty of identifying CR points without prior knowledge, low efficiency, and limited extraction accuracy. An extraction method of the phase center for CR points without prior knowledge was proposed in this paper through the construction of a geometric error compensation model for SAR images and the study of high precision fitting methods of the phase center for CR points. The effectiveness of the proposed method was verified by the Gaofen-3 (GF-3) satellite image data. Experimental results demonstrated that the extraction accuracy of CR points could be better than 0.2 pixel for GF-3 images.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00156" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A High-Quality Underwater 3D Reconstruction Solution for Coral Reef Environments Leveraging Advanced Photogrammetric Computer Vision Techniques
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: April 7, 2025</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Coral reefs, with their intricate 3D structures, are vital marine ecosystems increasingly threatened by environmental stressors. Accurate 3D reconstruction of these underwater structures is essential for scientific research and conservation management. While underwater photogrammetry has emerged as a promising tool for this purpose, technical challenges persist in capturing fine-scale features under underwater conditions, particularly the intricate morphology of coral reefs and the highly complex textures that hinder high-fidelity reconstruction. Recent breakthroughs in computer vision and deep learning have introduced new opportunities for underwater photogrammetry. This study begins by outlining a photogrammetric workflow that can integrate current advanced technologies, followed by summarizing cutting-edge methods used in the key stages, i.e., sparse and dense reconstruction. Building on previous research, we propose a hierarchical reconstruction strategy for accurate and efficient dense modeling. Our approach first performs an efficient global coarse-grained reconstruction to capture the overall scene structure, followed by fine-scale modeling in key regions of interest. Using image data collected from a coral reef site at Moorea Island, we compare and evaluate various techniques, analyzing their respective strengths and limitations. In sparse reconstruction, the classical feature method scale-invariant feature transform demonstrates competitive performance. Deep learning–based methods, such as ALIKED feature and the SuperGlue matching network, achieve superior results on certain metrics. For dense reconstruction, Neural Radiance Fields and 3D Gaussian Splatting–based methods yield high-quality reconstructions but are computationally intensive. In contrast, the deep learning–based multi-view stereo approach achieves comparable reconstruction quality with greater efficiency. Experimental results on reconstruction result fusion further validate that our approach offers a scalable and practical solution for coral reef monitoring, advancing conservation science and ecosystem management practices.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00116" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Texture-Semantic Point: Registration for Point Clouds of Porcelain Relics
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: April 8, 2025</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Point cloud models of porcelain are captured through the multi-registration of point clouds, which presents a challenging task. On the one hand, the smooth surface of porcelain lacks geometric feature variation, making it difficult to establish corresponding points. On the other hand, the overall geometric symmetry of most porcelain relics can easily lead to iterative calculations falling into the local minimum convergence trap. To address the difficulties in feature point selection, we propose a novel approach using texture-semantic points as features for coarse registration. We first select rich texture points as 2D candidates and establish a 2D-3D matching relationship, giving each candidate its 3D spatial location and associated texture information. Using these correspondences, we perform coarse alignment of the point clouds. However, in reality, the point clouds are not aligned, and the registration calculation fails because of geometric symmetry issues. To address this, we integrate a control net into the iterative closest point (ICP) calculation to guide iterations towards the correct Special Euclidean group in 3 dimensions (SE(3)) transformation, achieving refined alignment. Finally, considering porcelain’s symmetrical geometry, we introduce a pose optimization constraint using the symmetry axis as a weighted parameter to limit degrees of freedom and enhance registration accuracy. Experiments were conducted on seven porcelain datasets to evaluate the proposed approach. A qualitative analysis demonstrated successful refined alignment using the proposed approach. In addition, we performed a quantitative comparison with state-of-the-art methods. Experimental results showed that our approach outperformed others across all models when applied to the registration of geometrically symmetric porcelain; Specifically, the proposed method achieved a 50% enhancement in accuracy compared with others, measured by the distance between the labeled corresponding points. Paper-related resources are available at https://github.com/tnl-wcz/TSP_ICP.git.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-pers_24-00050" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Combined Use of Satellite Observations and the RIM for Assessing Recovery from Natural Disasters
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: April 1, 2025</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            In this study, we explore using satellite observations to assess community recovery from natural disasters such as fires and hurricanes, supplementing the Resilience Inference Model (RIM). The RIM model has been successfully used to quantify recoveries from hurricanes along the Gulf Coast, but it relies on long-term population changes over years or decades. Our approach integrates satellite observations to enhance recovery assessment with a shorter latency of weeks or months. Using fire, vegetation, and night light data from the Visible Infrared Imaging Radiometer Suite (VIIRS) with daily global observations, Sentinel-2, Landsat-8, and Geostationary Operational Environmental Satellite/Advanced Baseline Imager, we quantitatively evaluate fire intensity, light outage, and urban greenness changes, along with subsequent recovery, focusing on the 2023 Maui fire and selected hurricane cases along the Gulf Coast. This approach complements the RIM model by introducing quantifiable physical parameters with shorter latency, particularly beneficial in areas where census data are either unavailable or unreliable.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00046" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Leadership in COVID-19: Mitigation Strategies across Top 30 Gross Domestic Product Countries
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: May 7, 2025</div>
    <div>Authors: Ozdenerol, Esra; Bingham-Byrne, Rebecca Michelle</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            This study analyzes COVID-19 response strategies in the top 30 gross domestic product countries from the first case until 30 September 2021, a period marked by increased vaccinations, the dominance of the Delta variant, and changing public health measures. We examined which mitigation efforts effectively reduced new cases and the viral reproduction rate. Vaccination policies, movement restrictions, and mask mandates were critical in reducing cases per million, while testing, public information, income support, and contact tracing were crucial for lowering the viral reproduction rate. Vaccination policies did not significantly affect the reproduction rate during implementation. The gender of leaders showed no significant effect on cases, deaths, or excess mortality, though female leaders generally implemented fewer isolation days and prioritized additional measures like movement restrictions and stay-at-home orders.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00051" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatiotemporal Continuous Shallow Water Bathymetry from a Kriged Kalman Filter
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: June 9, 2025</div>
    <div>Authors: Wang, Lei; 1; Liu, Hongxing; 2; Kang, Lei; 3; Su, Haibin; 4; Shu, Song; 5; Wang, Jun; 6</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            In GIScience, problems of missing data in space or time are nontrivial. We implemented a Kriged Kalman filter (KKF)–based data interpolation and assimilation technique and tested it for mapping bathymetry at unsampled locations and times. This technique integrates the Kriging and Kalman filter computation frameworks to perform spatiotemporal data assimilation, which can produce spatially and temporally continuous bathymetric fields from samples that are scarce in space and time. The spatiotemporal bathymetric field over the estuary of the Yangtze River was mapped based on the four boat-based depth echo-sounding surveys conducted in 1982, 1997, 2002, and 2010. Our validation and verification analyses showed that the KKF assimilation model can predict bathymetry accurately and reliably at unsampled locations and times. This paper demonstrates that KKF is superior to traditional spatial interpolation methods because it informs the interpolator with the temporal component that also extends the prediction to the time domain. The experiments indicate that greater time intervals in conducting bathymetric surveys result in a more pronounced influence on the performance of KKF than the spatial sparsity of depth samples. The ability of space-time prediction of bathymetry allows underwater depth measurements to be accurately aligned with satellite images, which is essential for improving multispectral image inversion in bathymetry studies.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00048" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Enhancing Residential Building Identification in a Coastal Texas City: An Integrated Framework Leveraging Remote Sensing, GIS, and Transfer Learning Techniques
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: June 9, 2025</div>
    <div>Authors: Ye, Xinyue; 1; Bai, Weishan; 1; Huang, Xiao; 2</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Rapid urbanization and population growth have intensified the need for accurate and efficient identification of residential buildings in urban planning and disaster management. This paper presents a novel approach for identifying residential buildings by leveraging multiple source data and a transfer learning model, with a case study conducted in Galveston Island, Texas. We propose an integrated framework that combines very-high-resolution (VHR) remote sensing imagery, lidar data, points of interest (POI) data, and GIS-based land use information to extract features of residential buildings. We fine-tune a pretrained deep learning model with our data sets to enhance the model’s adaptability and efficiency in detecting residential buildings in the study area. The results demonstrate that the proposed approach achieves high accuracy in identifying residential buildings, with overall accuracy of 85.6% in the case study. This study also offers valuable insights into the potential of combining multiple data sources and transfer learning techniques for improving residential building identification and other related tasks in urban remote sensing applications.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-2400045a" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Lyme Disease Risk Map and Prioritizing Areas for Vaccine Deployment
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: May 18, 2025</div>
    <div>Authors: Bingham-Byrne, Rebecca Michelle; Ozdenerol, Esra</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Disease surveillance, including risk modeling, is beneficial for infectious diseases. Risk prediction maps can be used to prioritize areas to deliver vaccines reducing disease transmission. For zoonotic infectious diseases such as Lyme disease, potential vaccine deployment areas include cost-effective places key to vectors, wildlife hosts, and humans. This paper uses machine-learning techniques to develop models to predict Lyme disease risk, then uses the critical variables within the models, as well those considered principal from previous literature, to create a Lyme disease risk map and to prioritize areas for wildlife vaccine deployment. It was found that highest disease risk is in the eastern United States, especially the upper Midwest and Northeast regions, which coincides with previous literature. The study found national parks and counties within these areas to prioritize wildlife vaccine deployment. Future work includes ground-truthing of the risk map and dispersal of vaccine to priority areas.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00122" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Integration of Near-Proximal and Proximal Lidar Sensing for Fine-Resolution Forest Inventory
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: June 27, 2025</div>
    <div>Authors: Zhao, Chunxi; 1; Hanafy, Hazem; 1; Eissa, Aser M; 1; Hany, Youssef; 1; Habib, Ayman; 1; Shao, Jinyuan; 2; Fei, Songlin; 2</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Near-proximal and proximal light detection and ranging (lidar) systems are increasingly used for high-resolution forest inventory. Near-proximal lidar systems, such as those onboard uncrewed aerial vehicles (UAVs), offer high absolute positional accuracy due to continuous global navigation satellite system (GNSS) signal accessibility. Proximal lidar systems, such as backpack-based mobile mapping, excel at capturing detailed under-canopy information, including forest floor, tree trunks, and debris. However, each system has limitations when used alone. UAV lidar encounters challenges in under-canopy data collection due to occlusions and longer sensor-to-object distance, while proximal lidar faces GNSS signal outages and incomplete top canopy scanning in dense forest areas. This study proposes an approach to integrate near-proximal (UAV) and proximal (backpack) lidar data for fine-resolution forest inventory. Specifically, a framework is presented for integrating both data sources to improve the trajectory of the backpack system and establish an inventory pipeline for individual tree detection/localization, height, and diameter at breast height (DBH) estimation. The experimental results show great potential in generating high-quality, georeferenced point clouds across northern red oak plantation and coniferous forest scenarios. For northern red oak plantation inventory, the proposed pipeline achieved a 100% F1 score in tree detection and stem mapping using integrated lidar data, with root mean square errors (RMSEs) of 3 cm and 2 m for DBH and tree height estimation, respectively, when compared to field measurements. For the mixed coniferous forest inventory, due to the lack of field reference, the integrated UAV and backpack data set have been used to manually count the trees and establish their heights. The pipeline results are compared with those derived manually. The results show that UAV data have the lowest tree-detection accuracy, with an 81.06% F1 score, while proximal and integrated data excel in stem mapping, with a 91.92% F1 score. For the estimated height results, the near-proximal and integrated data have an RMSE value of almost 1.8 m, while the backpack has an RMSE value of 2.25 m. These results demonstrate the advantage of near-proximal/proximal data integration for best evaluation of tree detection, localization, and DBH/height estimation.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00015" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            LFSA-YOLOv7&#65306;A Novel Method for Ship Detection in Remote Sensing Images with Complex Backgrounds
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: June 17, 2025</div>
    <div>Authors: Gu, Heng; 1; Li, Wei; 1; Zhang, Linlin; 2; Meng, Qingyan; 2; Wei, Lianhuan; 3; Wu, Hantian; 4; Ma, Jian; 5</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Ship target detection is a critical component for marine environmental monitoring and the protection of maritime rights and interests. However, existing methods for ship detection in remote sensing imagery often face challenges such as false positives and missed detections, particularly in the presence of complex backgrounds and multi-scale targets. To address these issues, we propose a novel method called LFSA-YOLOv7, which is designed to enhance the accuracy and robustness of ship detection in complex backgrounds: (a) a novel network architecture (LDConv + SPPFCSPC-G + SimAM) to enhance the model’s ship detection capabilities, and (b) a novel Alpha-CoIoU loss function to improve the model’s ship localization accuracy. To evaluate the performance of our algorithm in complex backgrounds, we have developed a novel ship classification detection (SCD) data set and conducted comprehensive experiments. Furthermore, we have validated the generalization of our algorithm across various remote sensing ship data sets. The experimental results demonstrate that our algorithm outperforms previous techniques, exhibiting strong generalization and robustness, making it effectively suitable for ship detection in complex backgrounds. The SCD data set and code will be available at https://github.com/wionn/LFSA-YOLOv7.git.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00140" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            On the Transferability of Semantic Segmentation for Very-High-Resolution Remote Sensing Data of Multi-City Environments
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: June 25, 2025</div>
    <div>Authors: Qin, Rongjun; 1; Zhang, Guixiang; 2; Tang, Yang; 3</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Semantic segmentation of very-high-resolution remote sensing (RS) data is foundational for many RS applications in urban environments. Images from sources like Worldview-3/4 and Pleiades-Neo offer resolutions as high as 0.3 m, enabling a fine-grained understanding of urban structures. Recent deep learning–based methods have significantly outperformed traditional approaches in RS semantic segmentation/classification tasks. However, they require large training data sets and often lack transferability due to highly disparate RS image content across different geographical regions. However, no comprehensive analysis exists on their transferability—i.e., to what extent a model trained on a source domain can be applied to a target domain in urban areas. This paper investigates the raw transferability of traditional and deep-learning models and the effectiveness of domain adaptation approaches in enhancing deep-learning model transferability (adapted transferability). Using five highly diverse RS data sets from different cities (6792 patches of 1024 × 1024 pixels each), we trained six models with and without three domain adaptation approaches to quantitatively analyze transferability between data sets. To facilitate easy assessment of model transferability, we developed a simple method to quantify transferability using spectral indices as a medium, demonstrating its effectiveness in evaluating model transferability at the target domain when labels are unavailable. Our experiments yield several important but under-reported observations on raw and adapted transferability. Moreover, our proposed label-free transferability assessment method outperforms posterior model confidence and can guide model selection for urban studies globally. The models and datasets are publicly available on GitHub at: https://github.com/GDAOSU/Transferability-Remote-Sensing.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00114" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Hierarchical Multi-Scale Cross Interaction Network for Enhanced Hyperspectral Image Classification
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: June 23, 2025</div>
    <div>Authors: Feng, Yuting; 1; Yang, Lina; Wu, Thomas; 2; Huang, Youju; 3</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The significance of hyperspectral image classification lies in its ability to discern subtle differences between materials, making it essential in fields such as agriculture, mineral exploration, and urban planning. Convolutional neural networks (CNNs) and transformer-based methods have become standard for hyperspectral imagery classification, with hybrid approaches gaining popularity. Yet, these methods often lack efficient interaction between the features extracted by CNNs and transformers. To address this, we propose the hierarchical multi-scale cross interaction network (HMCI-Net), which leverages both CNNs and transformers to enhance classification accuracy. The CNN branch extracts local spatial-spectral features, and the transformer branch captures global spectral information, allowing the network to model long-range dependencies and complex correlations. Additionally, HMCI-Net incorporates a hierarchical multi-scale feature extraction module and a multi-view feature fusion module, further improving its ability to extract fine-grained, multi-perspective features. Extensive experiments on four benchmark hyperspectral data sets—Indian Pines, Pavia University, WHU-Hi-LongKou, and Houston2013—demonstrate that HMCI-Net outperforms existing methods, achieving an average improvement of 6.24% in average accuracy, 6.14% in kappa coefficient, and 5.55% in overall accuracy. Specifically, HMCI-Net achieves significant gains, with overall accuracy higher by 8.86%, 4.34%, 4.44%, and 4.42% on Indian Pines, Pavia University, WHU-Hi-LongKou, and Houston2013, respectively. Similarly, average accuracy is higher by 10.57%, 4.94%, 5.65%, and 4.81% for Indian Pines, Pavia University, WHU-Hi-LongKou, and Houston2013, respectively; kappa coefficient is higher by 10.39%, 4.88%, 4.70%, and 4.60%, respectively, on these data sets. The code and data set for this paper can be accessed at: https://github.com/codemanvon30/HMCI_Net.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00002" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Global Multi-Scale Fusion Self-Calibration Network for Remote Sensing Object Detection
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: July 15, 2025</div>
    <div>Authors: Chen, Yan; 1; Shi, Xinlu; 1; Wang, Xiaofeng; 1; Gu, Qi; 1; Zhang, Chen; 1; Xu, Lixiang; 1; Zhan, Shian; 2; Yu, Wenle; 2</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Applications of remote sensing images in both defense and civilian sectors have spurred substantial research interest. In the field of remote sensing, object detection confronts challenges such as complex backgrounds, scale diversity, and the presence of dense small objects. To address these issues, we propose an improved deep learning-based model, the Global Multi-scale Fusion Self-calibration Network, which is expected to contribute to alleviating the challenges. It consists of three main components: the hierarchical feature aggregation backbone, which uses improved modules such as the receptive field context-aware feature extraction module, the global information acquisition module, and the simple parameter-free attention module to extract key features and minimize the background interference. To couple multi-scale features, we enhanced the fusing component and designed the multi-scale enhanced pyramid structure integrating the proposed new modules. During the detection phase, especially when focusing on small object detection, we designed a novel convolutional attention feature fusion head. This head is constructed to integrate local and global branches for feature extraction by leveraging channel shuffling and multi-head attention mechanisms for efficient and accurate detection. Experiments on the Detection in Optical Remote Sensing Images (DIOR), Northwestern Polytechnical University Very High-Resolution-10 (NWPU VHR-10), remote sensing object detection (RSOD), and DOTAv1.0 data sets show that our method achieves mAP50(mean average precision at 50% intersection over union) of 69.7%, 91.3%, 94.2%, and 70.0%, respectively, outperforming existing comparative methods. The proposed network is expected to provide new perspectives for remote sensing tasks and possible solutions for relevant applications in the image domain.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00018" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Lightweight and Accurate Video Synthetic Aperture Radar Target Detection Network
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: July 11, 2025</div>
    <div>Authors: Ma, Huilian; 1; Li, Yinwei; 2; 3; 4; 1; Li, Weisong; 5; Zhu, Yiming; 1; 2; 6</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Video synthetic aperture radar (SAR) target detection has seen rapid development in recent years, but current methods generally suffer from high computational complexity as well as frequent false alarms and missed alarms. Additionally, in video SAR scenarios, small targets become more difficult to identify due to low resolution and complex backgrounds. Recently, leveraging shadows for video SAR moving target detection has proven advantageous as it provides accurate location information and additional boundary details. To address the challenges of target detection in video SAR caused by low resolution and complex backgrounds, as well as the high computational cost of current methods, this paper proposes a lightweight algorithm for video SAR shadow detection based on the YOLOv5 architecture, named Lavs-DeNet. First, a lightweight feature extraction backbone network combining both global and local information called CGLLNet is proposed. This backbone improves the detection performance by mitigating false alarms and missed alarms caused by defocus in video SAR images. Next, a slanted ladder bidirectional feature pyramid network (SL-BiFPN) is designed. By using tilted upsampling with multiple stacked layers, this network efficiently extracts multi-scale features, further reducing missed alarms. Finally, a lightweight information interaction module (LII-C3) has been developed, significantly reducing computational complexity. To validate the effectiveness of the proposed method, experiments were conducted using the data set released by Sandia National Laboratories. The experimental results show that the proposed Lavs-DeNet can achieve 97.88% detection accuracy, requiring only 21.02G floating points and 0.88M parameters, which is superior to the current classical video SAR target detection networks.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00020r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Thermal Point Cloud Generation and Evaluation from Uncalibrated Unmanned Aerial Vehicle&#8211;Visible and Infrared Camera Images Using Position and Orientation System Prior
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: August 27, 2025</div>
    <div>Authors: Liu, Siqi; 1; Wang, Qiang; 1; Liang, Yubin; 1; Wang, Qian; 2; Fan, Shenghong; 3; Cui, Ximin; 4; Zou, Yue; 5</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            This paper presents a methodology for thermal infrared (TIR) point cloud acquisition, leveraging information from the visible camera, TIR camera, and their respective unmanned aerial vehicle (UAV) position and their position and orientation systems (POSs) prior. First, the visible images and their corresponding UAV POS information are used to generate a red–green–blue (RGB) dense point cloud with geographic coordinates; subsequently, using the RGB dense point cloud as a reference, the acquisition of a calibration parameter between global navigation satellite system/inertial measurement unit and the TIR camera is conducted using the POS information corresponding to the selected TIR images; finally, a TIR point cloud of the entire survey area is generated based on the calibration parameters. The availability and suitability of the fused TIR point cloud generated and UAV POS data are also evaluated in terms of localization deviations and orientation deviations. The experimental results demonstrate that the acquisition of coarse-quality TIR point cloud can be achieved by using a priori POS data provided by different small UAVs. The TIR point cloud generated by the UAV POS data guidance can be directly applied to medium- and large-sized buildings. The POS-guided projected images achieved an 86.67% registration success rate with the TIR images, indicating that the orientation deviations are within acceptable limits and establishing a solid foundation for subsequent fine fusion.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00010" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Review of NASA Earth Observations, Recent Science, and Practical Applications
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: August 15, 2025</div>
    <div>Authors: Schollaert Uz, Stephanie; 1; Anyamba, Assaf; 2; 1</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Freely available government satellite observations enable scientists to monitor changes across the Earth system. Calibrated and validated global satellite data have advanced our understanding of interactions within and between the energy, carbon, and water cycles. In addition to answering fundamental science questions, these environmental indicators are also used to inform practical decisions relevant to agriculture, health, renewable energy, infrastructure, and more. Assimilating satellite observations into models fills data gaps and provides predictive tools. Partnering with other organizations to reach new communities and combining environmental data with sector-specific data increases its utility and societal benefit. Additionally, under the principles of open science, government agencies are working together to make useful information derived from global observations, both data and software tools, more easily accessible. This paper reviews recent NASA Earth satellite missions, highlights a few examples of science discoveries and practical applications, and describes new activities and directions.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00063" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Convolutional Neural Networks for Land Use and Land Cover Multi-class Maps from Historical Aerial Photographs
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: August 8, 2025</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Historical maps that describe past land use and land cover (LULC) forms can be a precious source of information in many scientific fields studying long-term spatial and temporal changes in the landscape. Such repositories were created manually in small areas in the past, which was a time-consuming and labor-intensive task. Recently, there has been a growing tendency to use machine learning models for this purpose, along with deep learning methods. However, having a massive amount of labeled data is necessary for these methods to train the networks. Training data are often manually labeled, posing a significant challenge and limiting the automation of these methods. This article presents a method that uses topographic databases to extract complex multi-class maps representing LULC from historical aerial photographs, eliminating the time-consuming data labeling step. The method uses transfer learning with a pretrained model on 2020 and 2014 data and attempts to reconstruct LULC types with the same convolutional neural network (CNN) network on archived images from 2006. The experiment covered 488 km² and included seven LULC classes. The method was tested using different CNN architectures (U-Net, Pyramid Scene Parsing Network [PSPNet], and LinkNet) with backbones (ResNeXt+SE, EfficientNet, and Inception). The PSPNet–EfficientNet-b7 network model achieved the best results, with 90% overall accuracy for predicting LULC classes based on the 2006 archived aerial images.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00121r2" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Thirty Years of the U.S. National Land Cover Database: Impacts and Future Direction
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: August 27, 2025</div>
    <div>Authors: Sohl, Terry; 1; Jin, Suming; 1; Dewitz, Jon; 1; Brown, Jesslyn; 1; Tollerud, Heather; 1; Wickham, James; 2; Stehman, Stephen; 3; Herold, Nathaniel; 4; Schleeweis, Karen; 5; Deering, Carol; 6</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The National Land Cover Database (NLCD), developed through the Multi-Resolution Land Characteristics Consortium, was initiated 30 years ago and has continually provided critical, Landsat-based land-cover and land-change information for the United States. Originally launched to address the lack of national-scale, moderate-resolution land-cover data, NLCD has evolved from the pioneering 1992 dataset into a comprehensive, annually updated product suite. Key innovations include the introduction of impervious surface mapping, forest canopy mapping, standardized Landsat mosaics, national-scale accuracy assessments, continual evolution of deep learning and artificial intelligence methodologies, and a transition toward operational, change-focused monitoring. The NLCD has become an essential resource for scientific research, land management, and policy development, with extensive adoption across federal, state, and local agencies; academia; and the private sector. The NLCD data underpin a wide array of applications, including biodiversity conservation, urban planning, hydrology, human health studies, and natural hazard assessment. As new global and high-resolution commercial land-cover products emerge, the NLCD continues to distinguish itself through its temporal depth, federal backing, and thematic consistency. Moving forward, the NLCD will maintain its niche as the leading, moderate-resolution, long-term land-cover and land-change dataset for the United States, ensuring continued support for broad national applications while complementing higher-resolution and global-mapping efforts.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00079r2" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Scale-adaptive Knowledge Distillation with Superpixel for Hyperspectral Image Classification
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: September 26, 2025</div>
    <div>Authors: Dong, Shuang; 1; Li, Ying; 1; Xie, Ming; 1; Han, Tingting; 2</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Hyperspectral image (HSI) classification is a critical area in remote sensing with broad applications in geoscience. While deep learning methods have gained popularity for HSI classification, their potential remains underexplored due to limited labeled data. To address this, we propose a scale-adaptive knowledge distillation with superpixel framework that trains deep neural networks using unlabeled samples. The proposed framework incorporates three core components: (1) scale-adaptive superpixel knowledge distillation, (2) bilateral spatial–spectral attention mechanisms, and (3) three-dimensional (3D) hyperspectral data transformation. The distillation module implements self-supervised learning through dynamically generated soft labels based on cross-dimensional similarity metrics. The workflow proceeds through three stages: Initially, spatial–spectral joint distance metrics evaluate the affinity between unlabeled superpixels and target classes. Subsequently, these measurements inform probabilistic soft label assignments for each superpixel cluster. Finally, an end-to-end trainable dense convolutional network with dual attention pathways is refined by optimizing the divergence between the adaptive label distributions and network predictions. Additionally, 3D transformations, including spectral and spatial rotations of the HSI cube, are applied to maximize the utility of labeled data. Experiments on three public HSI data sets demonstrate that the proposed method achieves competitive accuracy and efficiency compared to existing approaches. The implementation code is available at https://github.com/San-dow/Awnsome-SAKDS_HSI.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00038r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Efficient Irregular Texture Nesting Method via Hybrid NFP-SADE with Adaptive Container Resizing
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: September 9, 2025</div>
    <div>Authors: Wang, Xin; 1; Lou, Liyuan; 2; Li, Wenyen; 3; Yu, Jingle; 4; Zhan, Zongqian; 5</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Efficient irregular texture nesting, which is necessary for improving the efficiency of texture mapping and 3D model rendering, especially for large-scale 3D reconstruction tasks, has emerged as a critical research topic in the fields of photogrammetry, computer graphics, and computer vision. However, persistent inefficiencies and high computational costs in existing texture nesting algorithms pose significant challenges when dealing with vast quantities of irregularly shaped texture patches. To solve this problem, this work presents an efficient and well-structured texture nesting for reorganizing irregular textures in a space-efficient and time-efficient way. More specifically, a hybrid optimization approach that integrates an enhanced no-fit polygon (NFP) method with an improved simplified atavistic differential evolution (SADE) algorithm is proposed. The canonical SADE is reformulated, tailored for texture nesting optimization, and a novel self-adaptive container resizing strategy is used to surpass traditional NFP approaches in polygon processing efficiency. The experimental results demonstrate that the proposed method significantly improves irregular texture nesting efficiency, achieving speed improvements of up to 5.44 times compared with the common genetic algorithm–based method and 5.21 times over the simulated annealing–based method. Furthermore, it consistently improves space use by approximately 6.56%, indicating a more effective layout strategy and optimized resource use. Code is available at https://github.com/louliyuan/NFP-SADE-With-Adaptive-Container-Resizing.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00052r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Novel Multi-level Feature Collaborative Matching Network for Optical and Synthetic Aperture Radar Image Registration
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: September 24, 2025</div>
    <div>Authors: Pang, Bo; 1; Wang, Lei; 2; Wei, Bo; 3; Zhu, Wenlei; 2; Gao, Haiyun; 2</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Due to the complementary characteristics of synthetic aperture radar (SAR) and optical images, image registration as a prerequisite for their information fusion has received increasing attention. Currently, learning-based methods can better handle the significant radiometric and geometric differences between optical and SAR images compared to traditional registration approaches, but they still have limitations in distinguishing difficult samples, making high-precision registration a remaining challenge. To address these challenges, this paper proposes a multi-level feature collaborative matching network (MFC-Net) that effectively integrates high-level abstract features and low-level spatial features for precise registration. Furthermore, a novel dual-dimension joint attention module (DDJA) is designed to dynamically capture feature dependencies across both channel and spatial dimensions, enhancing cross-modal feature consistency and improving matching performance. Additionally, to address the problem of similarity between hard positive and negative samples caused by high-precision registration requirements, a dynamic differentiation factor is introduced at the loss function level, enabling the model to better distinguish between these similar samples in training. Extensive experiments conducted on the WHU-OPT-SAR data set and WHU-SEN-City data set demonstrate that the proposed MFC-Net outperforms state-of-the-art methods in both matching accuracy and precision, validating its superiority in cross-modal image registration tasks.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00095" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Stripe Noise Removal of ZY1-02D Hyperspectral Images Using an Improved Three-Dimensional U-Net Network
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: September 29, 2025</div>
    <div>Authors: Gao, Ruoheng; 1; Dong, Xinfeng; 2; 3; 4; Li, Na; 5; Cui, Jing; 6; Li, Tongtong; 7; Wu, Jingkai; 2; Bai, Wei; 8; Zhang, Rui; 9</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The ZY1-02D satellite, equipped with China’s first civilian hyperspectral payload, provides valuable data for remote sensing applications. However, its hyperspectral images (HSIs) are often degraded by stripe noise, significantly limiting their practical utility. Traditional denoising methods are challenged by the complex spatial and spectral characteristics inherent to HSIs, frequently resulting in compromised image quality. Fusion residual block and attention mechanism U-Net (FEA–U-Net), a novel three-dimensional destriping network, is proposed to eliminate stripe noise in hyperspectral imagery. This framework innovatively integrates cross-dimensional attention mechanisms with deep residual learning. A composite loss function combining mean squared error and spectral angle was designed to ensure spectral fidelity before and after denoising. Through systematic evaluation across varying input band numbers, the optimal network configuration was determined. When evaluated on ZY1-02D data sets, state-of-the-art performance is achieved by FEA–U-Net, demonstrating superior geological information preservation and computational efficiency. Compared with existing methods, the highest reported denoising performance was observed, with peak signal-to-noise ratio and structural similarity index reaching 48.1681 and 0.9998, respectively. Spectral curve integrity is effectively preserved, enhancing lithological classification and mineral identification accuracy in hyperspectral imagery.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00085r2" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Assessing the Accuracy of a 2021 Kansas&#8211;Oklahoma Tallgrass Ecosystem Burn Area Map
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 28, 2025</div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00091r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            RSIDetNet: An Efficient Oriented Small Object Detection Model for Remote Sensing Images Based on Cross-Scale Feature Fusion and Large Kernel Decomposition
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 22, 2025</div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00037r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Joint Detection and Parameters Regression of Detailed Windows based on Facade Textures via an Adaptive Soft Teacher
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 10, 2025</div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00081r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Landsat-Derived Rainfed and Irrigated-Area Product for Conterminous United States for the Year 2020 (LRIP30 CONUS 2020) Using Supervised and Unsupervised Machine Learning on the Cloud
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 6, 2025</div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00030r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Novel Spatiotemporal ConvLSTM-Based Cellular Automata Model for Simulating Urban Expansion
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 10, 2025</div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00093r2" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Evaluating Three-Dimensional Elevation Program Lidar Consistency and Accuracy at Scale Using Cloud-Native, Open-Source Methods
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 23, 2025</div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00035r4" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            CGMSANet: Hyperspectral Image Classification through Channel-Grouped Multi-Scale Feature Fusion and Attention Mechanisms
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 14, 2025</div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00094r4" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Lightweight YOLO-Based Algorithm for Digital Target Recognition Integrating Positional Enhancement and Morphological Processing
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 31, 2025</div>
    <div>Authors: Wang, Sheng; Zheng, Nae; Lv, Pinpin; Gao, Tian</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Target detection is vital for modern military applications, yet deploying deep learning models on resource-limited edge devices remains challenging. Existing lightweight models often exhibit poor boundary localization and low digit recognition accuracy, falling short of real-time and precision requirements. This paper introduces a lightweight YOLO-based algorithm enhanced with a novel positional mechanism and morphological processing. The key component, a position-enhanced feature pyramid network (Enhanced-FPN), fuses shallow high-resolution and deep semantic features to improve localization accuracy. A ShuffleNetv2 backbone ensures low computational overhead, while a postdetection module applies morphological processing to robustly extract digit contours and orientation. Evaluated on a custom military data set, the model achieves 49.92% mean average precision at a 50% intersection over union threshold (mAP50) at 11.24 frames/second on an edge device—improving accuracy by 7.02 points and speed by 12.8% over the Yolo-FastestV2 baseline, with a comparable 0.11 GFLOP (Giga Floating Point Operations per second) cost. These results highlight the method’s effectiveness for real-time, high-precision target recognition in constrained environments.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00016r32" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Efficient Multi-Scale Transformer Network with Fusion-Attention for Point Cloud&#8211;Semantic Segmentation in Urban Environments
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: November 11, 2025</div>
    <div>Authors: Guo, Bo; 1; Tang, Shengjun; 2; 3; Wambugu, Naftaly; 3; Wang, Ruisheng; 2; 1; Huang, Zhihai; 4; Deng, Xiaolong; 4; Hay-Man Ng, Alex; 4; Guo, Wenchao; 5</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            This article investigates point-cloud segmentation, which is crucial but challenging for scene interpretation, especially for three-dimensional (3D) urban scenes at a city scale. Compared with the previous approaches, the proposed method gains a competitive advantage by leveraging an efficient multi-scale transformer, which complements the convolution in a hierarchical network to improve the representation ability with globally contextual information. More specifically, to address the problem of quadratic complexity that hinders large-scale point-cloud processing, a lightweight attention module with linear complexity is introduced by sequentially implementing channel and spatial attention to replace quadratic dot-product attention. Based on this lightweight attention module, an encoder based on a transformer is implemented to aggregate the feature sequence within a scale into a learnable token. To improve the efficiency of integrating information of multiple scales with no inductive bias, fusion attention is proposed, using only learned tokens to calculate the query, in which the complexity of the attention map can be bounded to be linear. The fusion-attention module is embedded in the multi-scale transformer to further expand the receptive field. The proposed method extends the previous hierarchical networks of point-cloud processing by incorporating the detailed information extracted via convolution and the globally contextual information extracted by the multi-scale transformer to greatly improve the representative ability of features for the accurate segmentation of point-cloud data. Two benchmark datasets (Dayton Annotated LiDAR Earth Scan [DALES] and Toronto-3D) were used to assess the proposed method. This method achieved an improvement of approximately 1.5% in mean intersection over union for semantic segmentation on the DALES dataset compared with the state-of-the-art methods. Meanwhile, an ablation study showed that consistent improvements were mainly attributed to the wide applicability of the efficient attention mechanism for enlarging the receptive field.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00028r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Graph Neural Network&#8211;Based Land Cover&#8211;Classification of Remote Sensing Images Using Multi-Scale and Depth Features
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 1, 2025</div>
    <div>Authors: Liu, Jiexi; 1; Xie, Zhiwei; 2; Sun, Lishuang; 1; Liu, Ruizhao; 1</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Remote sensing land-cover classification can provide valuable data support for natural resource management. Existing classification methods based on graph-neural networks rely mainly on the global features and non-Euclidean structural features of image objects without considering the local features that describe their internal structures and the raster-depth features in the form of Euclidean structures. To this end, this paper presents a multi-scale and deep-feature, remote sensing–image land cover–classification method that embeds raster-depth features into node features and captures multi-scale graph-embedding information from global graphs and subgraphs to fully express image information. The depth-feature map of the image is obtained through a visual geometry Group 16–layer network and integrated into the feature space. The fractal network evolution algorithm is adopted to obtain multi-scale image objects. Global-scale features such as spectral, texture, index, and raster-depth features of the image objects are extracted, and local-scale features (e.g., average degree, average path length, graph diameter, average clustering coefficient, small-world effect) of the subgraphs are extracted to construct multi-scale depth features. The composite global graph structure is constructed by adopting adaptive weights, the graph embeddings are extracted via the graph convolutional network, and the node categories are predicted via SoftMax. For the Gaofen Image Dataset (GID-15, a public benchmark dataset for land cover classification) and the 2017 China Computer Federation Remote Sensing Image Classification Dataset (CCF 2017, released in the 2017 China Computer Federation Big Data and Computational Intelligence Contest), as compared with the traditional method that considers only the global scale and the single-graph structure, this method improves the overall accuracy by 3.83% and 3.46%, respectively, and increases the kappa coefficient by 0.0681 and 0.0637, respectively, which indicates its effectiveness.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00092r4" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Probabilistic Uncertainty-Guided Salient Object Detection in Remote Sensing Images
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 1, 2025</div>
    <div>Authors: Yang, Wei; 1; 2; Chen, Xuhui; 3; Huang, Andong; 4; Yao, Yongxiang; 2; Li, Hongli; 3; Wang, Ying; 5</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Salient object detection holds significant application value in fields such as agricultural monitoring, disaster assessment, and urban planning, providing critical support for precise decision-making. Existing deep learning–based detection methods often rely on nonlinear mappings to perform binary classification of pixels. However, considerable uncertainty often occurs in areas where objects and backgrounds are alike because of lighting variations, shadow effects, and similar object interference. This uncertainty negatively affects the detection performance, especially at pixels near the decision boundary. To address this issue, a remote sensing salient object-detection method is proposed based on probabilistic uncertainty assessment (uncertainty guided network [UGNet]). First, a multi-scale encoder–decoder framework with deep supervision is designed for the uncertainty calculation of confusing features. It uses high-level semantic features as guidance to enhance the ability to distinguish confusing features. Then, an uncertainty estimation mapping module is constructed, which uses Gaussian distribution to weight uncertain pixels, thereby improving the semantic distinction in confusing regions. A multi-scale focus fusion module is then introduced to integrate global and local information, reducing the uncertainty of multi-scale confusing features. Finally, multi-scale deep supervision is used to enhance the accuracy of salient object detection. Experimental results on two public data sets, optical remote sensing saliency detection and extended optical remote sensing saliency detection, demonstrate that the proposed UGNet outperforms 18 mainstream methods, with significantly improved detection performance. The code is available at https://github.com/OrangeCat12352/UGNet.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00108r2" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Comparison of Intertidal Terrain Extraction Methods Based on ICESat-2 and Tidal Data
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: November 4, 2025</div>
    <div>Authors: Chen, Deliang; 1; Lu, Zixuan; 1; Zhuang, Qizhi; 2; 3; 4; 5; Xiao, Jianbo; 2; 4; 3; Cheng, Liang; 2; 4; 3</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The intertidal zone is a transitional area between land and sea, characterized by both marine and terrestrial features, with rich resources in mudflats. Accurately mapping the intertidal topography and understanding its dynamic characteristics are of great significance. In this study, Sentinel-2 imagery was used, combined with tidal data and ICESat-2 data, respectively. Four methods—the waterline method, the inundation frequency method, random forest regression, and the long short-term memory (LSTM) model—were applied to extract intertidal topography in the large radial sand ridges along the Jiangsu coast. When validated with ICESat-2 data and unmanned aerial vehicle (UAV) data, the root mean square error (RMSE) values of all four methods combined with ICESat-2 data were lower than those combined with tidal data. Using ICESat-2 data for validation, the waterline method combined with ICESat-2 data achieved the lowest RMSE of 0.218 m. When validated with UAV data, the inundation frequency method combined with ICESat-2 data yielded the lowest RMSE of 0.864 m. From 2020 to 2024, the intertidal zone in this region was initially dominated by erosion, followed by deposition, ultimately reaching a dynamic equilibrium. This study achieved two objectives: (1) under identical area conditions using the same image data and two elevation data, four different methods were validated to compare their topographic extraction performance and identify the optimal approach; and (2) the optimal method was applied to generate multi-temporal topographic results of a local intertidal zone along the Jiangsu coast, analyzing terrain change in the region.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00099r4" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Optimization of Canopy Height Model Generation Parameters for Precise Forestry
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 4, 2025</div>
    <div>Authors: Płatek-Żak, Anna; 1; Bakuła, Krzysztof; 1; Marczykowska, Dorota; 1; Pilarska-Mazurek, Magdalena; 1; Kolendo, Łukasz; 2; Ksepko, Marek; 1; Koźniewski, Marcin; 3</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The usage of photogrammetric technologies is essential in the concept of precise forestry. Dense unmanned laser scanning (ULS) point clouds are the innovative and precise data source for canopy height model (CHM) generation. It is necessary to choose the CHM generation method and its settings appropriately. This study evaluated different CHM generation methods and aimed to select the optimal parameters for CHM generation based on dense ULS point clouds of a temperate forest in central Europe. The results show that the choice of method and settings influences the quality of parameters describing forest stands, such as tree height or volume, and determining the location of tree tops and 2D tree contours. The most accurate CHMs were generated using the pit-free method. This method provides the lowest differences between the reference values, which were evaluated using the proposed CHM quality index. The cell size of generated rasters had the most significant influence on the quality of CHM, regardless of the method. Among all variants, the optimal variant was selected with a spatial resolution of CHM of 20 cm and a number of height levels of 4 and no interpolation of values for areas without data. For coniferous forest, this variant has a mean tree top location error of 0.1 m, a mean tree top height error of 0.1 m, and a mean tree crown volume error of 8.5 m³. For deciduous forest, this variant has a mean tree top location error of 0.3 m, a mean tree top height error of 0.7 m, and a mean tree crown volume error of 40.8 m³.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00112r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Semantic Change Detection with Constrained Dual-Head Convolutional Neural Network Architecture for Oil/Gas Well Site Monitoring
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 1, 2025</div>
    <div>Authors: Xu, Hongzhang; 1; He, Hongjie; 2; Zhang, Ying; 3; Zhang, Dedong; 4; Li, Jonathan; 5; 4</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            High-resolution mapping of land disturbance and reclamation is important for assessing the cumulative environmental effects of oil/gas production. The growing availability of high-resolution satellite imagery, combined with recent advances in deep learning, offers a desirable solution for detecting land surface changes on disturbed land. In this study, we constructed the Alberta oil/gas wells semantic change detection (SCD) data set in Alberta, Canada, based on high-resolution satellite imagery from WorldView-2 and SPOT-6. The data set consists of 328 pairs of bitemporal images (512 × 512 pixels at 1.5-m resolution), along with corresponding semantic change maps, binary change maps, and land cover maps. In addition, we proposed a constrained dual-head convolutional neural network (CNN) framework that jointly learns semantic change and binary change tasks. Specifically, two segmentation heads are designed—one for semantic changes and one for binary changes—and are explicitly connected through a cosine similarity loss that enforces consistency between the two tasks. Taking High-Resolution Net (HRNet)-v2 as the backbone, our model was pretrained on the large-scale SEmantic Change detectiON Data Set (SECOND) and fine-tuned on our developed data set. Comparative experiments with BiSRNet, HGINet, and SCanNet demonstrate that our approach achieves superior performance, with the highest mean intersection over union (mIoU) (79.47%) and separated Kappa (SeK) (28.40%) after fine-tuning. Incorporating land cover maps as additional supervision further boosts results, with our approach reaching an mIoU of 80.05% and a SeK of 29.71%. These findings highlight the effectiveness of the proposed constrained dual-head CNN architecture and the benefit of leveraging land cover information for advancing SCD in remote sensing.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00107r2" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatiotemporal Dynamics and Drivers of Phytoplankton Bloom in Dongting Lake from 2014 to 2022
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 1, 2025</div>
    <div>Authors: Lin, Yanyan; 1; 2; Zhong, Liang; 1; Yu, Xiaolong; 3; Yu, Peng; 1; 2; Luan, Hualong; 4; Xie, Zhiying; 1; 2; Zhou, Yunxuan; 5; Zhong, Xiaojing; 6</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Phytoplankton blooms are a major global environmental issue, affecting aquatic ecosystems, aquaculture, food production, and water supply security. This study systematically investigated the spatiotemporal dynamics of phytoplankton blooms in Dongting Lake from 2014 to 2022 using Floating Algae Index (FAI) time-series data derived from Landsat imagery via the Google Earth Engine (GEE) platform. The research aimed to characterize bloom distribution patterns and assess the influence of environmental and meteorological drivers. Using multiple statistical and spatial methods—including Theil–Sen trend estimation, the Mann–Kendall test, the Hurst index, spatial autocorrelation, and geographic detector analysis—the study explored the nonlinear bivariate relationships underpinning bloom formation. Multiscale temporal analyses (daily, monthly, seasonal, and annual) provided a detailed understanding beyond conventional single-scale studies. The results indicated that algal blooms predominantly occurred in the eastern and southern regions of Dongting Lake, with lower frequency in the west. Bloom extent peaked in summer and autumn. At the daily scale, total phosphorus (TP), chlorophyll a (Chl-a), and air temperature were key promoters of bloom development, whereas total nitrogen (TN) and barometric pressure exhibited inhibitory effects. Monthly analyses revealed significant positive correlations between TN, Chl-a, air temperature, and bloom growth. On seasonal and annual scales, Chl-a concentration closely correlated with bloom intensity. The largest bloom, recorded in 2014, covered 1094.57 km2. This comprehensive analysis elucidated the spatial patterns and multi-year trends of blooms in Dongting Lake and identified seasonal hot spots, interannual variability, and recurring high-risk periods. The findings provide a critical reference for long-term monitoring, management, and risk mitigation of blooms in Dongting Lake and similar ecosystems, supporting optimized water resource management strategies.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00100r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Optimized 3D Building Mapping and Reconstruction via Cross-View Collaboration in Densely Built-Up Areas
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: November 4, 2025</div>
    <div>Authors: Tang, Shengjun; 1; Yu, Tian; 1; Xie, Linfu; 1; Wang, Weixi; 1; Guo, Renzhong; 1; Chen, Yujie; 2; Li, You; 3</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            High-precision urban three-dimensional (3D) point clouds can be effectively generated using airborne oblique photogrammetry and lidar scanning. However, due to occlusions from dense building layouts and vegetation, existing airborne acquisition methods often struggle to capture complete building geometries. This incompleteness poses serious challenges for applications such as urban planning, smart city management, and autonomous navigation, which require structurally complete and accurate 3D data. To address this limitation, this paper proposes a novel cross-view collaborative surveying framework that integrates aerial and ground-based data collection for precise and efficient 3D reconstruction of urban buildings. The framework begins by performing automated building completeness detection on aerial point clouds using a multi-layer slice projection algorithm, which enables accurate identification of missing regions in both point-wise and surface-wise forms. These detected deficiencies are then used to guide the generation of optimized ground-based supplementary acquisition routes, incorporating a global-local planning mechanism and a multi-objective technologies for autonomous robot exploration (TARE) strategy to enable autonomous and adaptive data collection. Comprehensive experiments were conducted in both simulated and real-world urban environments. Evaluation metrics focused on point cloud completeness and 3D reconstruction accuracy. The results demonstrate that the proposed method significantly enhances the completeness of building point clouds, achieving an average detection accuracy above 90%, while also reducing reconstruction error by up to 65% in complex urban scenarios. The proposed method provides a valuable tool for urban mapping professionals, autonomous systems, and digital city infrastructure developers who depend on high-quality 3D building models.
        </details>
    </div>
</article>
