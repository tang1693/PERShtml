<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00124r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Detecting Altimetric Changes in Arctic Landscapes Using Historical Aerial Imagery-Derived Digital Elevation Models (hDEMs): Case Study of the Black Mountain Alluvial Fan Complex, Canada
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 22, 2025</div>
    <div>Authors: Menio, Emma; Theiss, Hank; Cothren, Jackson</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            In the rapidly changing Arctic, reconstructing landscapes over the last 50 years is essential to understanding effects due to climate-induced geomorphic change. While region-wide warming became measurable in the 1980s, spatially extensive high-latitude elevation data sets extend temporally back to the 2000s. Historical aerial imagery archives provide data sets of high-resolution imagery from the mid- to late 1900s with stereo-capability that can be harnessed to create historical digital elevation models (hDEMs). Reconstructing a surface from the past is challenging due to a lack of ground control from that era to constrain it in space, especially at high latitudes. The main purpose of this study was to determine whether an hDEM could be used to detect altimetric change in an area of poor ground control. We developed an hDEM from historical aerial imagery over the Black Mountain alluvial fan complex in the Northwest Territories, Canada, and used satellite imagery-derived ground control points to constrain the model in space. The resulting hDEM, when compared with the ArcticDEM, yields a vertical root mean square error of 5.19 m. We were able to isolate approximately 30 to 40 m of altimetric change from a landslide (circa 2013 to 2016) in the Black Mountain Fan catchment, supporting the supervised use of hDEMs for change detection studies. Data produced from this study are available on Dryad (doi:10.5061/dryad.mw6m90691).
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00073r4" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Comparative Review of Representative Techniques in Image-Based 3D Dense Reconstruction
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 19, 2025</div>
    <div>Authors: Zhu, Minghui; Yan, Jianguo; Zhong, Jiageng; Elfadaly, Abdelaziz; Dai, Liangzhi</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            A dense model of the target scene can be generated using dense three-dimensional (3D) reconstruction techniques based on the known camera poses. Over the past two decades, numerous advanced algorithms have emerged that not only enhance the quality and accuracy of 3D reconstruction but also point to promising directions for future developments. These methods include traditional multi-view stereo (MVS), deep learning–based MVS approaches, and more recent techniques such as neural radiance fields (NeRFs), which excel in modeling complex scenes, and 3D Gaussian splatting (3DGS), which shows promise for real-time performance and high visual fidelity. Each of these techniques varies in terms of accuracy, robustness, efficiency, computational resource usage, and applicability to different types of scenes. In this paper, we provide a comprehensive review of these methods, analyze their strengths and limitations, and experimentally compare their performance under different conditions to guide future advancements in high-quality dense 3D reconstruction.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00126r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Accuracy Evaluation of Airborne and Terrestrial Point Cloud Registration
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 13, 2025</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Terrestrial laser scanners (TLS) and drones have revolutionized mapping in recent years. However, neither can fully cover large sites since scanners miss objects’ topmost parts and drones fail to capture objects’ vertical elements. Therefore, there is a need to integrate their data to provide complete coverage of sites. This study aims to inspect the accuracy of combining such terrestrial and airborne point clouds. To this end, data of three sites were collected using a TLS and a drone and processed separately. Airborne data were georeferenced using ground control points, but terrestrial data were not georeferenced intentionally so that it could be scaled, shifted, and rotated freely in 3D space to fit the airborne data more closely. Then, nonlinear least squares adjustment was applied for this coordinate transformation, and the results were assessed both visually and quantitatively. No major gap or misalignment between two data types could be visually identified after integration. For quantitative analysis, the root mean square error (RMSE), the change in flatness and orientation of some areas after data integration were computed. The results indicate the maximum RMSE, flatness, and orientation change were 0.037 m, 0.01 m, and 37 arcminutes, respectively, suggesting that integration error was within the allowable tolerance. Statistical significance testing was also applied to the obtained results indicating, with a 99% significance level, that the average flatness did not change after data integration, and the average orientation change was less than 15 arcminutes for all test areas.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00099r4" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Optimization of Canopy Height Model Generation Parameters for Precise Forestry
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 4, 2025</div>
    <div>Authors: Płatek-Żak, Anna; Bakuła, Krzysztof; Marczykowska, Dorota; Pilarska-Mazurek, Magdalena; Kolendo, Łukasz; Ksepko, Marek; Koźniewski, Marcin</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The usage of photogrammetric technologies is essential in the concept of precise forestry. Dense unmanned laser scanning (ULS) point clouds are the innovative and precise data source for canopy height model (CHM) generation. It is necessary to choose the CHM generation method and its settings appropriately. This study evaluated different CHM generation methods and aimed to select the optimal parameters for CHM generation based on dense ULS point clouds of a temperate forest in central Europe. The results show that the choice of method and settings influences the quality of parameters describing forest stands, such as tree height or volume, and determining the location of tree tops and 2D tree contours. The most accurate CHMs were generated using the pit-free method. This method provides the lowest differences between the reference values, which were evaluated using the proposed CHM quality index. The cell size of generated rasters had the most significant influence on the quality of CHM, regardless of the method. Among all variants, the optimal variant was selected with a spatial resolution of CHM of 20 cm and a number of height levels of 4 and no interpolation of values for areas without data. For coniferous forest, this variant has a mean tree top location error of 0.1 m, a mean tree top height error of 0.1 m, and a mean tree crown volume error of 8.5 m³. For deciduous forest, this variant has a mean tree top location error of 0.3 m, a mean tree top height error of 0.7 m, and a mean tree crown volume error of 40.8 m³.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00028r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Graph Neural Network&#8211;Based Land Cover&#8211;Classification of Remote Sensing Images Using Multi-Scale and Depth Features
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 1, 2025</div>
    <div>Authors: Liu, Jiexi; Xie, Zhiwei; Sun, Lishuang; Liu, Ruizhao</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Remote sensing land-cover classification can provide valuable data support for natural resource management. Existing classification methods based on graph-neural networks rely mainly on the global features and non-Euclidean structural features of image objects without considering the local features that describe their internal structures and the raster-depth features in the form of Euclidean structures. To this end, this paper presents a multi-scale and deep-feature, remote sensing–image land cover–classification method that embeds raster-depth features into node features and captures multi-scale graph-embedding information from global graphs and subgraphs to fully express image information. The depth-feature map of the image is obtained through a visual geometry Group 16–layer network and integrated into the feature space. The fractal network evolution algorithm is adopted to obtain multi-scale image objects. Global-scale features such as spectral, texture, index, and raster-depth features of the image objects are extracted, and local-scale features (e.g., average degree, average path length, graph diameter, average clustering coefficient, small-world effect) of the subgraphs are extracted to construct multi-scale depth features. The composite global graph structure is constructed by adopting adaptive weights, the graph embeddings are extracted via the graph convolutional network, and the node categories are predicted via SoftMax. For the Gaofen Image Dataset (GID-15, a public benchmark dataset for land cover classification) and the 2017 China Computer Federation Remote Sensing Image Classification Dataset (CCF 2017, released in the 2017 China Computer Federation Big Data and Computational Intelligence Contest), as compared with the traditional method that considers only the global scale and the single-graph structure, this method improves the overall accuracy by 3.83% and 3.46%, respectively, and increases the kappa coefficient by 0.0681 and 0.0637, respectively, which indicates its effectiveness.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00092r4" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Probabilistic Uncertainty-Guided Salient Object Detection in Remote Sensing Images
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 1, 2025</div>
    <div>Authors: Yang, Wei; Chen, Xuhui; Huang, Andong; Yao, Yongxiang; Li, Hongli; Wang, Ying</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Salient object detection holds significant application value in fields such as agricultural monitoring, disaster assessment, and urban planning, providing critical support for precise decision-making. Existing deep learning–based detection methods often rely on nonlinear mappings to perform binary classification of pixels. However, considerable uncertainty often occurs in areas where objects and backgrounds are alike because of lighting variations, shadow effects, and similar object interference. This uncertainty negatively affects the detection performance, especially at pixels near the decision boundary. To address this issue, a remote sensing salient object-detection method is proposed based on probabilistic uncertainty assessment (uncertainty guided network [UGNet]). First, a multi-scale encoder–decoder framework with deep supervision is designed for the uncertainty calculation of confusing features. It uses high-level semantic features as guidance to enhance the ability to distinguish confusing features. Then, an uncertainty estimation mapping module is constructed, which uses Gaussian distribution to weight uncertain pixels, thereby improving the semantic distinction in confusing regions. A multi-scale focus fusion module is then introduced to integrate global and local information, reducing the uncertainty of multi-scale confusing features. Finally, multi-scale deep supervision is used to enhance the accuracy of salient object detection. Experimental results on two public data sets, optical remote sensing saliency detection and extended optical remote sensing saliency detection, demonstrate that the proposed UGNet outperforms 18 mainstream methods, with significantly improved detection performance. The code is available at https://github.com/OrangeCat12352/UGNet.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00112r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Semantic Change Detection with Constrained Dual-Head Convolutional Neural Network Architecture for Oil/Gas Well Site Monitoring
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 1, 2025</div>
    <div>Authors: Xu, Hongzhang; He, Hongjie; Zhang, Ying; Zhang, Dedong; Li, Jonathan</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            High-resolution mapping of land disturbance and reclamation is important for assessing the cumulative environmental effects of oil/gas production. The growing availability of high-resolution satellite imagery, combined with recent advances in deep learning, offers a desirable solution for detecting land surface changes on disturbed land. In this study, we constructed the Alberta oil/gas wells semantic change detection (SCD) data set in Alberta, Canada, based on high-resolution satellite imagery from WorldView-2 and SPOT-6. The data set consists of 328 pairs of bitemporal images (512 × 512 pixels at 1.5-m resolution), along with corresponding semantic change maps, binary change maps, and land cover maps. In addition, we proposed a constrained dual-head convolutional neural network (CNN) framework that jointly learns semantic change and binary change tasks. Specifically, two segmentation heads are designed—one for semantic changes and one for binary changes—and are explicitly connected through a cosine similarity loss that enforces consistency between the two tasks. Taking High-Resolution Net (HRNet)-v2 as the backbone, our model was pretrained on the large-scale SEmantic Change detectiON Data Set (SECOND) and fine-tuned on our developed data set. Comparative experiments with BiSRNet, HGINet, and SCanNet demonstrate that our approach achieves superior performance, with the highest mean intersection over union (mIoU) (79.47%) and separated Kappa (SeK) (28.40%) after fine-tuning. Incorporating land cover maps as additional supervision further boosts results, with our approach reaching an mIoU of 80.05% and a SeK of 29.71%. These findings highlight the effectiveness of the proposed constrained dual-head CNN architecture and the benefit of leveraging land cover information for advancing SCD in remote sensing.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00107r2" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatiotemporal Dynamics and Drivers of Phytoplankton Bloom in Dongting Lake from 2014 to 2022
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: December 1, 2025</div>
    <div>Authors: Lin, Yanyan; Zhong, Liang; Yu, Xiaolong; Yu, Peng; Luan, Hualong; Xie, Zhiying; Zhou, Yunxuan; Zhong, Xiaojing</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Phytoplankton blooms are a major global environmental issue, affecting aquatic ecosystems, aquaculture, food production, and water supply security. This study systematically investigated the spatiotemporal dynamics of phytoplankton blooms in Dongting Lake from 2014 to 2022 using Floating Algae Index (FAI) time-series data derived from Landsat imagery via the Google Earth Engine (GEE) platform. The research aimed to characterize bloom distribution patterns and assess the influence of environmental and meteorological drivers. Using multiple statistical and spatial methods—including Theil–Sen trend estimation, the Mann–Kendall test, the Hurst index, spatial autocorrelation, and geographic detector analysis—the study explored the nonlinear bivariate relationships underpinning bloom formation. Multiscale temporal analyses (daily, monthly, seasonal, and annual) provided a detailed understanding beyond conventional single-scale studies. The results indicated that algal blooms predominantly occurred in the eastern and southern regions of Dongting Lake, with lower frequency in the west. Bloom extent peaked in summer and autumn. At the daily scale, total phosphorus (TP), chlorophyll a (Chl-a), and air temperature were key promoters of bloom development, whereas total nitrogen (TN) and barometric pressure exhibited inhibitory effects. Monthly analyses revealed significant positive correlations between TN, Chl-a, air temperature, and bloom growth. On seasonal and annual scales, Chl-a concentration closely correlated with bloom intensity. The largest bloom, recorded in 2014, covered 1094.57 km2. This comprehensive analysis elucidated the spatial patterns and multi-year trends of blooms in Dongting Lake and identified seasonal hot spots, interannual variability, and recurring high-risk periods. The findings provide a critical reference for long-term monitoring, management, and risk mitigation of blooms in Dongting Lake and similar ecosystems, supporting optimized water resource management strategies.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00016r32" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Efficient Multi-Scale Transformer Network with Fusion-Attention for Point Cloud&#8211;Semantic Segmentation in Urban Environments
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: November 11, 2025</div>
    <div>Authors: Guo, Bo; Tang, Shengjun; Wambugu, Naftaly; Wang, Ruisheng; Huang, Zhihai; Deng, Xiaolong; Hay-Man Ng, Alex; Guo, Wenchao</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            This article investigates point-cloud segmentation, which is crucial but challenging for scene interpretation, especially for three-dimensional (3D) urban scenes at a city scale. Compared with the previous approaches, the proposed method gains a competitive advantage by leveraging an efficient multi-scale transformer, which complements the convolution in a hierarchical network to improve the representation ability with globally contextual information. More specifically, to address the problem of quadratic complexity that hinders large-scale point-cloud processing, a lightweight attention module with linear complexity is introduced by sequentially implementing channel and spatial attention to replace quadratic dot-product attention. Based on this lightweight attention module, an encoder based on a transformer is implemented to aggregate the feature sequence within a scale into a learnable token. To improve the efficiency of integrating information of multiple scales with no inductive bias, fusion attention is proposed, using only learned tokens to calculate the query, in which the complexity of the attention map can be bounded to be linear. The fusion-attention module is embedded in the multi-scale transformer to further expand the receptive field. The proposed method extends the previous hierarchical networks of point-cloud processing by incorporating the detailed information extracted via convolution and the globally contextual information extracted by the multi-scale transformer to greatly improve the representative ability of features for the accurate segmentation of point-cloud data. Two benchmark datasets (Dayton Annotated LiDAR Earth Scan [DALES] and Toronto-3D) were used to assess the proposed method. This method achieved an improvement of approximately 1.5% in mean intersection over union for semantic segmentation on the DALES dataset compared with the state-of-the-art methods. Meanwhile, an ablation study showed that consistent improvements were mainly attributed to the wide applicability of the efficient attention mechanism for enlarging the receptive field.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00100r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Optimized 3D Building Mapping and Reconstruction via Cross-View Collaboration in Densely Built-Up Areas
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: November 4, 2025</div>
    <div>Authors: Tang, Shengjun; Yu, Tian; Xie, Linfu; Wang, Weixi; Guo, Renzhong; Chen, Yujie; Li, You</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            High-precision urban three-dimensional (3D) point clouds can be effectively generated using airborne oblique photogrammetry and lidar scanning. However, due to occlusions from dense building layouts and vegetation, existing airborne acquisition methods often struggle to capture complete building geometries. This incompleteness poses serious challenges for applications such as urban planning, smart city management, and autonomous navigation, which require structurally complete and accurate 3D data. To address this limitation, this paper proposes a novel cross-view collaborative surveying framework that integrates aerial and ground-based data collection for precise and efficient 3D reconstruction of urban buildings. The framework begins by performing automated building completeness detection on aerial point clouds using a multi-layer slice projection algorithm, which enables accurate identification of missing regions in both point-wise and surface-wise forms. These detected deficiencies are then used to guide the generation of optimized ground-based supplementary acquisition routes, incorporating a global-local planning mechanism and a multi-objective technologies for autonomous robot exploration (TARE) strategy to enable autonomous and adaptive data collection. Comprehensive experiments were conducted in both simulated and real-world urban environments. Evaluation metrics focused on point cloud completeness and 3D reconstruction accuracy. The results demonstrate that the proposed method significantly enhances the completeness of building point clouds, achieving an average detection accuracy above 90%, while also reducing reconstruction error by up to 65% in complex urban scenarios. The proposed method provides a valuable tool for urban mapping professionals, autonomous systems, and digital city infrastructure developers who depend on high-quality 3D building models.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00108r2" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Comparison of Intertidal Terrain Extraction Methods Based on ICESat-2 and Tidal Data
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: November 4, 2025</div>
    <div>Authors: Chen, Deliang; Lu, Zixuan; Zhuang, Qizhi; Xiao, Jianbo; Cheng, Liang</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The intertidal zone is a transitional area between land and sea, characterized by both marine and terrestrial features, with rich resources in mudflats. Accurately mapping the intertidal topography and understanding its dynamic characteristics are of great significance. In this study, Sentinel-2 imagery was used, combined with tidal data and ICESat-2 data, respectively. Four methods—the waterline method, the inundation frequency method, random forest regression, and the long short-term memory (LSTM) model—were applied to extract intertidal topography in the large radial sand ridges along the Jiangsu coast. When validated with ICESat-2 data and unmanned aerial vehicle (UAV) data, the root mean square error (RMSE) values of all four methods combined with ICESat-2 data were lower than those combined with tidal data. Using ICESat-2 data for validation, the waterline method combined with ICESat-2 data achieved the lowest RMSE of 0.218 m. When validated with UAV data, the inundation frequency method combined with ICESat-2 data yielded the lowest RMSE of 0.864 m. From 2020 to 2024, the intertidal zone in this region was initially dominated by erosion, followed by deposition, ultimately reaching a dynamic equilibrium. This study achieved two objectives: (1) under identical area conditions using the same image data and two elevation data, four different methods were validated to compare their topographic extraction performance and identify the optimal approach; and (2) the optimal method was applied to generate multi-temporal topographic results of a local intertidal zone along the Jiangsu coast, analyzing terrain change in the region.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00094r4" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Lightweight YOLO-Based Algorithm for Digital Target Recognition Integrating Positional Enhancement and Morphological Processing
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 31, 2025</div>
    <div>Authors: Wang, Sheng; Zheng, Nae; Lv, Pinpin; Gao, Tian</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Target detection is vital for modern military applications, yet deploying deep learning models on resource-limited edge devices remains challenging. Existing lightweight models often exhibit poor boundary localization and low digit recognition accuracy, falling short of real-time and precision requirements. This paper introduces a lightweight YOLO-based algorithm enhanced with a novel positional mechanism and morphological processing. The key component, a position-enhanced feature pyramid network (Enhanced-FPN), fuses shallow high-resolution and deep semantic features to improve localization accuracy. A ShuffleNetv2 backbone ensures low computational overhead, while a postdetection module applies morphological processing to robustly extract digit contours and orientation. Evaluated on a custom military data set, the model achieves 49.92% mean average precision at a 50% intersection over union threshold (mAP50) at 11.24 frames/second on an edge device—improving accuracy by 7.02 points and speed by 12.8% over the Yolo-FastestV2 baseline, with a comparable 0.11 GFLOP (Giga Floating Point Operations per second) cost. These results highlight the method’s effectiveness for real-time, high-precision target recognition in constrained environments.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00091r3" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            RSIDetNet: An Efficient Oriented Small Object Detection Model for Remote Sensing Images Based on Cross-Scale Feature Fusion and Large Kernel Decomposition
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 22, 2025</div>
    <div>Authors: Kang, Zizhuang; He, Bing; Luo, Wen; Fu, Ying; He, Wei; Han, Yihui; Jia, Mingquan</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Small object detection in remote sensing images is crucial for maximizing data utility, but small objects face challenges due to their limited pixel coverage, low resolution, and susceptibility to background noise. This paper proposes an orientated small object detection model for remote sensing images based on cross-scale feature fusion and large kernel decomposition. The model consists of four main components: the image feature extraction module, the multi-scale feature fusion module, the cross-fusion region proposal network for generating candidate regions, and the dual detection head for predicting target categories and rotating bounding boxes. Experiments are conducted on two datasets, SODA-A and HRSC-2016, and the results show that the proposed model improves the mean average precision (mAP) by at least 6.3% over classical 1-stage models and by at least 2.6% over classical 2-stage model. In particular, when detecting very small objects (area less than 144 pixels), the mAP value is as high as 17.2%, which is a significant improvement compared with other models, indicating that it is very effective in dealing with the difficult task of small object detection.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00035r4" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            CGMSANet: Hyperspectral Image Classification through Channel-Grouped Multi-Scale Feature Fusion and Attention Mechanisms
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: October 14, 2025</div>
    <div>Authors: Ye, Mengyan; Tan, Xicheng; Li, Chaopeng; Chaudhary, Hamza; Meng, Xiaoliang; Sha, Zongyao; Wang, Huamin; Tu, Jianguang; Chen, Chunhui; Zhong, Yanfei; Ma, Ailong</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            To address the limitations of traditional convolutional neural networks in hyperspectral image classification under sample-limited scenarios, including insufficient feature extraction and information redundancy, this paper proposes a novel network model called CGMSANet. This model replaces traditional batch normalization with group normalization (GN) and integrates it with attention mechanisms to effectively mitigate statistical bias in small-sample training, thereby enhancing the model’s generalization capability. Additionally, a dual-path attention module is designed, combining spatial dependency capture (SDC) and spectral feature weighting (SFW). SDC models cross-band global dependencies in the spatial dimension, while SFW dynamically suppresses redundant channels in the spectral dimension, thereby enhancing focus on discriminative features. Experiments on three public data sets—HongHu, LongKou, and HanChuan—demonstrate that CGMSANet achieves superior performance in terms of F1 score and classification accuracy. Specifically, on the HongHu data set, the average F1 score reaches 96.93%, with a 57% reduction in computational complexity and a 2.45× improvement in inference speed compared to the transformer-based SpectralFormer model. Ablation studies further validate the collaborative effectiveness of the GN, SDC, and SFW modules. This model provides an efficient and robust solution for few-shot hyperspectral classification tasks and shows significant potential for practical applications in agricultural monitoring and resource management. The data are available at https://github.com/YMY666yy/CGMASNet.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00095" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Stripe Noise Removal of ZY1-02D Hyperspectral Images Using an Improved Three-Dimensional U-Net Network
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: September 29, 2025</div>
    <div>Authors: Gao, Ruoheng; Dong, Xinfeng; Li, Na; Cui, Jing; Li, Tongtong; Wu, Jingkai; Bai, Wei; Zhang, Rui</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            The ZY1-02D satellite, equipped with China’s first civilian hyperspectral payload, provides valuable data for remote sensing applications. However, its hyperspectral images (HSIs) are often degraded by stripe noise, significantly limiting their practical utility. Traditional denoising methods are challenged by the complex spatial and spectral characteristics inherent to HSIs, frequently resulting in compromised image quality. Fusion residual block and attention mechanism U-Net (FEA–U-Net), a novel three-dimensional destriping network, is proposed to eliminate stripe noise in hyperspectral imagery. This framework innovatively integrates cross-dimensional attention mechanisms with deep residual learning. A composite loss function combining mean squared error and spectral angle was designed to ensure spectral fidelity before and after denoising. Through systematic evaluation across varying input band numbers, the optimal network configuration was determined. When evaluated on ZY1-02D data sets, state-of-the-art performance is achieved by FEA–U-Net, demonstrating superior geological information preservation and computational efficiency. Compared with existing methods, the highest reported denoising performance was observed, with peak signal-to-noise ratio and structural similarity index reaching 48.1681 and 0.9998, respectively. Spectral curve integrity is effectively preserved, enhancing lithological classification and mineral identification accuracy in hyperspectral imagery.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00079r2" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Scale-adaptive Knowledge Distillation with Superpixel for Hyperspectral Image Classification
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: September 26, 2025</div>
    <div>Authors: Dong, Shuang; Li, Ying; Xie, Ming; Han, Tingting</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Hyperspectral image (HSI) classification is a critical area in remote sensing with broad applications in geoscience. While deep learning methods have gained popularity for HSI classification, their potential remains underexplored due to limited labeled data. To address this, we propose a scale-adaptive knowledge distillation with superpixel framework that trains deep neural networks using unlabeled samples. The proposed framework incorporates three core components: (1) scale-adaptive superpixel knowledge distillation, (2) bilateral spatial–spectral attention mechanisms, and (3) three-dimensional (3D) hyperspectral data transformation. The distillation module implements self-supervised learning through dynamically generated soft labels based on cross-dimensional similarity metrics. The workflow proceeds through three stages: Initially, spatial–spectral joint distance metrics evaluate the affinity between unlabeled superpixels and target classes. Subsequently, these measurements inform probabilistic soft label assignments for each superpixel cluster. Finally, an end-to-end trainable dense convolutional network with dual attention pathways is refined by optimizing the divergence between the adaptive label distributions and network predictions. Additionally, 3D transformations, including spectral and spatial rotations of the HSI cube, are applied to maximize the utility of labeled data. Experiments on three public HSI data sets demonstrate that the proposed method achieves competitive accuracy and efficiency compared to existing approaches. The implementation code is available at https://github.com/San-dow/Awnsome-SAKDS_HSI.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-25-00010" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Review of NASA Earth Observations, Recent Science, and Practical Applications
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: August 15, 2025</div>
    <div>Authors: Schollaert Uz, Stephanie; Anyamba, Assaf</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Freely available government satellite observations enable scientists to monitor changes across the Earth system. Calibrated and validated global satellite data have advanced our understanding of interactions within and between the energy, carbon, and water cycles. In addition to answering fundamental science questions, these environmental indicators are also used to inform practical decisions relevant to agriculture, health, renewable energy, infrastructure, and more. Assimilating satellite observations into models fills data gaps and provides predictive tools. Partnering with other organizations to reach new communities and combining environmental data with sector-specific data increases its utility and societal benefit. Additionally, under the principles of open science, government agencies are working together to make useful information derived from global observations, both data and software tools, more easily accessible. This paper reviews recent NASA Earth satellite missions, highlights a few examples of science discoveries and practical applications, and describes new activities and directions.
        </details>
    </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/content/asprs/pers/pre-prints/content-24-00122" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Integration of Near-Proximal and Proximal Lidar Sensing for Fine-Resolution Forest Inventory
        </a>
    </h3>
    <div style="font-style: italic;">Avaliable online: June 27, 2025</div>
    <div>Authors: Zhao, Chunxi; Hanafy, Hazem; Eissa, Aser M; Hany, Youssef; Habib, Ayman; Shao, Jinyuan; Fei, Songlin</div>
    <div>
        Abstract:
        <details>
            <summary style="color: #1b5faa;">Read more...</summary>
            Near-proximal and proximal light detection and ranging (lidar) systems are increasingly used for high-resolution forest inventory. Near-proximal lidar systems, such as those onboard uncrewed aerial vehicles (UAVs), offer high absolute positional accuracy due to continuous global navigation satellite system (GNSS) signal accessibility. Proximal lidar systems, such as backpack-based mobile mapping, excel at capturing detailed under-canopy information, including forest floor, tree trunks, and debris. However, each system has limitations when used alone. UAV lidar encounters challenges in under-canopy data collection due to occlusions and longer sensor-to-object distance, while proximal lidar faces GNSS signal outages and incomplete top canopy scanning in dense forest areas. This study proposes an approach to integrate near-proximal (UAV) and proximal (backpack) lidar data for fine-resolution forest inventory. Specifically, a framework is presented for integrating both data sources to improve the trajectory of the backpack system and establish an inventory pipeline for individual tree detection/localization, height, and diameter at breast height (DBH) estimation. The experimental results show great potential in generating high-quality, georeferenced point clouds across northern red oak plantation and coniferous forest scenarios. For northern red oak plantation inventory, the proposed pipeline achieved a 100% F1 score in tree detection and stem mapping using integrated lidar data, with root mean square errors (RMSEs) of 3 cm and 2 m for DBH and tree height estimation, respectively, when compared to field measurements. For the mixed coniferous forest inventory, due to the lack of field reference, the integrated UAV and backpack data set have been used to manually count the trees and establish their heights. The pipeline results are compared with those derived manually. The results show that UAV data have the lowest tree-detection accuracy, with an 81.06% F1 score, while proximal and integrated data excel in stem mapping, with a 91.92% F1 score. For the estimated height results, the near-proximal and integrated data have an RMSE value of almost 1.8 m, while the backpack has an RMSE value of 2.25 m. These results demonstrate the advantage of near-proximal/proximal data integration for best evaluation of tree detection, localization, and DBH/height estimation.
        </details>
    </div>
</article>
