
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 06 - Year 2020</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 06 - Year 2020</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2020/00000086/00000006" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2020/00000086/00000006/art00013;jsessionid=3mama5697diea.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Trajectory Drift–Compensated Solution of a Stereo RGB-D Mapping System
        </a>
    </h3>
    <div style="font-style: italic;">202006, pp. 359-372(14)</div>
    <div>Authors: Tang, Shengjun; Zhu, Qing; Li, You; Chen, Wu; Wu, Bo; Guo, Renzhong; Li, Xiaoming; Wang, Chisheng; Wang, Weixi</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Multiple sensors are commonly used for three-dimensional (3D)-mapping or robotic-vision applications, as they provide a larger field of view and sufficient observations to fulfill frame-registration and map-updating tasks. However, the data sequences generated by multiple sensors can be inconsistent and contain significant time drift. In this paper, we describe the trajectory drift–compensated strategy that we designed to eliminate the influence of time drift between sensors, remove the inconsistency between the sequences from various sensors, and thereby generate a coarse-to-fine procedure for robust camera-tracking based on two-dimensional–3D observations from stereo sensors. We present the mathematical analysis of the iterative optimizations for pose tracking in a stereo red, green, blue plus depth (RGB-D) camera. Finally, complex indoor scenario experiments demonstrate the efficiency of the proposed stereoRGB-Dsimultaneous localization and mapping solution. The results verify that the proposed stereoRGB-Dmapping solution effectively improves the accuracies of both camera-tracking and 3D reconstruction.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2020/00000086/00000006/art00014;jsessionid=3mama5697diea.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            The Geometric Imaging Model for High-Resolution Optical Remote Sensing Satellites Considering Light Aberration and Atmospheric Refraction Errors
        </a>
    </h3>
    <div style="font-style: italic;">202006, pp. 373-382(10)</div>
    <div>Authors: Wang, Mi; Zhu, Ying; Wang, Yanli; Cheng, Yufeng</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                With advances in satellite maneuvering imaging capability, stereoscopic images with large roll and pitch angles can be captured to improve the efficiency of observations. At the same time, the influences of light aberration and atmospheric refraction on image positioning accuracy will be more significant. However, these errors are not accounted for in the traditional imaging and calibration model for optical agile satellites. In this study, the formation mechanisms of the aberration and atmospheric refraction errors in optical remote sensing satellite Earth observation imaging were analyzed quantitatively, and correction models were constructed. From this, the traditional geometric imaging model was refined by introducing a correction model for aberration and atmospheric refraction errors to create a more comprehensive geometric imaging model. The feasibility of an extended rational function model, based on the constructed more comprehensive geometric imaging model, was verified quantitatively.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2020/00000086/00000006/art00015;jsessionid=3mama5697diea.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Using 250-m MODIS Data for Enhancing Spatiotemporal Fusion by Sparse Representation
        </a>
    </h3>
    <div style="font-style: italic;">202006, pp. 383-392(10)</div>
    <div>Authors: Wang, Liguo; Wang, Xiaoyi; Wang, Qunming</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Spatiotemporal fusion is an important technique to solve the problem of incompatibility between the temporal and spatial resolution of remote sensing data. In this article, we studied the fusion of Landsat data with fine spatial resolution but coarse temporal resolution and Moderate Resolution Imaging Spectroradiometer (MODIS) data with coarse spatial resolution but fine temporal resolution. The goal of fusion is to produce time-series data with the fine spatial resolution of Landsat and the fine temporal resolution of MODIS. In recent years, learning-based spatiotemporal fusion methods, in particular the sparse representation-based spatiotemporal reflectance fusion model (SPSTFM), have gained increasing attention because of their great restoration ability for heterogeneous landscapes. However, remote sensing data from different sensors differ greatly on spatial resolution, which limits the performance of the spatiotemporal fusion methods (including SPSTFM) to some extent. In order to increase the accuracy of spatiotemporal fusion, in this article we used existing 250-m MODISbands (i.e., red and near-infrared bands) to downscale the observed 500-m MODIS bands to 250 m before SPTSFM-based fusion of MODIS and Landsat data. The experimental results show that the fusion accuracy of SPTSFM is increased when using 250-m MODIS data, and the accuracy of SPSTFM coupled with 250-m MODIS data is greater than the compared benchmark methods.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 06 - Year 2020</p>
        </footer>
    </body>
    </html>
    