<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2021/00000087/00000012/art00011;jsessionid=2v5wf1ht3p798.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Semi-Automated Roller Parameters Extraction from Terrestrial Lidar
        </a>
    </h3>
    <div style="font-style: italic;">202112, pp. 879-890(12)</div>
    <div>Authors: Deshpande, Sagar S.; Falk, Mike; Plooster, Nathan</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Rollers are an integral part of a hot-rolling steel mill. They transport hot metal from one end of the mill to another. The quality of the steel highly depends on the surface quality of the rollers. This paper presents semi-automated methodologies to extract roller parameters from terrestrial lidar points. The procedure was divided into two steps. First, the three-dimensional points were converted to a two-dimensional image to detect the extents of the rollers using fast Fourier transform image matching. Lidar points for every roller were iteratively fitted to a circle. The radius and center of the fitted circle were considered as the average radius and average rotation axis of the roller, respectively. These parameters were also extracted manually and were compared to the measured parameters for accuracy analysis. The proposed methodology was able to extract roller parameters at millimeter level. Erroneously identified rollers were identified by moving average filters. In the second step, roller parameters were determined using the filtered roller points. Two data sets were used to validate the proposed methodologies. In the first data set, 366 out of 372 rollers (97.3%) were identified and modeled. The second, smaller data set consisted of 18 rollers which were identified and modelled accurately.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2021/00000087/00000012/art00012;jsessionid=2v5wf1ht3p798.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            UAV Remote Sensing Assessment of Crop Growth
        </a>
    </h3>
    <div style="font-style: italic;">202112, pp. 891-899(9)</div>
    <div>Authors: Dorbu, Freda Elikem; Hashemi-Beni, Leila; Karimoddini, Ali; Shahbazi, Abolghasem</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The introduction of unmanned-aerial-vehicle remote sensing for collecting high-spatial- and temporal-resolution imagery to derive crop-growth indicators and analyze and present timely results could potentially improve the management of agricultural businesses and enable farmers to apply appropriate solution, leading to a better food-security framework. This study aimed to analyze crop-growth indicators such as the normalized difference vegetation index (NDVI), crop height, and vegetated surface roughness to determine the growth of corn crops from planting to harvest. Digital elevation models and orthophotos generated from the data captured using multispectral, red/green/blue, and near-infrared sensors mounted on an unmanned aerial vehicle were processed and analyzed to calculate the various crop-growth indicators. The results suggest that remote sensing-based growth indicators can effectively determine crop growth over time, and that there are similarities and correlations between the indicators.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2021/00000087/00000012/art00013;jsessionid=2v5wf1ht3p798.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            MSegnet, a Practical Network for Building Detection from High Spatial Resolution Images
        </a>
    </h3>
    <div style="font-style: italic;">202112, pp. 901-906(6)</div>
    <div>Authors: Yu, Bo; Chen, Fang; Dong, Ying; Wang, Lei; Wang, Ning; Yang, Aqiang</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Building detection in big earth data by remote sensing is crucial for urban development. However, improving its accuracy remains challenging due to complicated background objects and different viewing angles from various remotely sensed images. The hereto proposed methods predominantly focus on multi-scale feature learning, which omits features in multiple aspect ratios. Moreover, postprocessing is required to refine the segmentation performance. We propose modified semantic segmentation (MSegnet), a single-shot semantic segmentation model based on a matrix of convolution layers to extract features in multiple scales and aspect ratios. MSegnet consists of two modules: backbone feature learning and matrix convolution to conduct vertical and horizontal learning. The matrix convolution comprises a set of convolution operations with different aspect ratios. MSegnet is applied to a public building data set that is widely used for evaluation and shown to achieve satisfactory accuracy, compared with the published single-shot methods.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2021/00000087/00000012/art00014;jsessionid=2v5wf1ht3p798.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Estimation of Rock Characteristics Based on Polarization Spectra: Surface Roughness, Composition, and Density
        </a>
    </h3>
    <div style="font-style: italic;">202112, pp. 907-912(6)</div>
    <div>Authors: Zhang, Feizhou; Liu, Xufang; Xiang, Yun; Zhang, Zihan; Liu, Siyuan; Yan, Lei</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Surface polarization characteristics provide crucial structural information of the Earth's surface. As two key elements of the natural geographical environment, rocks and soils play an important role in the study of surface processes. Inherent surface characteristics, such as surface roughness, composition, and density are critical parameters for the remote monitoring of land surfaces as they affect the polarization characteristics of scattered light waves. In this study, we investigated the relationship between surface roughness, composition, and density, and the polarization spectra of limestone-dolomite series rock. Results reveal a power function relationship between the surface roughness and the degree of polarization peaks among different detection zenith and azimuth angles. The depth and position of the absorbing waveband are significantly correlated with the characteristic component contents. The rock density was determined via the polarized reflection spectra, with the Earth's surface density calculations associated with a 2.6% divergence from the current recognized data. Our results demonstrate the ability of polarized spectra to retrieve surface roughness, composition, and density, with potential for further development in future work.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2021/00000087/00000012/art00015;jsessionid=2v5wf1ht3p798.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Automatic Registration of Mobile Mapping System Lidar Points and Panoramic-Image Sequences by Relative Orientation Model
        </a>
    </h3>
    <div style="font-style: italic;">202112, pp. 913-922(10)</div>
    <div>Authors: Zhu, Ningning; Yang, Bisheng; Dong, Zhen; Chen, Chi; Huang, Xia; Xiao, Wen</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                To register mobile mapping system (MMS) lidar points and panoramic-image sequences, a relative orientation model of panoramic images (PROM) is proposed. The PROM is suitable for cases in which attitude or orientation parameters are unknown in the panoramic-image sequence. First, feature points are extracted and matched from panoramic-image pairs using the SURF algorithm. Second, these matched feature points are used to solve the relative attitude parameters in the PROM. Then, combining the PROM with the absolute position and attitude parameters of the initial panoramic image, the MMS lidar points and panoramic-image sequence are registered. Finally, the registration accuracy of the PROM method is assessed using corresponding points manually selected from the MMSlidar points and panoramic-image sequence. The results show that three types of MMSdata sources are registered accurately based on the proposed registration method. Our method transforms the registration of panoramic images and lidar points into image feature-point matching, which is suitable for diverse road scenes compared with existing methods.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2021/00000087/00000012/art00016;jsessionid=2v5wf1ht3p798.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Dense Bathymetry in Turbid Coastal Zones Using Airborne Hyperspectral Images
        </a>
    </h3>
    <div style="font-style: italic;">202112, pp. 923-927(5)</div>
    <div>Authors: Vargas, Steven Martinez; Delrieux, Claudio; Blanco, Katy L.; Vitale, Alejandro</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                We used airborne hyperspectral images to generate a dense survey of bathymetric data in the Bahía Blanca estuary (Buenos Aires Province, Argentina). This estuarine area is characterized by intense sediment transport turning the water muddy, and thus optical bathymetric estimations are difficult. We used 24 spectral bands in a range of 500–900 nm acquired with a hyperspectral camera aboard an unmanned aerial vehicle, together with 100 bathymetry data points surveyed with a sonar sensor aboard an unmanned surface vehicle, covering an area of about 800 m2. Random-forest and support-vector-machine regressors were trained with this data set. The resulting model yielded a determination coefficient of 0.815 with unseen data, a root-mean-square error of 0.166 m, and an absolute average error less than 2%. These results allow dense and accurate reconstructions of the underwater profile in wide, muddy, shallow regions of the Bahía Blanca estuary, showing the feasibility of hyperspectral imagery combined with sonar data in turbid shallow waters.
            </details>
        </div>
</article>
