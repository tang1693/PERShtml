
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 10 - Year 2024</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 10 - Year 2024</h1>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000010/art00008;jsessionid=27o0mw2ek145f.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Attention Heat Map-Based Black-Box Local Adversarial Attack for Synthetic Aperture Radar Target Recognition
        </a>
    </h3>
    <div style="font-style: italic;">202410, pp. 601-609(9)</div>
    <div>Authors: Wan, Xuanshen; Liu, Wei; Niu, Chaoyang; Lu, Wanjie</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Synthetic aperture radar (SAR) automatic target recognition (ATR) models based on deep neural networks (DNNs) are susceptible to adversarial attacks. In this study, we proposed an SAR black-box local adversarial attack algorithm named attention heat map- based black-box local adversarial attack (AH-BLAA). First, we designed an attention heat map extraction module combined with the layer-wise relevance propagation (LRP) algorithm to obtain the high concerning areas of the SAR-ATR models. Then, to gener- ate SAR adversarial attack examples, we designed a perturbation generator module, introducing the structural dissimilarity (DSSIM) metric in the loss function to limit image distortion and the dif- ferential evolution (DE) algorithm to search for optimal perturba- tions. Experimental results on the MSTAR and FUSAR-Ship datasets showed that compared with existing adversarial attack algorithms, the attack success rate of the AH-BLAA algorithm increased by 0.63% to 33.59% and 1.05% to 17.65%, respectively. Moreover, the low- est perturbation ratios reached 0.23% and 0.13%, respectively.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000010/art00010;jsessionid=27o0mw2ek145f.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Exploring the Potential of the Hyperspectral Remote Sensing Data China OrbitaZhuhai-1in Land Cover Classification
        </a>
    </h3>
    <div style="font-style: italic;">202410, pp. 611-619(9)</div>
    <div>Authors: Li, Caixia; Xiong, Xiaoyan; Wang, Lin; Li, Yunfan; Wang, Jiaqi; Zhang, Xiaoli</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Responding to the shortcomings of China's civil remote sensing data in land cover classification, such as the difficulty of data acquisition and the low utilization rate, we used Landsat-8, China Orbita Zhuhai-1 hyperspectral remote sensing (OHS) data, and Landsat-8 + OHS data combined with band (red, green, and blue) and vegetation index features to classify land cover using maximum likelihood (ML), Mahalanobis distance (MD), and support vector machine (SVM). The results show that Landsat-8 + OHS data have the highest classification accuracy in SVM, with an overall accuracy of 83.52% and a kappa coefficient of 0.71, and this result is higher than that of Landsat-8 images and OHS images separately. In addition, the classification accuracy of OHS images was higher than that of Landsat-8 images. The results of the study provide a reference for the use of civil satellite remote sensing data in China.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000010/art00012;jsessionid=27o0mw2ek145f.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Teacher-Student Prototype Enhancement Network for a Few-Shot Remote Sensing Scene Classification
        </a>
    </h3>
    <div style="font-style: italic;">202410, pp. 621-630(10)</div>
    <div>Authors: Zhu, Ye; Yang, Shanying; Yu, Yang</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Few-shot remote sensing scene classification identifies new classes from limited labeled samples where the great challenges are intraclass diversity, interclass similarity, and limited supervision. To alleviate these problems, a teacher-student prototype enhancement network is proposed for a few-shot remote sensing scene classification. Instead of introducing an attentional mechanism in mainstream studies, a prototype enhancement module is recommended to adaptively select high-confidence query samples, which can enhance the support prototype representations to emphasize intraclass and interclass relationships. The construction of a few-shot teacher model generates more discriminative predictive representations with inputs from many labeled samples, thus providing a strong supervisory signal to the student model and encouraging the network to achieve accurate classification with a limited number of labeled samples. Extensive experiments of four public datasets, including NWPU-remote sens ing image scene classification (NWPU-RESISC45), aerial image dataset (AID), UC Merced, and WHU-RS19, demonstrate that this method achieves superior competitive performance than the state-of-the-art methods on five-way, one-shot, and five-shot classifications.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000010/art00013;jsessionid=27o0mw2ek145f.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Bank Line Extraction by Integration of Orthoimages and Lidar Digital Elevation Model Using Principal Component Analysis and Alpha Matting
        </a>
    </h3>
    <div style="font-style: italic;">202410, pp. 631-638(8)</div>
    <div>Authors: Deshpande, Sagar S.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Riverbank lines change over time, causing loss of land and property. Accurate mapping of riverbank lines is essential for restoration and preservation. This paper presents a method to map riverbank lines by combining georeferenced orthoimages and lidar digital elevation model (DEM). This method uses the properties that lidar can provide elevations under trees and open water edges are visible in orthoimages to extract the planimetric locations of bank lines. The orthoimage pixels with less than 0.15% slope on the DEM were replaced by water pixels. Principal component analysis (PCA) was conducted using DEM, slope, and orthoimage bands. Training data of river body and the background were identified manually on the first three component images. An alpha matting–based method was implemented using the training data to extract the bank lines. Bankline using α value of 50% probability were statistically and visually better when compared to the manual bank lines.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 10 - Year 2024</p>
        </footer>
    </body>
    </html>
    