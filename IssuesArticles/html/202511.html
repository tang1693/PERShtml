
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 11 - Year 2025</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 11 - Year 2025</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000011" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000011/art00008;jsessionid=5mho1n1q3hadi.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Efficient Irregular Texture Nesting Method via Hybrid NFP-SADE with Adaptive Container Resizing
        </a>
    </h3>
    <div style="font-style: italic;">202511, pp. 681-691(11)</div>
    <div>Authors: Lou, Liyuan; Li, Wanyun; Yu, Jingle; Wang, Xin; Zhan, Zongqian</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/11/An Efficient Irregular Texture Nesting Method via Hybrid NFP-SADE with Adaptive Container Resizing.jpeg" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Efficient irregular texture nesting, which is necessary for improving the efficiency of texture mapping and 3D model rendering, especially for large-scale 3D reconstruction tasks, has emerged as a critical research topic in the fields of photogrammetry, computer graphics, and computer vision. However, persistent inefficiencies and high computational costs in existing texture nesting algorithms pose significant challenges when dealing with vast quantities of irregularly shaped texture patches. To solve this problem, this work presents an efficient and well structured texture nesting for reorganizing irregular textures in a space efficient and time efficient way. More specifically, a hybrid optimization approach that integrates an enhanced no fit polygon (NFP) method with an improved simplified atavistic differential evolution (SADE) algorithm is proposed. The canonical SADE is reformulated, tailored for texture nesting optimization, and a novel self-adaptive container resizing strategy is used to surpass traditional NFP approaches in polygon processing efficiency. The experimental results demonstrate that the proposed method significantly improves irregular texture nesting efficiency, achieving speed improvements of up to 5.44 times compared with the common genetic algorithm–based method and 5.21 times over the simulated annealing–based method. Furthermore, it consistently improves space use by approximately 6.56%, indicating a more effective layout strategy and optimized resource use. Code is available at https:// github. com/louliyuan/NFP-SADE-With-Adaptive-Container-Resizing.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000011/art00010;jsessionid=5mho1n1q3hadi.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Thermal Point Cloud Generation and Evaluation from Uncalibrated Unmanned Aerial Vehicle–Visible and Infrared Camera Images Using Position and Orientation System Prior
        </a>
    </h3>
    <div style="font-style: italic;">202511, pp. 693-702(10)</div>
    <div>Authors: Liu, Siqi; Wang, Qiang; Fan, Shenghong; Cui, Ximin; Zou, Yue; Liang, Yubin; Wang, Qian</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/11/Thermal Point Cloud Generation and Evaluation from Uncalibrated Unmanned Aerial Vehicle–Visible and Infrared Camera Images Using Position and Orientation System Prior.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper presents a methodology for thermal infrared (TIR) point cloud acquisition, leveraging information from the visible camera, TIR camera, and their respective unmanned aerial vehicle (UAV) position and their position and orientation systems (POSs) prior. First, the visible images and their corresponding UAV POS information are used to generate a red–green–blue (RGB) dense point cloud with geographic coordinates; subsequently, using the RGB dense point cloud as a reference, the acquisition of a calibration parameter between global navigation satellite system/inertial measurement unit and the TIR camera is conducted using the POS information corresponding to the selected TIR images; finally, a TIR point cloud of the entire survey area is generated based on the calibration parameters. The availability and suitability of the fused TIR point cloud generated and UAV POS data are also evaluated in terms of localization deviations and orientation deviations. The experimental results demonstrate that the acquisition of coarse quality TIR point cloud can be achieved by using a priori POS data provided by different small UAVs. The TIR point cloud generated by the UAV POS data guidance can be directly applied to medium and large-sized buildings. The POS guided projected images achieved an 86.67% registration success rate with the TIR images, indicating that the orientation deviations are within acceptable limits and establishing a solid foundation for subsequent fine fusion.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000011/art00012;jsessionid=5mho1n1q3hadi.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Landsat-Derived Rainfed and Irrigated-Area Product for Conterminous United States for the Year 2020 (LRIP30 CONUS 2020) Using Supervised and Unsupervised Machine Learning on the Cloud
        </a>
    </h3>
    <div style="font-style: italic;">202511, pp. 703-714(12)</div>
    <div>Authors: Teluguntla, Pardhasaradhi; Thenkabail, Prasad S.; Oliphant, Adam; Aneece, Itiya; Biggs, Trent; Gumma, Murali Krishna; Foley, Daniel; McCormick, Richard; Neelam, Rohitha; Long, Emerson; Lawton, Jake</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/11/Landsat-Derived Rainfed and Irrigated-Area Product for Conterminous United States for the Year 2020 LRIP30 CONUS 2020 Using Supervised and Unsupervised Machine Learning on the Cloud.jpg" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Accurate maps of irrigated and rainfed croplands are crucial for assessing global food and water security. Irrigated croplands yield two to four times more grain and biomass than rainfed croplands. To meet rising food demand, the proportion of cropland that is irrigated must be increased globally. Because agriculture uses 80% to 90% of global fresh water, understanding changes in cropland extent, crop type, and irrigation is critical for meeting nutritional needs sustainably. The United States has one of the most productive rainfed and irrigated croplands in the world and is a leading producer and exporter of agricultural crops. Precise maps of irrigated and rainfed croplands in the United States are crucial for assessing the current and the future agricultural production capacity in supporting food security. We developed a 30-m resolution rainfed and irrigated area map for the conterminous United States derived from 2019 to 2021 multi-date Landsat-8 data (LRIP30 CONUS 2020). A total of 96 harmonized spectral bands comprising monthly median value composites of eight bands (blue, green, red, NIR, SWIR1, SWIR2, TIR, and enhanced vegetation index [EVI]) were used. A cropland mask was then applied, and reference data were sourced from various sources. A pixel based supervised random forest classifier, and pixel based unsupervised ISODATA clustering classifier were implemented on Google Earth Engine and the ERDAS Imagine workstation to classify, identify, map, and assess accuracies of irrigated and rainfed cropland areas. The LRIP30 CONUS 2020 product achieved an overall accuracy of 93.9%. The irrigated and rainfed classes had producer's accuracies of 90.2% and 95.7%, respectively, and user's accuracies of 90.8% and 95.4%, respectively. The total net cropland area was estimated at 139.4 million hectares (Mha), of which 94.9 Mha (68%) was classified as rainfed and 44.5 Mha (32%) was classified as irrigated. State level summaries highlight regional differences and their implications for national and global food and water security.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000011/art00013;jsessionid=5mho1n1q3hadi.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Novel Spatiotemporal ConvLSTM-Based Cellular Automata Model for Simulating Urban Expansion
        </a>
    </h3>
    <div style="font-style: italic;">202511, pp. 715-726(12)</div>
    <div>Authors: Zhou, Ye; Qiu, Yu; Wu, Tao; Lv, Laishui</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/11/Novel Spatiotemporal ConvLSTM-Based Cellular Automata Model for Simulating Urban Expansion.jpg" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Accurately capturing and simulating the spatiotemporal dynamics of urban expansion remains a persistent challenge in urban planning and environmental management. Traditional machine learning methods often struggle to capture the complexity of spatial features and the multi-scale spatiotemporal dynamics involved in deriving the transition rules of cellular automata (CA), making it difficult to balance detailed local expansion with broader overall trends during the urban expansion simulation process. To address these challenges, we propose a novel spatiotemporal CA model based on the convolutional long short-term memory (ConvLSTM) neural network. This model, termed ST-ConvLSTM-CA, simultaneously enhances critical spatial features and incorporates multi-scale spatiotemporal features by integrating the shuffle attention mechanism and a spatiotemporal expression (SE) block into the ConvLSTM architecture. First, the channel dimensions of spatial features are grouped and channel spatial attention is performed in parallel, allowing for efficient fusion and weighting of key spatial features with minimal computational cost. Second, the SE module is used to integrate the multi-scale information within ConvLSTM cells, enhancing the representation of neighborhood details across different spatial granularities and capturing dynamic temporal features. Finally, the improved model generates conversion probability maps, which are used by the CA model to simulate time series urban expansion maps. Using the urban agglomerations of the Guangdong-Hong Kong-Macao Greater Bay Area, the Yangtze River Delta, and Beijing-Tianjing-Tangshan as case studies, results show that ST-ConvLSTM-CA achieves the highest accuracy across all three regions. This demonstrates its robust adaptability to complex spatiotemporal data and superior predictive performance.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 11 - Year 2025</p>
        </footer>
    </body>
    </html>
    