<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2014/00000080/00000006/art00001;jsessionid=2148pfs7i89b9.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            On-Orbit Geometric Calibration of the Panchromatic/Multispectral Camera of the ZY-1 02C Satellite based on Public Geographic Data
        </a>
    </h3>
    <div style="font-style: italic;">201406, nan</div>
    <div>Authors: Tao, Pengjie; Lu, Luping; Zhang, Yong; Xu, Biao; Zou, Songbai</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper describes the theory and method of on-orbit geometric calibration of the panchromatic/multispectral (P/MS) camera of the ZY-1 02C satellite. Instead of using a high quality test field specifically designed for camera calibration purpose, this study intends to make a use of readily available, public geographic data as the reference. Such public data may include Google Earthâ„¢ images, Bing Map images, SRTM DEM, and ASTER GDEM. For comparison purpose, the standard national digital orthophoto maps (DOM) and digital elevation models (DEM) are also utilized. To carry out the calibration, ground control points (GCPs) are selected by matching the satellite images to the reference data. It is shown that the calibration corrects large amounts of systematic errors caused by significant lens distortion and poor direct georeferencing. After calibration, the interior accuracy of the images is improved from a range of six to eight pixels to sub-pixel level, and the direct georeferencing accuracy is increased from 1.8 km to 115 m. The study demonstrates the necessity of camera calibration and the value of public geographic data for this need.
            </details>
        </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2014/00000080/00000006/art00002;jsessionid=2148pfs7i89b9.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Performance Evaluation of Object-based and Pixel-based Building Detection Algorithms from Very High Spatial Resolution Imagery
        </a>
    </h3>
    <div style="font-style: italic;">201406, pp. 519-528(10)</div>
    <div>Authors: Khosravi, Iman; Momeni, Mehdi; Rahnemoonfar, Maryam</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper reviews and evaluates four building extraction algorithms including two pixel-based and two object-based methods using a diverse set of very high spatial resolution imagery. The applied images are chosen from different places (the cities of Isfahan, Tehran, and Ankara) and different sensors (QuickBird and GeoEye-1), which are diverse in terms of building shape, size, color, height, alignment, brightness, and density. The results indicate that the performance and the reliability of two object-based algorithms are better than pixel-based algorithms; about 10 percent to 15 percent better for the building detection rate and 6 percent to 10 percent better for the reliability rate. However, in some cases, the detection rate of pixel-based algorithms has been greater than 80 percent, which is a satisfactory result. On the other hand, segmentation errors can cause limitations and errors in the object-based algorithms, so that the commission error of object-based algorithms has been higher than pixel-based algorithms in some cases.
            </details>
        </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2014/00000080/00000006/art00003;jsessionid=2148pfs7i89b9.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Effective Morphological Index in Automatic Recognition of Built-up Area Suitable for High Spatial Resolution Images as ALOS and SPOT Data
        </a>
    </h3>
    <div style="font-style: italic;">201406, pp. 529-536(8)</div>
    <div>Authors: Wang, Li; Yu, Bo; Niu, Zheng; Shakir, Muhammad</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Building detection from remote sensed images is the main technique to monitor economic or environmental development of an area. Advanced Land Observing Satellite (ALOS) and SPOT data are reliable sources due to the limitation of weather, position, time, and other practical reasons. However, to the best of our knowledge, algorithms proposed in the identification of buildings mostly aim only at images with very high spatial resolution or high spectral resolution. There are few algorithms for detecting buildings from ALOS and SPOT data. A built-up detection index (BDI) is proposed in this paper to automatically identify buildings from images with 10 meters resolution. It synthesizes morphological theory and normalized differential vegetation index (NDVI) to enhance buildings by suppressing vegetation. Four images of ALOS and SPOT are used to verify the efficiency, stability and accuracy of BDI. Experiments show that BDI is suitable to detect buildings from 10 meters resolution with reliable accuracy.
            </details>
        </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2014/00000080/00000006/art00004;jsessionid=2148pfs7i89b9.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Annual Crop Type Classification of the US Great Plains for 2000 to 2011
        </a>
    </h3>
    <div style="font-style: italic;">201406, pp. 537-549(13)</div>
    <div>Authors: Howard, Daniel M.; Wylie, Bruce K.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The purpose of this study was to increase the spatial and temporal availability of crop classification data. In this study, nearly 16.2 million crop observation points were used in the training of the US Great Plains classification tree crop type model (CTM). Each observation point was further defined by weekly Normalized Difference Vegetation Index, annual climate, and a number of other biogeophysical environmental characteristics. This study accounted for the most prevalent crop types in the region, including, corn, soybeans, winter wheat, spring wheat, cotton, sorghum, and alfalfa. Annual CTM crop maps of the US Great Plains were created for 2000 to 2011 at a spatial resolution of 250 meters. The CTM achieved an 87 percent classification success rate on 1.8 million observation points that were withheld from model training. Product validation was performed on greater than 15,000 county records with a coefficient of determination of R2= 0.76.
            </details>
        </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2014/00000080/00000006/art00005;jsessionid=2148pfs7i89b9.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            OBIA Flood Delimitation Assisted by Threshold Determination with Principal Component Analysis
        </a>
    </h3>
    <div style="font-style: italic;">201406, pp. 551-557(7)</div>
    <div>Authors: Roque, Dora; Afonso, Nuno; Fonseca, Ana M.; Heleno, Sandra</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Accurate and rapidly mapped flood boundaries are extremely important for emergency management operations and hydraulic flood model calibration. This study presents a methodology for automatic flood delimitation in SAR images. An Object-based Image Analysis (OBIA) is applied to SAR images and to a Digital Terrain Model (DTM), organizing a database for hydraulic flood models calibration. Principal Component Analysis (PCA) is proposed to automate the determination of flood / non-flood decision thresholds. A previous classification, with a visual threshold selection, is performed for a small set of training images. A first PCA detects correlation between the training thresholds, image, and flood parameters; while a second PCA allows the automatic determination of the threshold for the remaining dataset classification. For the quality assessment, averages of 88 percent for properly detected flooded area and of 10 percent for commission error are achieved. It is verified that the algorithm performs well for images acquired during most weather conditions.
            </details>
        </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2014/00000080/00000006/art00006;jsessionid=2148pfs7i89b9.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Planar Block Adjustment and Orthorectification of ZY-3 Satellite Images
        </a>
    </h3>
    <div style="font-style: italic;">201406, pp. 559-570(12)</div>
    <div>Authors: Wang, Taoyang; Zhang, Guo; Li, Deren; Tang, Xinming; Jiang, Yonghua; Pan, Hongbo; Zhu, Xiaoyong</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                For the problem of block adjustment for satellite images, which cannot be solved under conditions of weak geometric convergence, this paper proposes a strategy that uses a planar block adjustment method to solve the orientation parameters of all satellite images, and then each satellite image is orthorectified. This strategy can ensure both the uniformity of the positioning accuracy and the strictness of the relative positions of the adjacent orthoimages. Tests of 139 panchromatic nadir images from the ZY-3 satellite show that by using only a small number of ground control points (GCPs), whose plane accuracy is 5 m, the plane accuracy of independent check points (ICPs) is better than 7 m after planar block adjustment. This accuracy meets the requirements for Chinese 1:50 000 topographic maps. Moreover, the preciseness of tie points (TPs) in adjacent images is better than 0.5 pixels, so a seamless level in mosaic geometry is attained.
            </details>
        </div>
</article>
