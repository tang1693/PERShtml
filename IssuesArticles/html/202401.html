
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 01 - Year 2024</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 01 - Year 2024</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000001" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000001/art00007;jsessionid=37qh3q627goe5.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Terrain Complexity and Maximal Poisson-Disk Sampling-Based Digital Elevation Model Simplification
        </a>
    </h3>
    <div style="font-style: italic;">202401, pp. 13-20(8)</div>
    <div>Authors: Dong, Jingxian; Ming, Fan; Kabika, Twaha; Jiang, Jiayao; Zhang, Siyuan; Chervan, Aliaksandr; Natallia, Zhukouskaya; Hou, Wenguang</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                With the rapid development of lidar, the accuracy and density of the Digital Elevation Model (DEM) point clouds have been continuously improved. However, in some applications, dense point cloud has no practical meaning. How to effectively sample from the dense points and maximize the preservation of terrain features is extremely important. This paper will propose a DEM sampling algorithm that utilizes terrain complexity and maximal Poisson-disk sampling to extract key feature points for adaptive DEM sampling. The algorithm estimates terrain complexity based on local terrain variation and prioritizes points with high complexity for sampling. The sampling radius is inversely proportional to terrain complexity, while ensuring that points within the radius of accepted samples are not considered new samples. This way makes more points of concern in the rugged regions. The results show that the proposed algorithm has higher global accuracy than the classic six sampling methods.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000001/art00008;jsessionid=37qh3q627goe5.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            I2-FaçadeNet: An Illumination-invariant Façade Recognition Network Leveraging Sparsely Gated Mixture of Multi-color Space Experts for Aerial Oblique Imagery
        </a>
    </h3>
    <div style="font-style: italic;">202401, pp. 21-31(11)</div>
    <div>Authors: Huang, Shengzhi; Hu, Han; Zhu, Qing</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Façade image recognition under complex illumination conditions is crucial for various applications, including urban three-dimensional modeling and building identification. Existing methods relying solely on Red-Green-Blue (RGB) images are prone to texture ambiguity in complex illumination environments. Furthermore, façades display varying orientations and camera viewing angles, resulting in performance issues within the RGB color space. In this study, we introduce an illumination-invariant façade recognition network (I2-FaçadeNet) that leverages sparsely gated multi-color space experts for enhanced façade image recognition in challenging illumination environments. First, RGB façade images are converted into multi-color spaces to eliminate the ambiguous texture in complex illumination. Second, we train expert networks using separate channels of multi-color spaces. Finally, a sparsely gated mechanism is introduced to manage the expert networks, enabling dynamic activation of expert networks and the merging of results. Experimental evaluations leveraging both the International Society for Photogrammetry and Remote Sensing benchmark data sets and the Shenzhen data sets reveal that our proposed I2-FaçadeNet surpasses various depths of ResNet in façade recognition under complex illumination conditions. Specifically, the classification accuracy for poorly illuminated façades in Zurich improves by nearly 8%, while the accuracy for over-illuminated areas in Shenzhen increases by approximately 3%. Moreover, ablation studies conducted on façade images with complex illumination indicate that compared to traditional RGB-based ResNet, the proposed network achieves an accuracy improvement of 3% to 4% up to 100% for overexposed images and an accuracy improvement of 3% to 10% for underexposed images.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000001/art00009;jsessionid=37qh3q627goe5.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Development of Soil-Suppressed Impervious Surface Area Index for Automatic Urban Mapping
        </a>
    </h3>
    <div style="font-style: italic;">202401, pp. 33-43(11)</div>
    <div>Authors: Javed, Akib; Shao, Zhenfeng; Ara, Iffat; Ahmad, Muhammad Nasar; Huq, Md.Enamul; Saleem, Nayyer; Karim, Fazlul</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Expanding urban impervious surface area (ISA) mapping is crucial to sustainable development, urban planning, and environmental studies. Multispectral ISA mapping is challenging because of the mixed-pixel problems with bare soil. This study presents a novel approach using spectral and temporal information to develop a Soil-Suppressed Impervious Surface Area Index (SISAI) using the Landsat Operational Land Imager (OLI) data set, which reduces the soil but enhances the ISA signature. This study mapped the top 12 populated megacities using SISAI and achieved an over-all accuracy of 0.87 with an F1-score of 0.85. It also achieved a higher Spatial Dissimilarity Index between the ISA and bare soil. However, it is limited by bare gray soil and shadows of clouds and hills. SISAI encourages urban dynamics and inter-urban compari- son studies owing to its automatic and unsupervised methodology.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000001/art00010;jsessionid=37qh3q627goe5.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Dual-branch Branch Networks Based on Contrastive Learning for Long-Tailed Remote Sensing
        </a>
    </h3>
    <div style="font-style: italic;">202401, pp. 45-53(9)</div>
    <div>Authors: Zhang, Lei; Peng, Lijia; Xia, Pengfei; Wei, Chuyuan; Yang, Chengwei; Zhang, Yanyan</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Deep learning has been widely used in remote sensing image classification and achieves many excellent results. These methods are all based on relatively balanced data sets. However, in real-world scenarios, many data sets belong to the long-tailed distribution, resulting in poor performance. In view of the good performance of contrastive learning in long-tailed image classification, a new dual-branch fusion learning classification model is proposed to fuse the discriminative features of remote sensing images with spatial data, making full use of valuable image representation information in imbalance data. This paper also presents a hybrid loss, which solves the problem of poor discrimination of extracted features caused by large intra-class variation and inter-class ambiguity. Extended experiments on three long-tailed remote sensing image classification data sets demonstrate the advantages of the proposed dual-branch model based on contrastive learning in long-tailed image classification.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000001/art00011;jsessionid=37qh3q627goe5.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Comparison of 3D Point Cloud Completion Networks for High Altitude Lidar Scans of Buildings
        </a>
    </h3>
    <div style="font-style: italic;">202401, pp. 55-64(10)</div>
    <div>Authors: Kulawiak, Marek</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                High altitude lidar scans allow for rapid acquisition of big spatial data representing entire city blocks. Unfortunately, the raw point clouds acquired by this method are largely incomplete due to object occlusions and restrictions in scanning angles and sensor resolution, which can negatively affect the obtained results. In recent years, many new solutions for 3D point cloud completion have been created and tested on various objects; however, the application of these methods to high-altitude lidar point clouds of buildings has not been properly investigated yet. In the above context, this paper presents the results of applying several state-of-the-art point cloud completion networks to various building exteriors acquired by simulated airborne laser scanning. Moreover, the output point clouds generated from partial data are compared with complete ground-truth point clouds. The performed tests show that the SeedFormer network trained on the ShapeNet-55 data set provides promising shape completion results.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 01 - Year 2024</p>
        </footer>
    </body>
    </html>
    