<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2023/00000089/00000002/art00009;jsessionid=g3jgijj90rsqk.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            UAS Edge Computing of Energy Infrastructure Damage Assessment
        </a>
    </h3>
    <div style="font-style: italic;">202302, pp. 79-87(9)</div>
    <div>Authors: Bowman, Jordan; Yang, Lexie; Thomas, Orrin; Kirk, Jerry; Duncan, Andrew; Hughes, David; Meade, Shannon</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Energy infrastructure assessments are needed within 72 hours of natural disasters, and previous data collection methods have proven too slow. We demonstrate a scalable end-to-end solution using a prototype unmanned aerial system that performs on-the-edge detection, classification (i.e., damaged or undamaged), and geo-location of utility poles. The prototype is suitable for disaster response because it requires no local communication infrastructure and is capable of autonomous missions. Collections before, during, and after Hurricane Ida in 2021 were used to test the system. The system delivered an F1 score of 0.65 operating with a 2.7 s/frame processing speed with the YOLOv5 large model and an F1 score of 0.55 with a 0.48 s/frame with the YOLOv5 small model. Geo-location uncertainty in the bottom half of the frame was ∼8 m, mostly driven by error in camera pointing measurement. With additional training data to improve performance and detect additional types of features, a fleet of similar drones could autonomously collect actionable post-disaster data.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2023/00000089/00000002/art00010;jsessionid=g3jgijj90rsqk.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Apricot Tree Detection from UAV-Images Using Mask R-CNN and U-Net
        </a>
    </h3>
    <div style="font-style: italic;">202302, pp. 89-96(8)</div>
    <div>Authors: Erdem, Firat; Ocer, Nuri Erkin; Matci, Dilek Kucuk; Kaplan, Gordana; Avdan, Ugur</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Monitoring trees is necessary to manage and take inventory of forests, monitor plants in urban areas, distribute vegetation, monitor change, and establish sensitive and renewable agricultural systems. This study aims to automatically detect, count, and map apricot trees in an orthophoto, covering an area of approximately 48 ha on the ground surface using two different algorithms based on deep learning. Here, Mask region-based convolutional neural network (Mask R-CNN) and U-Net models were run together with a dilation operator to detect apricot trees in UAV images, and the performances of the models were compared. Results show that Mask R-CNN operated in this way performs better in tree detection, counting, and mapping tasks compared to U-Net. Mask R-CNN with the dilation operator achieved a precision of 98.7%, recall of 99.7%, F1 score of 99.1%, and intersection over union (IoU) of 74.8% for the test orthophoto. U-Net, on the other hand, has achieved a recall of 93.3%, precision of 97.2%, F1 score of 95.2%, and IoU of 58.3% when run with the dilation operator. Mask R-CNN was able to produce successful results in challenging areas. U-Net, on the other hand, showed a tendency to overlook existing trees rather than generate false alarms.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2023/00000089/00000002/art00011;jsessionid=g3jgijj90rsqk.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Comparative Analysis of Different CNN Models for Building Segmentation from Satellite and UAV Images
        </a>
    </h3>
    <div style="font-style: italic;">202302, pp. 97-105(9)</div>
    <div>Authors: Sariturk, Batuhan; Kumbasar, Damla; Seker, Dursun Zafer</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Building segmentation has numerous application areas such as urban planning and disaster management. In this study, 12 CNN models (U-Net, FPN, and LinkNet using EfficientNet-B5 backbone, U-Net, SegNet, FCN, and six Residual U-Net models) were generated and used for building segmentation. Inria Aerial Image Labeling Data Set was used to train models, and three data sets (Inria Aerial Image Labeling Data Set, Massachusetts Buildings Data Set, and Syedra Archaeological Site Data Set) were used to evaluate trained models. On the Inria test set, Residual-2 U-Net has the highest F1 and Intersection over Union (IoU) scores with 0.824 and 0.722, respectively. On the Syedra test set, LinkNet-EfficientNet-B5 has F1 and IoU scores of 0.336 and 0.246. On the Massachusetts test set, Residual-4 U-Net has F1 and IoU scores of 0.394 and 0.259. It has been observed that, for all sets, at least two of the top three models used residual connections. Therefore, for this study, residual connections are more successful than conventional convolutional layers.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2023/00000089/00000002/art00012;jsessionid=g3jgijj90rsqk.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Unmanned Aerial Vehicle (UAV)–Based Imaging Spectroscopy for Predicting Wheat Leaf Nitrogen
        </a>
    </h3>
    <div style="font-style: italic;">202302, pp. 107-116(10)</div>
    <div>Authors: Sahoo, Rabi N.; Gakhar, Shalini; Rejith, R.G.; Ranjan, Rajeev; Meena, Mahesh C.; Dey, Abir; Mukherjee, Joydeep; Dhakar, Rajkumar; Arya, Sunny; Daas, Anchal; Babu, Subhash; Upadhyay, Pravin K.; Sekhawat, Kapila; SudhirKumar; Kumar, Mahesh; Chinnusamy, Viswanathan; Khanna, Manoj</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Quantitative estimation of crop nitrogen is the key to site-specific management for enhanced nitrogen (N) use efficiency and a sustainable crop production system. As an alternate to the conventional approach through wet chemistry, sensor-based noninvasive, rapid, and near-real-time assessment of crop N at the field scale has been the need for precision agriculture. The present study attempts to predict leaf N of wheat crop through spectroscopy using a field portable spectroradiometer (spectral range of 400–2500 nm) on the ground in the crop field and an imaging spectrometer (spectral range of 400–1000 nm) from an unmanned aerial vehicle (UAV) with the objectives to evaluate (1) four multivariate spectral models (i.e., artificial neural network, extreme learning machine [ELM], least absolute shrinkage and selection operator, and support vector machine regression) and (2) two sets of hyperspectral data collected from two platforms and two different sensors. In the former part of the study, ELM outperforms the other methods with maximum calibration and validation R2 of 0.99 and 0.96, respectively. Furthermore, the image data set acquired from UAV gives higher performance compared to field spectral data. Also, significant bands are identified using stepwise multiple linear regression and used for modeling to generate a wheat leaf N map of the experimental field.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2023/00000089/00000002/art00013;jsessionid=g3jgijj90rsqk.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Car Detection from Very High-Resolution UAV Images Using Deep Learning Algorithms
        </a>
    </h3>
    <div style="font-style: italic;">202302, pp. 117-123(7)</div>
    <div>Authors: Kaya, Yunus; Şenol, Halil İbrahim; Yiğit, Abdurahman Yasin; Yakar, Murat</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                It is important to determine car density in parking lots, especially in hospitals, large enterprises, and residential areas, which are used intensively, in terms of executing existing management systems and making precise plans for the future. In this study, cars in parking lots were detected using high-resolution unmanned aerial vehicle (UAV) images with deep learning methods. We tested the performance of the two approaches by determining the number of cars in a parking lot using the You Only Look Once (YOLOv3) and Mask Region–Based Convolutional Neural Networks (Mask R-CNN) approaches as deep learning methods and the deep learning tool of Esri ArcGIS Pro. High-resolution UAV images were processed by photogrammetry and used as input products for the R-CNN and YOLOv3 algorithm. Recall, F1 score, precision ratio/uncertainty accuracy, and average producer accuracy of products automatically extracted with the algorithm were determined as 0.862/0.941, 0.874/0.946, 0.885/0.951, and 0.776/0.897 for R-CNN and YOLOv3, respectively.
            </details>
        </div>
</article>
