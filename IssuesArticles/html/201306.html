
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 06 - Year 2013</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 06 - Year 2013</h1>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000006/art00001;jsessionid=29lf14i25r8l1.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Band Grouping versus Band Clustering in SVM Ensemble Classification of Hyperspectral Imagery
        </a>
    </h3>
    <div style="font-style: italic;">201306, nan</div>
    <div>Authors: Bigdeli, Behnaz; Samadzadegan, Farhad; Reinartz, Peter</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Due to the dense sampling of spectral signatures of land covers, hyperspectral images have a better discrimination among similar ground cover classes than traditional remote sensing data. However, these images are usually composed of tens or hundreds of spectrally close bands, which result in high redundancy and great amount of computation time in hyperspectral image classification. In addition, the large number of spectral bands, but limited availability of training samples creates the problem of Hughes phenomenon. Consequently, traditional classification strategies have often limited performance in classification of hyperspectral imagery. Referring to the limitation of single classifiers in these situations, classifier ensemble system may exhibit better performance. This paper presents a method for classification of hyper-spectral data based on two concepts of Band Clustering (BC) and Band Grouping (BG) through a Support Vector machine (SVM) ensemble system. The proposed method uses the BC\BG strategies to split data into few band portions. After this step, we applied SVM on each band cluster\group that is produced in previous step. Finally, Naive Bayes as a classifier fusion method combines the decisions of SVM classifiers. Experimental results show that the proposed method improves the classification accuracy in comparison to the standard SVM and to feature selection methods.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000006/art00002;jsessionid=29lf14i25r8l1.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Automated Approach for the Conflation of Vector Parcel Map with Imagery
        </a>
    </h3>
    <div style="font-style: italic;">201306, pp. 535-543(9)</div>
    <div>Authors: Song, Wenbo; Keller, James M.; Haithcoat, Timothy L.; Davis, Curt H.; Hinsen, Jason B.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Local governments frequently use parcel maps or other spatial data for decision-making, but much of this data is often inaccurate and outdated, producing a negative influence on the expected outcome. Remote sensing can provide accurate and current high-resolution imagery. However, one major bottleneck to the integration of high-resolution imagery into existing geographic information systems is the issue of positional accuracy of the existing line-work within the vector GIS database, making it difficult to match with imagery. This paper presents a vector-to-imagery conflation approach to improve the positional accuracy of digital parcel maps by conflating the vector parcel maps to make it consistent with high-resolution imagery. Road intersections are automatically extracted from imagery and used as control points. A relaxation labeling algorithm is used to find the matches between the two road intersection point sets from vector and imagery. The links are created from the matched point pairs and are used to perform a piecewise transformation. The test results show that this approach can improve the accuracy of vector parcel maps significantly. It is a very cost-effective method and has great potential to save considerable time and money for local governments to upgrade their inaccurate vector datasets.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000006/art00003;jsessionid=29lf14i25r8l1.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Developing Efficient Procedures for Automated Sinkhole Extraction from Lidar DEMs
        </a>
    </h3>
    <div style="font-style: italic;">201306, pp. 545-554(10)</div>
    <div>Authors: Miao, Xin; Qiu, Xiaomin; Wu, Shuo-Sheng; Luo, Jun; Gouzie, Douglas R.; Xie, Hongjie</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Sinkhole detection in karst areas is usually difficult through remote sensing image interpretation. We present an efficient approach to extract mature sinkholes from lidar DEM. First, an adaptive Wiener filter (AWF) and hierarchical watershed segmentation (HWS) are applied to identify all local depression or potential sinkholes. Second, a hole-filling algorithm is applied to the potential sinkholes, and nine spatial features are extracted. Finally, the random forest classifier is used to select true sinkholes from all potential sinkholes. Our results show that this approach is efficient for detecting mature sinkholes from lidar data, and it can be used for risk assessment and hazard preparedness in karst areas.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000006/art00004;jsessionid=29lf14i25r8l1.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Flexible Method for Zoom Lens Calibration and Modeling Using a Planar Checkerboard
        </a>
    </h3>
    <div style="font-style: italic;">201306, pp. 555-571(17)</div>
    <div>Authors: Wu, Bo; Hu, Han; Zhu, Qing; Zhang, Yeting</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper presents a flexible method for zoom lens calibration and modeling using a planar checkerboard. The method includes the following four steps. First, the principal point of the zoom-lens camera is determined by a focus-of-expansion approach. Second, the influences of focus changes on the principal distance are modeled by a scale parameter. Third, checkerboard images taken at varying object distances with convergent image geometry are used for camera calibration. Finally, the variations of the calibration parameters with respect to the various zoom and focus settings are modeled using polynomials. Three different types of lens are examined in this study. Experimental analyses show that high precision calibration results can be expected from the developed approach. The relative measurement accuracy (accuracy normalized with object distance) using the calibrated zoom-lens camera model ranges from 1:5 000 to 1:25 000. The developed method is of significance to facilitate the use of zoom-lens camera systems in various applications such as robotic exploration, hazard monitoring, traffic monitoring, and security surveillance.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000006/art00005;jsessionid=29lf14i25r8l1.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Stitching and Processing Gnomonic Projections for Close-Range Photogrammetry
        </a>
    </h3>
    <div style="font-style: italic;">201306, pp. 573-582(10)</div>
    <div>Authors: Barazzetti, Luigi; Previtali, Mattia; Scaioni, Marco</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper presents a 3D reconstruction methodology based on gnomonic projections generated from multiple central perspectives. The method can be useful when traditional images are insufficient to capture fine portions of objects with an adequate level of detail, especially in close-range photogrammetry. The aim of this paper is to prove that gnomonic projections are powerful tools to recover small details that cannot be reconstructed from standard images. The generation of the gnomonic projections is discussed, as well as the methodology to compute camera parameters, orient multiple projections, and finally extract textured 3D models or orthophotos. To verify the correctness of the methodology, some comparisons with sets of independent checkpoints were carried out. The accuracy assessment confirmed the correctness of the mathematical approach and underlined how gnomonic projections are a valid alternative to standard images. The method itself has been already used in surveys for cultural heritage documentation.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 06 - Year 2013</p>
        </footer>
    </body>
    </html>
    