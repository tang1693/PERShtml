<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2010/00000076/00000003/art00001;jsessionid=60tkmejnse8rm.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Land-Cover Change Detection using One-Class Support Vector Machine
        </a>
    </h3>
    <div style="font-style: italic;">201003, nan</div>
    <div>Authors: Li, Peijun; Xu, Haiqing</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Change detection using remote sensing has considerable potential for monitoring land-cover change. Commonly, one specific class of change is of interest in many applications. In this paper, a recently developed one-class classifier, the One-Class Support Vector Machine (OCSVM), is proposed for the change detection of one specific class by multitemporal classification. The classifier only requires samples from the change class of interest as the training data. The performance of the proposed method was evaluated in two applications by comparing with conventional post-classification comparison methods. The results demonstrated the proposed method achieved both higher overall accuracy and higher kappa coefficient than the conventional methods, and demonstrated good potential for further application. The study also indicated that with the OCSVM, the analysis can focus only on the specific class of interest and does not need to treat other classes, thus providing highly accurate change detection. The OCSVM-based change detection method, as a general and easily implemented method, can be used for applications where only the change of one specific class is of interest.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2010/00000076/00000003/art00002;jsessionid=60tkmejnse8rm.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Study of Trajectory Models for Satellite Image Triangulation
        </a>
    </h3>
    <div style="font-style: italic;">201003, pp. 265-276(12)</div>
    <div>Authors: Jeong, In-seong; Bethel, James</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Many Spaceborne imagery products are provided with metadata or support data having diverse types, representations, frequencies, and conventions. According to the variability of metadata, a compatible physical sensor model approach must be constructed. Among the three components of the sensor model, i.e., trajectory model, projection equations, and parameter subset selection, the construction of the position and attitude trajectory is closely linked with the availability and type of support data. In this paper, we show how trajectory models can be implemented based on support data from six satellite image types: QuickBird, Hyperion, SPOT-3, ASTER, PRISM, and EROS-A. Triangulation for each image is implemented to investigate the feasibility and suitability of the different trajectory models. The results show the effectiveness of some of the simple models while indicating that careful use of dense ephemeris information is necessary. These results are based on having a number of high quality ground control points.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2010/00000076/00000003/art00003;jsessionid=60tkmejnse8rm.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Evaluation for Damaged Degree of Vegetation by Forest Fire using Lidar and a Digital Aerial Photograph
        </a>
    </h3>
    <div style="font-style: italic;">201003, pp. 277-287(11)</div>
    <div>Authors: Kwak, Doo-Ahn; Chung, Jinwon; Lee, Woo-Kyun; Kafatos, Menas; Lee, Si Young; Cho, Hyun-Kook; Lee, Seung-Ho</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The amount of vegetation physically damaged by forest fire can be evaluated using lidar (Light Detection And Ranging) data because the loss of canopy height and width by forest fire can be relevant to the number of points transmitted to the ground through the canopy of the damaged forest. On the other hand, the biological damage of vegetation caused by forest fire can be obtained from the Normalized Difference Vegetation Index (NDVI), which determines the vegetation vitality. In this study, the degree of physical damage from the lidar data was classified into serious physical damage (SPD) and light physical damage (LPD). The degree of biological damage using NDVI was likewise classified into serious biological damage (SBD) and light biological damage (LBD). Finally, the damaged area was graded into four categories: (a) SPD and SBD, (b) LPD and SBD, (c) SPD and LBD, and (d) LPD and LBD. The accuracy assessment for the area classified into four grades showed an overall accuracy of 0.74, and a kappa value of 0.61 which provides improvement over previous works.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2010/00000076/00000003/art00004;jsessionid=60tkmejnse8rm.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Accuracy Assessment Measures for Object-based Image Segmentation Goodness
        </a>
    </h3>
    <div style="font-style: italic;">201003, pp. 289-299(11)</div>
    <div>Authors: Clinton, Nicholas; Holt, Ashley; Scarborough, James; Yan, Li; Gong, Peng</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                To select an image segmentation from sets of segmentation results, measures for ranking the segmentations relative to a set of reference objects are needed. We review selected vector-based measures designed to compare the results of object-based image segmentation with sets of training objects extracted from the image of interest. We describe and compare area-based and location-based measures that measure the shape similarity between segments and training objects. By implementing the measures in two object-based image processing software packages, we illustrate their use in terms of automatically identifying parsimonious parameter combinations from arbitrarily large sets of segmentation results. The results show that the measures have divergent performance in terms of the identification of parameter combinations. Clustering of the results in measure space narrows the search. We illustrate combination schemes for the measures for generating rankings of segmentation results. The ranked segmentation results are illustrated and described.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2010/00000076/00000003/art00005;jsessionid=60tkmejnse8rm.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Mathematical Expression for Stereoscopic Depth Perception
        </a>
    </h3>
    <div style="font-style: italic;">201003, pp. 301-306(6)</div>
    <div>Authors: Rosas, Humberto; Vargas, Watson; Cerón, Alexander; Domínguez, Darío; Cárdenas, Adriana</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The metric nature of stereoscopic depth perception has remained an enigma. Several mathematical formulations proposed for measuring the stereoscopic effect have not shown to be reliable. This may be due to the lack of a conceptual distinction between the 3D model geometrically obtained by intersection of visual rays (geometric model), and the 3D model perceived in the observer’s mind (perceptual model). Based on the assumption that retinal parallax is the only source of information on depth available to the brain, we developed an equation that shows real and perceptual space to be connected by a logarithmic function. This relationship has allowed us to formulate the vertical exaggeration for all sorts of stereoscopic conditions, including natural stereo-vision. The obtained formulations might involve possibilities of technological applications, such as the artificial recreation of a natural stereo-vision effect, and the design of stereoscopic instruments with a desired degree of vertical exaggeration.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2010/00000076/00000003/art00006;jsessionid=60tkmejnse8rm.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Co-registration of Surfaces by 3D Least Squares Matching
        </a>
    </h3>
    <div style="font-style: italic;">201003, pp. 307-318(12)</div>
    <div>Authors: Akca, Devrim</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A method for the automatic co-registration of 3D surfaces is presented. The method utilizes the mathematical model of Least Squares 2D image matching and extends it for solving the 3D surface matching problem. The transformation parameters of the search surfaces are estimated with respect to a template surface. The solution is achieved when the sum of the squares of the 3D spatial (Euclidean) distances between the surfaces are minimized. The parameter estimation is achieved using the Generalized Gauss-Markov model. Execution level implementation details are given. Apart from the co-registration of the point clouds generated from spaceborne, airborne and terrestrial sensors and techniques, the proposed method is also useful for change detection, 3D comparison, and quality assessment tasks. Experiments using terrain data examples show the capabilities of the method.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2010/00000076/00000003/art00007;jsessionid=60tkmejnse8rm.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Point Set Topology Extraction for Branch and Crown-level Species Classification
        </a>
    </h3>
    <div style="font-style: italic;">201003, pp. 319-330(12)</div>
    <div>Authors: Niccolai, Andrew; Niccolai, Melissa; Dearing Oliver, Chadwick</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Contextually-based distance and proximity functions derived from the axioms of point set topology are applied at the branch and crown-level for species differentiation between eastern hemlock (Tsuga canadensis) and eastern white pine (Pinus strobus). Point set topology is a branch of mathematics that offers methods to describe the connectivity and orientation of spatial objects and therefore allows object grouping based on spatial characteristic metrics unlike traditional fixed distance measures imposed from a global metric. Local neighborhood membership functions based on topological space are robust to variation in object sizes within a fixed image resolution and are therefore useful for describing spatial characteristics in highly variable objects such as tree morphology. We investigated the utility of topological space for describing branch-level needle orientations and crown-level patterns of high and low spectral intensity clusters. Branch-level measures of orientations within topologically-derived neighborhoods resulted in a classification accuracy of 96 percent for hemlock and pine; this represents a 23 percentage point (pp) improvement over traditional spectral feature classification. A crown-level species feature extraction and classification methodology that incorporated a local Getis (Gi*) statistic contour analysis produced species discrimination results that were improved by 8 pp for an overall accuracy of 85 percent over traditional color and shape-based feature classification.
            </details>
        </div>
</article>
