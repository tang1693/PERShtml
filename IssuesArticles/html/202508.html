
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 08 - Year 2025</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 08 - Year 2025</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000008" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000008/art00010;jsessionid=3qfmtrbm2rj7l.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Hierarchical Multi-Scale Cross Interaction Network for Enhanced Hyperspectral Image Classification
        </a>
    </h3>
    <div style="font-style: italic;">202508, pp. 495-507(13)</div>
    <div>Authors: Feng, Yuting; Yang, Lina; Wu, Thomas; Huang, Youju</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/08/Hierarchical Multi-Scale Cross Interaction Network for Enhanced Hyperspectral Image Classification.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The significance of hyperspectral image classification lies in its ability to discern subtle differences between materials, making it essential in fields such as agriculture, mineral exploration, and urban planning. Convolutional neural networks (CNNs) and transformer-based methods have become standard for hyperspectral imagery classification, with hybrid approaches gaining popularity. Yet, these methods often lack efficient interaction between the features extracted by CNNs and transformers. To address this, we propose the hierarchical multiscale cross interaction network (HMCI-Net), which leverages both CNNs and transformers to enhance classification accuracy. The CNN branch extracts local spatial-spectral features, and the transformer branch captures global spectral information, allowing the network to model long-range dependencies and complex correlations. Additionally, HMCI-Net incorporates a hierarchical multi-scale feature extraction module and a multi-view feature fusion module, further improving its ability to extract fine-grained, multi-perspective features. Extensive experiments on four benchmark hyperspectral data sets—Indian Pines, Pavia University, WHU-Hi-LongKou, and Houston2013—demonstrate that HMCI-Net outperforms existing methods, achieving an average improvement of 6.24% in average accuracy, 6.14% in kappa coefficient, and 5.55% in overall accuracy. Specifically, HMCI-Net achieves significant gains, with overall accuracy higher by 8.86%, 4.34%, 4.44%, and 4.42% on Indian Pines, Pavia University, WHU-HiLongKou, and Houston2013, respectively. Similarly, average accuracy is higher by 10.57%, 4.94%, 5.65%, and 4.81% for Indian Pines, Pavia University, WHU-Hi-LongKou, and Houston2013, respectively; kappa coefficient is higher by 10.39%, 4.88%, 4.70%, and 4.60%, respectively, on these data sets. The code and data set for this paper can be accessed at: https://github.com/codemanvon30/HMCI_Net.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <span style="background-color: gold; color: black; font-weight: bold; padding: 3px 8px; border-radius: 5px; font-size: 12px; margin-left: 0px;">
        Editor’s Choice
    </span>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000008/art00013;jsessionid=3qfmtrbm2rj7l.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Efficient Coral Survey Using Aerial Remote Sensing and Multi-modal Segmentation for Large-Scale Ecological Assessment
        </a>
    </h3>
    <div style="font-style: italic;">202508, pp. 509-516(8)</div>
    <div>Authors: Qin, Jiangying; Li, Ming; Armin, Gruen; Gong, Jianya; Zhong, Jiageng; Liao, Xuan</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/08/Efficient Coral Survey Using Aerial Remote Sensing and Multi-modal Segmentation for Large-Scale Ecological Assessment.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Due to the ecological pressures of global warming and human activities in coastal regions, coral reef ecosystems, predominantly located in shallow marine areas, are facing severe threats to their survival. Scientists and governmental managers are eager to leverage novel aerial remote sensing technologies to address the challenges of acquiring accurate, comprehensive, and timely data on coral reef health, structural complexity, and spatial distribution. This study aims to tackle these challenges by using precise and accurate aerial remote sensing data to support the restoration and sustainable prosperity of coral reef systems. Specifically, this study develops and applies an efficient coral survey method based on aerial remote sensing. The method integrates aerial imagery and bathymetric lidar (light detection and ranging) point cloud data and uses advanced photogrammetric computer vision and deep learning algorithms. Using a state-of-the-art multi-modal neural network segmentation technique, the proposed method enables high-precision and intelligent identification of coral reefs, facilitating detailed habitat mapping. Furthermore, by accurately delineating the habitat range and geometric structures of reefs, this approach allows for precise measurements of coral biomass production and skeletal calcification. These metrics help assess coral reef structural complexity and their adaptability to environmental stressors, providing robust scientific data for conservation strategies and policy making. The use of advanced multi-modal aerial remote sensing data not only enhances monitoring reliability and accuracy but also offers a cost-effective and flexible tool for coral reef ecological mapping. This approach effectively addresses challenges encountered in coastal ecological surveys, particularly in areas where direct human access or boat entry is difficult.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000008/art00014;jsessionid=3qfmtrbm2rj7l.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            On the Transferability of Semantic Segmentation for Very-High-Resolution Remote Sensing Data of Multi-City Environments
        </a>
    </h3>
    <div style="font-style: italic;">202508, pp. 517-528(12)</div>
    <div>Authors: Qin, Rongjun; Zhang, Guixiang; Tang, Yang</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/08/On the Transferability of Semantic Segmentation for Very-High-Resolution Remote Sensing Data of Multi-City Environments.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Semantic segmentation of very-high-resolution remote sensing (RS) data is foundational for many RS applications in urban environments. Images from sources like Worldview-3/4 and Pleiades-Neo offer resolutions as high as 0.3 m, enabling a fine-grained understanding of urban structures. Recent deep learning‐based methods have significantly outperformed traditional approaches in RS semantic segmentation/classification tasks. However, they require large training data sets and often lack transferability due to highly disparate RS image content across different geographical regions. However, no comprehensive analysis exists on their transferability—i.e., to what extent a model trained on a source domain can be applied to a target domain in urban areas. This paper investigates the raw transferability of traditional and deep-learning models and the effectiveness of domain adaptation approaches in enhancing deep-learning model transferability (adapted transferability). Using five highly diverse RS data sets from different cities (6792 patches of 1024 × 1024 pixels each), we trained six models with and without three domain adaptation approaches to quantitatively analyze transferability between data sets. To facilitate easy assessment of model transferability, we developed a simple method to quantify transferability using spectral indices as a medium, demonstrating its effectiveness in evaluating model transferability at the target domain when labels are unavailable. Our experiments yield several important but underreported observations on raw and adapted transferability. Moreover, our proposed label-free transferability assessment method outperforms posterior model confidence and can guide model selection for urban studies globally. The models and datasets are publicly available on GitHub at: https://github.com/GDAOSU/Transferability-Remote-Sensing.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 08 - Year 2025</p>
        </footer>
    </body>
    </html>
    