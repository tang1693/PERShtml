
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 02 - Year 2025</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 02 - Year 2025</h1>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000002/art00008;jsessionid=oun1mw0p67lv.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatiotemporal Behavior of Active Forest Fires Using Time-Series MODIS C6 Data
        </a>
    </h3>
    <div style="font-style: italic;">202502, pp. 85-90(6)</div>
    <div>Authors: Azimuddin, Syed; Dwivedi, R.S.</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/02/Spatio-temporal behaviour of active forest fires using time-series MODIS C6 data.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Forest fires have a profound influence on the economy, ecology, and environment. Realizing the potential of remote sensing in forest fire management, a study was taken up to investigate the spatiotemporal behavior of active forest fires in a mountainous terrain of Uttarakhand State, north India, using 15 years' time-series historical MODIS (C6) active fire point products. Results indicate an over-all fire incidence detection accuracy of 62.3% with a KHAT value of 0.59. Moreover, a regular trend in intra-annual behavior in fire incidences with peaks during the hot and dry period of the year was observed and a large year-to-year variability in fire regimes with no significant trends over time could be noticed. The approach and results are discussed in detail along with the future perspective.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000002/art00010;jsessionid=oun1mw0p67lv.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Artificial Neural Network Multi-layer Perceptron Models to Classify California's Crops using Harmonized Landsat Sentinel (HLS) Data
        </a>
    </h3>
    <div style="font-style: italic;">202502, pp. 91-100(10)</div>
    <div>Authors: McCormick, Richard; Thenkabail, Prasad S.; Aneece, Itiya; Teluguntla, Pardhasaradhi; Oliphant, Adam J.; Foley, Daniel</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/02/Artificial Neural Network Multilayer Perceptron Models to1 Classify Californiaâ€™s Crops using Harmonized Landsat2 Sentinel (HLS) Data.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Advances in remote sensing and machine learning are enhancing cropland classification, vital for global food and water security. We used multispectral Harmonized Landsat 8 Sentinel-2 (HLS) 30-m data in an artificial neural network (ANN) multi-layer perceptron (MLP) model to classify five crop classes (cotton, alfalfa, tree crops, grapes, and others) in California's Central Valley. The ANN MLP model, trained on 2021 data from the United States Department of Agriculture's Cropland Data Layer, was validated by classifying crops for an independent year, 2022. Across the five crop classes, the overall accuracy was 74%. Producer's and user's accuracies ranged from 65% to 87%, with cotton achieving the highest accuracies. The study highlights the potential of using deep learning with HLS time series data for accurate global crop classification.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000002/art00011;jsessionid=oun1mw0p67lv.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Individual Tree Segmentation Using Deep Learning and Climbing Algorithm: A Method for Achieving High-precision Single-tree Segmentation in High-density Forests under Complex Environments
        </a>
    </h3>
    <div style="font-style: italic;">202502, pp. 101-110(10)</div>
    <div>Authors: Ma, He; Zhang, Fangmin; Chen, Simin; Yu, Jinge</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/02/Individual tree segmentation using deep learning and climbing algorithm.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Accurate individual tree segmentation, which is important for forestry investigation, is still a difficult and challenging task. In this study, we developed a climbing algorithm and combined it with a deep learning model to extract forests and achieve individual tree segmentation using lidar point clouds. We tested the algorithm on mixed forests within complex environments scanned by unmanned aircraft system lidar in ecological restoration mining areas along the Yangtze River of China. Quantitative assessments of the segmentation results showed that the forest extraction achieved a kappa coefficient of 0.88, and the individual tree segmentation results achieved F-scores ranging from 0.86 to 1. The climbing algorithm successfully reduced false positives and false negatives with the increased crown overlapping and outperformed the widely used top-down region-growing point cloud segmentation method. The results indicate that the climbing algorithm proposed in this study will help solve the overlapped crown problem of tree segmentation under complex environments.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000002/art00012;jsessionid=oun1mw0p67lv.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Lightweight Ship Object Detection Algorithm for Remote Sensing Images Based on Multi-scale Perception and Feature Enhancement
        </a>
    </h3>
    <div style="font-style: italic;">202502, pp. 111-122(12)</div>
    <div>Authors: Sun, Wei; Shen, Xinyi; Zhang, Xiaorui; Guan, Fei</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/02/Lightweight Ship Object Detection Algorithm for.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                As global trade and maritime traffic develop, exploring ship detection in remote sensing images has become a research hotspot. However, ships in remote sensing images are so small that it leads to a high detection leakage rate and excessive model parameters, making them difficult to apply on remote sensing equipment with limited resources. To address the challenge, we propose a light-weight ship object detection algorithm, adaptive layered multi-scale You Only Look Once version 8 (ALM-YOLOv8), based on multi-scale perception and feature enhancement. To enhance the model's perception of contextual information in complex backgrounds, a multi-scale channel fusion module is constructed to extract features of various scales. To enhance the extracted features, a small-object detection layer and a dynamic channel attention convolution that assigns dynamic weights are proposed. Additionally, this study embeds the large separable kernel attention mechanism into the original network, which lightens the model. Experiments on the HRSC2016 dataset demonstrate the effectiveness of ALM-YOLOv8.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 02 - Year 2025</p>
        </footer>
    </body>
    </html>
    