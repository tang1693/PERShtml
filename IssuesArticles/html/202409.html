
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 09 - Year 2024</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 09 - Year 2024</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000009" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000009/art00007;jsessionid=1mbv2n6m4be2x.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Development of an Automatic Feature Point Classification Method for Three-Dimensional Mapping Around Slewing and Derricking Cranes
        </a>
    </h3>
    <div style="font-style: italic;">202409, pp. 538-552(15)</div>
    <div>Authors: Shigemori, Hisakazu; Susaki, Junichi; Yoneda, Mizuki; Ososinski, Marek</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Crane automation requires a three-dimensional (3D) map around cranes that should be reconstructed and updated quickly.In this study, a high-precision classification method was developed to distinguish stationary objects from moving objects in moving images captured by a monocular camera to stabilize 3D reconstruction. To develop the method, a moving image was captured while the crane was slewed with a monocular camera mounted vertically downward at the tip of the crane. The boom length and angle data were output from a control device, a controller area network. For efficient development, a simulator that imitated the environment of an actual machine was developed and used. The proposed method uses optical flow to track feature points. The classification was performed successfully, independent of derricking motion. Consequently, the proposed method contributes to stable 3D mapping around cranes in construction sites.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000009/art00009;jsessionid=1mbv2n6m4be2x.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Semantic Segmentation of Point Cloud Scene via Multi-Scale Feature Aggregation and Adaptive Fusion
        </a>
    </h3>
    <div style="font-style: italic;">202409, pp. 553-563(11)</div>
    <div>Authors: Guo, Baoyun; Sun, Xiaokai; Li, Cailin; Sun, Na; Wang, Yue; Yao, Yukai</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Point cloud semantic segmentation is a key step in 3D scene understanding and analysis. In recent years, deep learning–based point cloud semantic segmentation methods have received extensive attention from researchers. Multi-scale neighborhood feature learning methods are suitable for inhomogeneous density point clouds, but different scale branching feature learning increases the computational complexity and makes it difficult to accurately fuse different scale features to express local information. In this study, a point cloud semantic segmentation network based on RandLA-Net with multi-scale local feature aggregation and adaptive fusion is proposed. The designed structure can reduce computational complexity and accurately express local features. The mean intersection-over-union is improved by 1.1% on the SemanticKITTI data set with an inference speed of nine frames per second, while the mean intersection-over-union is improved by 0.9% on the S3DIS data set, compared with RandLA-Net. We also conduct ablation studies to validate the effectiveness of the proposed key structure.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000009/art00011;jsessionid=1mbv2n6m4be2x.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Robust Star Identification Algorithm for Resident Space Object Surveillance
        </a>
    </h3>
    <div style="font-style: italic;">202409, pp. 565-574(10)</div>
    <div>Authors: Wu, Liang; Hao, Pengyu; Zhang, Kaixuan; Zhang, Qian; Han, Ru; Cao, Dekun</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Star identification algorithms can be applied to resident space object (RSO) surveillance, which includes a large number of stars and false stars. This paper proposes an efficient, robust star identification algorithm for RSO surveillance based on a neural network. First, a feature called equal-frequency binning radial feature (EFB-RF) is proposed for guide stars, and a superficial neural network is constructed for feature classification. Then the training set is generated based on EFB-RF. Finally, the remaining stars are identified using a residual star matching method. The simulation experiment and results show that the identification rate of our algorithm can reach 99.82% under 1 pixel position noise, and it can reach 99.54% under 5% false stars. When the percentage of missing stars is 15%, it can reach 99.40%. The algorithm is verified by RSO surveillance.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000009/art00013;jsessionid=1mbv2n6m4be2x.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Wavelets for Self-Calibration of Aerial Metric Camera Systems
        </a>
    </h3>
    <div style="font-style: italic;">202409, pp. 575-587(13)</div>
    <div>Authors: Ye, Jun-Fu; Tsay, Jaan-Rong; Fritsch, Dieter</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In this paper, wavelets are applied to develop new models for the self-calibration of aerial metric camera systems. It is well known and mathematically proven that additional parameters (APs) can compensate image distortions and remaining error sources by a rigorous photogrammetric bundle-block adjustment. Thus, kernel functions based on orthogonal wavelets (e. g., asymmetric Daubechies wave- lets, least asymmetric Daubechies wavelets, Battle-Lemarié wavelets, Meyer wavelets) are used to build the wavelets-based family of APs for self-calibrating digital frame cameras. These new APs are called wavelet APs. Its applications in rigorous tests are accomplished by using aerial images taken by an airborne digital mapping camera in situ and practical calibrations. The test results demonstrate that these orthogonal wavelet APs are applicable and largely avoid the risk of over-parameterization. Their external accuracy is evaluated using reliable and high precision check points in the calibration field.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 09 - Year 2024</p>
        </footer>
    </body>
    </html>
    