
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 04 - Year 2019</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 04 - Year 2019</h1>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000004/art00011;jsessionid=2g8c3f7hcmqic.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Cloud Detection Method for High Resolution Remote Sensing Imagery Based on the Spectrum and Texture of Superpixels
        </a>
    </h3>
    <div style="font-style: italic;">201904, pp. 257-268(12)</div>
    <div>Authors: Dong, Zhipeng; Wang, Mi; Li, Deren; Wang, Yanli; Zhang, Zhiqi</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Image cloud detection is an important part of high spatial resolution remote sensing imagery (HSRI) information processing and analysis. The spectral threshold selection of image cloud detection and the influence of cloud-like ground objects are two vital factors in determining cloud detection results of HSRI. With respect to these two issues, a novel cloud detection method for HSRI based on the spectrum and texture of superpixels is proposed in the paper. First, the adaptive image cloud detection spectral threshold is obtained according to the image equalization histogram. Second, the initial cloud detection result is obtained based on spectral threshold of the cloud detection and spectral attributes of superpixels. Third, the initial cloud detection result is refined based on the gray value and angular second moment of the superpixels local binary patterns texture to eliminate the influence of cloud-like ground objects. Finally, the cloud detection result is processed using the region growing algorithm and expansion algorithm to obtain an accurate cloud detection result. The experimental results show that the proposed method can obtain good cloud detection results.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000004/art00012;jsessionid=2g8c3f7hcmqic.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Machine Learning-Based Ensemble Prediction of Water-quality Variables Using Feature-level and Decision-level Fusion with Proximal Remote Sensing
        </a>
    </h3>
    <div style="font-style: italic;">201904, pp. 269-280(12)</div>
    <div>Authors: Peterson, Kyle T.; Sagan, Vasit; Sidike, Paheding; Hasenmueller, Elizabeth A.; Sloan, John J.; Knouft, Jason H.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The objectives of this study were to accurately model relationships between spectral reflectance and water-quality parameters, including blue-green algae phycocyanin, chlorophyll a, total suspended solids, turbidity, and total dissolved solids; evaluate feature-level fusion to spectral data for water-quality modeling; and evaluate the effectiveness of machine learning regression techniques and decision-level fusion for water-quality variable prediction. We introduce the application of canonical correlation analysis fusion as a method for water-based spectral analysis to overcome the low signal-to-noise ratio of the data. Water-quality variables and spectral reflectance were used to create predictive modelsviamachine learning regression models, including multiple linear regression, partial least-squares regression, Gaussian process regression, support vector machine regression, and extreme learning machine regression. The models were then combined using decision-level fusion. Results indicate that canonical correlation analysis feature-level fusion and machine learning techniques are superior to traditional methods.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000004/art00013;jsessionid=2g8c3f7hcmqic.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Active-Passive Spaceborne Data Fusion for Mapping Nearshore Bathymetry
        </a>
    </h3>
    <div style="font-style: italic;">201904, pp. 281-295(15)</div>
    <div>Authors: Forfinski-Sarkozi, Nicholas A.; Parrish, Christopher E.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                "In anticipation of the National Aeronautics and Space Administration (NASA) ICESat-2 mission, which will employ the Advanced Topographic Laser Altimeter System (ATLAS), a 532 nm photon-counting Light Detection and Ranging (lidar), we demonstrate a spaceborne data-fusion approach that has the potential to significantly shrink the global nearshore data gap often referred to as the "white ribbon". Bathymetry algorithms relying on multispectral imagery are conventionally limited by the availability ofin situreference depths, particularly in remote or difficult-to-map areas. Therefore, a completely spaceborne approach could greatly extend the usefulness of such algorithms. The approach is tested with data from NASA's airborne ICESat-2 ATLAS simulator, Multiple Altimeter Beam Experimental Lidar (MABEL), and passive optical imagery from Landsat-8 using an existing spectral-ratio algorithm. The output bathymetric data set agrees with high-resolution Fugro LADS MK II bathymetric data to within an RMS difference of 1.1 m. The spatiotemporal variability of areas where this spaceborne data-fusion approach will potentially be useful is evaluated, based on worldwide coastal water clarity as interpreted from Visible Infrared Imaging Radiometer Suite (VIIRS) Kd(490) data.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000004/art00014;jsessionid=2g8c3f7hcmqic.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Vehicle Detection in Aerial Images
        </a>
    </h3>
    <div style="font-style: italic;">201904, pp. 297-304(8)</div>
    <div>Authors: Yang, Michael Ying; Liao, Wentong; Li, Xinbo; Cao, Yanpeng; Rosenhahn, Bodo</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The detection of vehicles in aerial images is widely applied in many applications. Comparing with object detection in the ground view images, vehicle detection in aerial images remains a challenging problem because of small vehicle size and the complex background. In this paper, we propose a novel double focal loss convolutional neural network (DFL-CNN) framework. In the proposed framework, the skip connection is used in the CNN structure to enhance the feature learning. Also, the focal loss function is used to substitute for conventional cross entropy loss function in both of the region proposal network (RPN) and the final classifier. We further introduce the first large-scale vehicle detection dataset ITCVD with ground truth annotations for all the vehicles in the scene. We demonstrate the performance of our model on the existing benchmark German Aerospace Center (DLR) 3K dataset as well as the ITCVD dataset. The experimental results show that our DFL-CNN outperforms the baselines on vehicle detection.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000004/art00015;jsessionid=2g8c3f7hcmqic.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Integrated Relative Orientation Based on Point and Line Features via Plücker Coordinates
        </a>
    </h3>
    <div style="font-style: italic;">201904, pp. 305-311(7)</div>
    <div>Authors: Sheng, Qinghong; Yang, Rui; Xiao, Hui</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Relative orientation based on point and line features can significantly reduce the problems caused by single flight strip bending and strengthen geometric stability. Nevertheless, the dimensional difference of points and lines and the inconsistency of their methods of representation result in low calculation efficiency. In this paper, it is proposed that three-dimensional points and four-dimensional lines are uniformly represented by Plücker coordinates to achieve integrated coordinate transformation. According to the geometric conditions that lines intersect lines and planes intersect planes, a relative orientation model based on pointline via Plücker coordinates (P-LPRO) is established, and the error equation is linearized by the least-squares method. Experimental results show that the integrated adjustment solution of P-LPRO is more accurate than the traditional method and can achieve faster convergence speed.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000004/art00016;jsessionid=2g8c3f7hcmqic.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A New Pseudorigorous Lidar System Calibration Strategy with Minimal Requirements and Higher Capability
        </a>
    </h3>
    <div style="font-style: italic;">201904, pp. 313-321(9)</div>
    <div>Authors: Ritelli, M.; Habib, A.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Most times, rigorous airborne Light Detection and Ranging (LiDAR) system calibration is not possible because the raw measurements are unavailable, and pseudorigorous approaches that synthesize the raw measurements from the point cloud (and in some cases the trajectory) are used. TheQuasi-Rigorous/Quasi-Simplifiedis a new pseudorigorous approach proposed here which is more useful when compared to the existing pseudorigorous approaches (SimplifiedandQuasi-Rigorous) because it has the same minimal data requirements as theSimplifiedbut provides the higher capabilities of theQuasi-Rigorous. The experimental results compare theQuasi-Rigorous/Quasi-Simplifiedapproach to the existing pseudorigorous (SimplifiedandQuasi-Rigorous) and rigorous approaches. When compared to the existing approach with minimal requirements (theSimplified), the results show that theQuasi-Rigorous/Quasi-Simplifiedapproach is equally successful in significantly reducing the impact of systematic errors and offers additional capabilities in that it can be used on any type of terrain, can use nonparallel flight lines, and can incorporate control.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 04 - Year 2019</p>
        </footer>
    </body>
    </html>
    