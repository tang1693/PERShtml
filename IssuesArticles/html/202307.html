<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2023/00000089/00000007/art00006;jsessionid=1e5cts1kd2tq9.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Lightweight Conditional Convolutional Neural Network for Hyperspectral Image Classification
        </a>
    </h3>
    <div style="font-style: italic;">202307, pp. 413-423(11)</div>
    <div>Authors: Wu, Linfeng; Wang, Huajun; Wang, Huiqing</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Deep learning (dl), especially convolutional neural networks (cnns), has been proven to be an excellent feature extractor and widely applied to hyperspectral image (hsi) classification. However, dl is a computationally demanding algorithm with many parameters and a high computational burden, which seriously restricts the deployment ofdl-basedhsiclassification algorithms on mobile and embedded systems. In this paper, we propose an extremely lightweight conditional three-dimensional (3D) hsi with a double-branch structure to solve these problems. Specifically, we introduce a lightweight conditional 3D convolution to replace the conventional 3D convolution to reduce the computational and memory cost of the network and achieve flexiblehsifeature extraction. Then, based on lightweight conditional 3D convolution, we build two parallel paths to independently exploit and optimize the diverse spatial and spectral features. Furthermore, to precisely locate the key information, which is conducive to classification, a lightweight attention mechanism is carefully designed to refine extracted spatial and spectral features, and improve the classification accuracy with less computation and memory costs. Experiments on three publichsidata sets show that the proposed model can effectively reduce the cost of computation and memory, achieve high execution speed, and better classification performance compared with several recent dl-based models.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2023/00000089/00000007/art00007;jsessionid=1e5cts1kd2tq9.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Multi-Level Perceptual Network for Urban Building Extraction from High-Resolution Remote Sensing Images
        </a>
    </h3>
    <div style="font-style: italic;">202307, pp. 427-434(8)</div>
    <div>Authors: Sun, Yueming; Chen, Jinlong; Huang, Xiao; Zhang, Hongsheng</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Building extraction from high-resolution remote sensing images benefits various practical applications. However, automation of this process is challenging due to the variety of building surface coverings, complex spatial layouts, different types of structures, and tree occlusion. In this study, we propose a multilayer perception network for building extraction from high-resolution remote sensing images. By constructing parallel networks at different levels, the proposed network retains spatial information of varying feature resolutions and uses the parsing module to perceive the prominent features of buildings, thus enhancing the model's parsing ability to target scale changes and complex urban scenes. Further, a structure-guided loss function is constructed to optimize building extraction edges. Experiments on multi-source remote sensing data sets show that our proposed multi-level perception network presents a superior performance in building extraction tasks.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2023/00000089/00000007/art00008;jsessionid=1e5cts1kd2tq9.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Classifying Building Roof Damage Using High Resolution Imagery for Disaster Recovery
        </a>
    </h3>
    <div style="font-style: italic;">202307, pp. 437-443(7)</div>
    <div>Authors: Gonsoroski, Elaina; Ahn, Yoonjung; Harville, Emily W.; Countess, Nathaniel; Lichtveld, Maureen Y.; Pan, Ke; Beitsch, Leslie; Sherchan, Samendra P.; Uejio, Christopher K.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Post-hurricane damage assessments are often costly and time-consuming. Remotely sensed data provides a complementary method of data collection that can be completed comparatively quickly and at relatively low cost. This study focuses on 15 Florida counties impacted by Hurricane Michael (2018), which had category 5 strength winds at landfall. The present study evaluates the ability of aerial imagery collected to cost-effectively measure blue tarps on buildings for disaster impact and recovery. A support vector machine model classified blue tarp, and parcels received a damage indicator based on the model's prediction. The model had an overall accuracy of 85.3% with a sensitivity of 74% and a specificity of 96.7%. The model results indicated approximately 7% of all parcels (27 926 residential and 4431 commercial parcels) in the study area as having blue tarp present. The study results may benefit jurisdictions that lacked financial resources to conduct on-the-ground damage assessments.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2023/00000089/00000007/art00009;jsessionid=1e5cts1kd2tq9.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Estimation of the Forest Stand Biomass and Greenhouse Gas Emissions Using Lidar Surveys
        </a>
    </h3>
    <div style="font-style: italic;">202307, pp. 445-454(10)</div>
    <div>Authors: Sultanova, Rida; Mustafin, Radik</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                At the research points, the relationship between the Normalized Difference Vegetation and Normalized Green Red Difference indices is characterized by a determination coefficient equal to 0.52. The estimation of the emission of carbon dioxide and nitrogen oxide in the forest air at an altitude of 40 m above the level of the soil cover during the growing season showed differences in their values during the daytime and at night. The results helped determine promising methods of inventory of the carbon landfill forest area for aboveground woody biomass assessment based on data obtained from several sources and land forest estimation research. The research involved: 1) integration of an unmanned aerial vehicle -based digital camera and lidar sensors in order to optimize the efficiency and cost of data collection; 2) taking advantage of high-resolution aerial photographs and sparse lidar point clouds using an information fusion approach and the ability to compensate for their shortcomings.
            </details>
        </div>
</article>
