<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000006/art00006;jsessionid=ehe231hgm4fh1.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Real-Time Semantic Segmentation of Remote Sensing Images for Land Management
        </a>
    </h3>
    <div style="font-style: italic;">202406, pp. 335-343(9)</div>
    <div>Authors: Zhang, Yinsheng; Ji, Ru; Hu, Yuxiang; Yang, Yulong; Chen, Xin; Duan, Xiuxian; Shan, Huilin</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Remote sensing image segmentation is a crucial technique in the field of land management. However, existing semantic segmentation networks require a large number of floating-point operations (FLOPs) and have long run times. In this paper, we propose a dual-path feature aggregation network (DPFANet) specifically designed for the low-latency operations required in land management applications. Firstly, we use four sets of spatially separable convolutions with varying dilation rates to extract spatial features. Additionally, we use an improved version of MobileNetV2 to extract semantic features. Furthermore, we use an asymmetric multi-scale fusion module and dual-path feature aggregation module to enhance feature extraction and fusion. Finally, a decoder is constructed to enable progressive up-sampling. Experimental results on the Potsdam data set and the Gaofen image data set (GID) demonstrate that DPFANet achieves overall accuracy of 92.2% and 89.3%, respectively. The FLOPs are 6.72 giga and the number of parameters is 2.067 million.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000006/art00007;jsessionid=ehe231hgm4fh1.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Land Use Change in the Yangtze River Economic Belt during 2010 to 2020 and Future Comprehensive Prediction Based on Markov and ARIMA Models
        </a>
    </h3>
    <div style="font-style: italic;">202406, pp. 345-354(10)</div>
    <div>Authors: Zheng, Haotian; Yu, Fan; Wan, Huawei; Shi, Peirong; Wang, Haonan</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The key data for accurate prediction is of great significance to accurately carry out the next step of sustainable land use development plan according to the demand of China. Consequently, the main purposes of our study are: (1) to delineate the characteristics of land use transitions within the Yangtze River Economic Belt; (2) to use the Markov model and the autoregressive integrated moving average (ARIMA) model for comparative analysis and prediction of land use distribution. This study analyzes land use/cover change (LUCC) data from 2010 and 2020 using the land use transition matrix, dynamic degree, and comprehensive index model and predicts 2025 land use by the Markov model. The study identifies a reduction in land usage over 11 years, particularly in grassland. The Markov and ARIMA models' significance is 0.002 (P < 0.01), showing arable land and woodland dominance, with varying changes in other land types.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000006/art00009;jsessionid=ehe231hgm4fh1.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Improved YOLO Network for Insulator and Insulator Defect Detection in UAV Images
        </a>
    </h3>
    <div style="font-style: italic;">202406, pp. 355-361(7)</div>
    <div>Authors: Zhou, Fangrong; Liu, Lifeng; Hu, Hao; Jin, Weishi; Zheng, Zezhong; Li, Zhongnian; Ma, Yi; Wang, Qun</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The power grid plays a vital role in the construction of livelihood projects by transmitting electrical energy. In the event of insulator explosions on power grid towers, these insulators may detach, presenting potential safety risks to transmission lines. The identification of such failures relies on the examination of images captured by unmanned aerial vehicles (UAVs). However, accurately detecting insulator defects remains challenging, particularly when dealing with variations in size. Existing methods exhibit limited accuracy in detecting small objects. In this paper, we propose a novel detection method that incorporates the convolutional block attention module (CBAM) as an attention mechanism into the backbone of the "you only look once" version 5 (YOLOv5) model. Additionally, we integrate a residual structure into the model to learn additional information and features related to insulators, thereby enhancing detection efficiency. Experimental results demonstrate that our proposed method achieved F1 scores of 0.87 for insulator detection and 0.89 for insulator defect detection. The improved YOLOv5 network shows promise in detecting insulators and their defects in UAV images.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000006/art00011;jsessionid=ehe231hgm4fh1.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Monitoring an Ecosystem in Crisis: Measuring Seagrass Meadow Loss Using Deep Learning in Mosquito Lagoon, Florida
        </a>
    </h3>
    <div style="font-style: italic;">202406, pp. 363-370(8)</div>
    <div>Authors: Insalaco, Stephanie A.; Herrero, Hannah V.; Limber, Russ; Oliver, Clancy; Wolfson, William B.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The ecosystem of Mosquito Lagoon, Florida, has been rapidly deteriorating since the 2010s, with a notable decline in keystone seagrass species. Seagrass is vital for many species in the lagoon, but nutrient overloading, algal blooms, boating, manatee grazing, and other factors have led to its loss. To understand this decline, a deep neural network analyzed Landsat imagery from 2000 to 2020. Results showed significant seagrass loss post-2013, coinciding with the 2011–2013 super algal bloom. Seagrass abundance varied annually, with the model performing best in years with higher seagrass coverage. While the deep learning method successfully identified seagrass, it also revealed that recent seagrass coverage is almost non-existent. This monitoring approach could aid in ecosystem recovery if coupled with appropriate policies for Mosquito Lagoon's restoration.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000006/art00012;jsessionid=ehe231hgm4fh1.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Real-Time Cross-View Image Matching and Camera Pose Determination for Unmanned Aerial Vehicles
        </a>
    </h3>
    <div style="font-style: italic;">202406, pp. 371-381(11)</div>
    <div>Authors: Chen, Long; Wu, Bo; Duan, Ran; Chen, Zeyu</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In global navigation satellite systems (GNSS)-denied environments, vision-based methods are commonly used for the positioning and navigation of aerial robots. However, traditional methods often suffer from accumulative estimation errors over time, leading to trajectory drift and lack real-time performance, particularly in large-scale scenarios. This article presents novel approaches, including feature-based cross-view image matching and the integration of visual odometry and photogrammetric space resection for camera pose determination in real-time. Experimental evaluation with real UAV datasets demonstrated that the proposed method reliably matches features in cross-view images with large differences in spatial resolution, coverage, and perspective views, achieving a root-mean-square error of 4.7 m for absolute position error and 0.33° for rotation error, and delivering real-time performance of 12 frames per second (FPS) when implemented in a lightweight edge device onboard UAV. This approach offters potential for diverse intelligent UAV applications in GNSS-denied environments based on real-time feedback control.
            </details>
        </div>
</article>
