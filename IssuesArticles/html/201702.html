
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 02 - Year 2017</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 02 - Year 2017</h1>
            <p>Displaying articles from Issue 02 - Year 2017.</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2017/00000083/00000002/art00010;jsessionid=6vzun0pgik7m.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Shadow Identification in High Resolution Satellite Images in the Presence of Water Regions
        </a>
    </h3>
    <div style="font-style: italic;">201702, pp. 87-94(8)</div>
    <div>Authors: Mostafa, Y.; Abdelhafiz, A.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Shadow is a main obstacle in features extraction from high resolution satellite images. Water areas provide low reflectance; therefore, they are commonly classified as shadow. In traditional shadow identification procedures, shadow is identified from image histogram in which, water and shadow pixels are mixed. In this study, a new image index is presented. The index histogram separates non-shadow, shadow, and water pixels using a proposed threshold method. The developed index is then integrated into a complete approach for shadow identification. Five study areas including water, shadow on water, and shallow water regions are tested using the developed approach. Afterward, an accuracy assessment is made to demonstrate the approach efficiency. The experimental results show that the proposed approach achieved overall accuracy about ninety-three percent. The rate of detecting shadow on water is about seventy-four percent. On the other hand, shallow water is still misclassified as shadow.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2017/00000083/00000002/art00011;jsessionid=6vzun0pgik7m.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Aerial Lidar Point Cloud Voxelization with its 3D Ground Filtering Application
        </a>
    </h3>
    <div style="font-style: italic;">201702, pp. 95-107(13)</div>
    <div>Authors: Wang, Liying; Xu, Yan; Li, Yu</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Compared to raster grid, Triangulated Irregular Network (TIN), and point cloud, the benefit of voxel representation lies in that the implicit notion of adjacency and the true 3D representation can be presented simultaneously. A binary voxel-based data (BVD) model is proposed to reconstruct aerial lidar point cloud and based on the constructed model 3D ground filtering (V3GF) is developed for separating ground points from unground ones. The proposed V3GF algorithm selects the lowest voxels with a value of 1 as ground seeds and then labels them and their 3D connected set as ground voxels. The ISPRS benchmark dataset are used to compare the performance of V3GF with those of eight other publicized filtering methods. Results indicate that the V3GF improves on Axelsson's performance on five samples in terms of total error. The average Kappa coefficients for sites with relatively flat urban areas, rough slope and discontinuous surfaces are 92.49 percent, 72.23 percent and 61.27 percent, respectively.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2017/00000083/00000002/art00013;jsessionid=6vzun0pgik7m.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Integrating Multiple Textural Features for Remote Sensing Image Change Detection
        </a>
    </h3>
    <div style="font-style: italic;">201702, pp. 109-121(13)</div>
    <div>Authors: Li, Qingyu; Huang, Xin; Wen, Dawei; Liu, Hui</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper proposes a multi-texture change detection method by integrating macro- and micro-texture features. Macro-textures are related to the information defined by the whole image scene, while micro-textures describe distributions and relationships of the gray levels within a local window. Moreover, we propose two strategies, random forests (RF) and a fuzzy set model, to integrate different characteristics of the textures. Experiments were conducted onZY-3(the first civilian high-resolution stereo mapping satellite of China) orthographic images of the cities of Wuhan and Tokyo, as well as WorldView-2 multi-spectral images of the city of Kuala Lumpur. Results showed that the wavelet-based features obtained the highest accuracy among the macro-textures, while the morphological attributes obtained the best results for the micro-textures. By integrating both micro- and macro-textures, the texture combination using both RF and a fuzzy set model can further improve the accuracy of change detection.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2017/00000083/00000002/art00014;jsessionid=6vzun0pgik7m.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            On the Fusion of Lidar and Aerial Color Imagery to Detect Urban Vegetation and Buildings
        </a>
    </h3>
    <div style="font-style: italic;">201702, pp. 123-136(14)</div>
    <div>Authors: Bandyopadhyay, Madhurima; van Aardt, Jan A.N.; Cawse-Nicholson, Kerry; Ientilucci, Emmett</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Three-dimensional (3D) data from light detection and ranging (lidar) sensor have proven advantageous in the remote sensing domain for characterization of object structure and dimensions. Fusion-based approaches of lidar and aerial imagery also becoming popular. In this study, aerial color (RGB) imagery, along with co-registered airborne discrete lidar data were used to separate vegetation and buildings from other urban classes/cover-types, as a precursory step towards the assessment of urban forest biomass. Both spectral and structural features such as object height, distribution of surface normals from the lidar, and a novel vegetation metric derived from combined lidar andRGBimagery, referred to as the lidar-infused vegetation index (LDVI) were used in this classification method. The proposed algorithm was tested on different cityscape regions to verify its robustness. Results showed a good separation of buildings and vegetation from other urban classes with on average an overall classification accuracy of 92 percent, with a kappa statistic of 0.85. These results bode well for the operational fusion of lidar andRGBimagery, often flown on the same platform, towards improved characterization of the urban forest and built environments.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2017/00000083/00000002/art00015;jsessionid=6vzun0pgik7m.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Automatic 3D Surface Co-Registration Using Keypoint Matching
        </a>
    </h3>
    <div style="font-style: italic;">201702, pp. 137-151(15)</div>
    <div>Authors: Persad, Ravi Ancil; Armenakis, Costas</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A framework for co-registering multi-temporal3Dpoint cloud surfaces (PCSs) is presented, which addresses the co-registration of urban and non-urban3Dsurfaces formed by3Dpoints. These surfaces are acquired from different surface measurement sensors and are in different coordinate systems. No prior information about initial transformation parameters or proximate matching is assumed. A keypoint matching approach is proposed to co-register twoPCSs. First, surface curvature information is utilized for scale-invariant keypoint extraction. Then, every keypoint is characterized by a scale, rotation, and translation invariant surface descriptor called the radial geodesic distance-slope histogram. Keypoints with similar surface descriptors on the twoPCSsare matched using bipartite graph matching. Given scale, rotation and translation changes between PCS pairs, co-registration tests on multi-sensor urban and non-urban datasets gave rotation errors from 0.017° to 0.023°, translation errors from 0.007 m to 0.013 m and scale factor errors from 0.0002 to 0.0014.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2017/00000083/00000002/art00017;jsessionid=6vzun0pgik7m.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Using Relative Orientation to Improve the Accuracy of Exterior Orientation Parameters of Low Cost POS
        </a>
    </h3>
    <div style="font-style: italic;">201702, pp. 153-161(9)</div>
    <div>Authors: Yan, Wenlin; Bastos, Luísa; Madeira, Sérgio; Magalhães, Américo; Gonçalves, José A.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A simple and inexpensivePOSfor application to light airborne platforms was developed, and a modified Kalman Filtering was designed to integrateGNSS/IMU/Image data, which uses the relative orientation information to improve the accuracy of the exterior orientation parameters. The precise relative orientation of conjugate images was obtained using aSIFT/SFMmatching algorithm. The relative exterior orientations were transformed from the camera frame to the navigation frame before they were used as external update information of the Kalman Filter. Combining all the relative orientation information retrieved from images, the Kalman Filter can give an improved output for the exterior orientation parameters. Airborne result from the tests of one straight strip shows that the heading accuracy of aGNSS/MEMS-IMUwas improved from 0.26° to 0.10°, and the result from one closed strip shows that heading accuracy was improved from 1.11° to 0.85° and roll accuracy from 0.54° to 0.43°.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>Generated automatically for Issue 02 - Year 2017</p>
        </footer>
    </body>
    </html>
    