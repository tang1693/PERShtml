<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000004/art00008;jsessionid=2quh4khn56cjt.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Using Improved YOLOv5 and SegFormer to Extract Tailings Ponds from Multi-Source Data
        </a>
    </h3>
    <div style="font-style: italic;">202404, pp. 223-231(9)</div>
    <div>Authors: Sun, Zhenhui; Xu, Ying; Wang, Dongchuan; Meng, Qingyan; Sun, Yunxiao</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper proposes a framework that combines the improved "You Only Look Once" version 5 (YOLOv5) and SegFormer to extract tailings ponds from multi-source data. Points of interest (POIs) are crawled to capture potential tailings pond regions. Jeffries–Matusita distance is used to evaluate the optimal band combination. The improved YOLOv5 replaces the backbone with the PoolFormer to form a PoolFormer backbone. The neck introduces the CARAFE operator to form a CARAFE feature pyramid network neck (CRF-FPN). The head is substituted with an efficiency decoupled head. POIs and classification data optimize improved YOLOv5 results. After that, the SegFormer is used to delineate the boundaries of tailings ponds. Experimental results demonstrate that the mean average precision of the improved YOLOv5s has increased by 2.78% compared to the YOLOv5s, achieving 91.18%. The SegFormer achieves an intersection over union of 88.76% and an accuracy of 94.28%.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000004/art00009;jsessionid=2quh4khn56cjt.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            GDP Spatialization in City of Zhengzhou Based on NPP/VIIRS Night-time Light and Socioeconomic Statistical Data Using Machine Learning
        </a>
    </h3>
    <div style="font-style: italic;">202404, pp. 233-240(8)</div>
    <div>Authors: Ullah, Inam; Li, Weidong; Meng, Fanqian; Nadeem, Muhammad Imran; Ahmed, Kanwal</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This article introduces a comprehensive methodology for mapping and assessing the urban built-up areas and establishing a spatial gross domestic product (GDP) model for Zhengzhou using night-time light (NTL) data, alongside socioeconomic statistical data from 2012 to 2017. Two supervised sorting algorithms, namely the support vector machine (SVM) algorithm and the deep learning (DL) algorithm, which includes the U-Net and fully convolutional neural (FCN) network models, are proposed for urban built-up area identification and image classification. Comparisons with Municipal Bureau of Statistics data highlight the U-Net neural network model exhibits superior accuracy, especially in areas with diverse characteristics. For each year from 2012 to 2017, a spatial GDP model was developed based on Zhengzhou's urban GDP and U-Net sorted images. This research provides valuable insights into urban development and economic assessment for the city.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000004/art00010;jsessionid=2quh4khn56cjt.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Monitoring Based on InSAR for the Xinmo Village Landslide in Western Sichuan, China
        </a>
    </h3>
    <div style="font-style: italic;">202404, pp. 243-249(7)</div>
    <div>Authors: Zheng, Zezhong; Yu, Shuang; Xie, Chuhang; Yang, Jiali; Zhu, Mingcang; He, Yong</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A devastating landslide incident occurred on 24 June 2017, causing huge losses for Xinmo Village in western Sichuan. In this paper, we used two interferometric synthetic aperture radar (InSAR) methods, permanent scatterer (PS)-InSAR and small baseline subset (SBAS)- InSAR, to analyze deformation signals in the area in the 2 years leading up to the landslide event using Sentinel-1A ascending data. Our experimental findings from PS-InSAR and SBAS-InSAR revealed that the deformation rates in the study region ranged between –50 to 20 mm/year and –30 to 10 mm/year, respectively. Furthermore, the deformation rates of the same points, as determined by these methods, exhibited a significant increase prior to the event. We also investigated the causal relationship between rainfall and landslide events, demonstrating that deformation rates correlate with changes in rainfall, albeit with a time lag. Therefore, using time-series InSAR for landslide monitoring in Xinmo Village is a viable approach.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000004/art00011;jsessionid=2quh4khn56cjt.x-ic-live-03" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Investigation of Underwater Photogrammetry Method with Cost-Effective Action Cameras and Comparative Analysis between Reconstructed 3D Point Clouds
        </a>
    </h3>
    <div style="font-style: italic;">202404, pp. 251-259(9)</div>
    <div>Authors: Hamal, Seda Nur Gamze; Ulvi, Ali</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Currently, digital cameras and equipment used underwater are often inaccessible to the general public due to their professional-grade quality and high cost. Therefore alternative solutions have been sought that are both cost-effective and suitable for nonprofessional use. A review of the literature shows that researchers primarily use GoPro action cameras, while other action cameras with similar capabilities are rarely used. This study thus examines underwater photogrammetry methods using a widely recognized action camera as a reference and compares it with another camera of similar characteristics as a potential alternative. For a comprehensive temporal analysis in underwater studies, both cameras were used to capture photographic and video imagery, and the resulting 3D point clouds were compared. Comparison criteria included data collection and processing times, point cloud densities, cloud-to-cloud analysis, and assessments of surface density and roughness. Having analysed, the study concluded that the proposed alternative action camera can feasibly be used in underwater photogrammetry.
            </details>
        </div>
</article>
