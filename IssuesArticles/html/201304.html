
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 04 - Year 2013</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 04 - Year 2013</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004/art00001;jsessionid=apagc42ht82c1.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Rational Function Model in Processing Historical Aerial Photographs
        </a>
    </h3>
    <div style="font-style: italic;">201304, nan</div>
    <div>Authors: Ma, Ruijin</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A large amount of aerial photographs were acquired early 1900s for various purposes. These aerial photographs provide valuable baseline data for a variety of environmental studies. To be used for analysis together with other data, these photographs need to be referenced to a ground coordinate system. Often large topographical variations in photo ground coverage demand these photographs be orthorectified using photogrammetric methods for acceptable mapping accuracy. However, camera models for these historical photographs are usually unavailable. Additionally, land-cover change over the years makes it a great challenge to collect accurate control points for calculating camera models. This study investigated the rational function model to orthorectify historical aerial photographs. The analysis results suggest that while the linear, the 2ndorder, and the 3rd order RFMs are all able to closely approximate the rigorous frame camera model, the linear RFM is optimal in representing imaging geometries of historical photographs..
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004/art00002;jsessionid=apagc42ht82c1.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Experimental Comparison of Semi-supervised Learning Algorithms for Multispectral Image Classification
        </a>
    </h3>
    <div style="font-style: italic;">201304, pp. 347-357(11)</div>
    <div>Authors: Tu, Enmei; Yang, Jie; Fang, Jiangxiong; Jia, Zhenghong; Kasabov, Nikola</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Semi-Supervised Learning (SSL) method has recently caught much attention in the fields of machine learning and computer vision owing to its superiority in classifying abundant unlabelled samples using a few labeled samples. The goal of this paper is to provide an experimental efficiency comparison between graph based SSL algorithms and traditional supervised learning algorithms (e.g., support vector machines) for multispectral image classification. This research shows that SSL algorithms generally outperform supervised learning algorithms in both classification accuracy and anti-noise ability. In the experiments carried out on two data sets (hyperspectral image and Landsat image), the mean overall accuracies (OAs) of supervised learning algorithms are 15 percent and 86 percent, while the mean OAs of SSL algorithms are 26 percent and 99 percent. To overcome the polynomial complexity of SSL algorithms, we also developed a linear-complexity algorithm by employing multivariate Taylor Series Expansion (TSE) and Woodbury Formula.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004/art00003;jsessionid=apagc42ht82c1.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Applicability of Multi-date Land Cover Mapping using Landsat-5 TM Imagery in the Northeastern US
        </a>
    </h3>
    <div style="font-style: italic;">201304, pp. 359-368(10)</div>
    <div>Authors: MacLean, Meghan Graham; Congalton, Russell G.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In many situations, multi-date image classification improves classification accuracies. However, with improved accuracies comes increased image processing time and effort. This work investigates the circumstances under which multi-date image classification is significantly better than single-date classification using Landsat-5 TM imagery for southeastern New Hampshire. Multiple Landsat images were processed for every three years from 1986 to 2010 and classified using an object-based image analysis approach (OBIA) with a classification and regression tree (CART) technique. Two maps were created for each of the mapping years, one using a single image, and another using multiple images from that year. The multi-date classification process generally performed better than the single-date process. However, the significance of the improvement was primarily dependent on the accuracy of the single-date map. Therefore, if the accuracy of the singledate classification is acceptable, it may not be necessary to perform the multi-date classification.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004/art00004;jsessionid=apagc42ht82c1.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Pedestrian Network Extraction from Fused Aerial Imagery (Orthoimages) and Laser Imagery (Lidar)
        </a>
    </h3>
    <div style="font-style: italic;">201304, pp. 369-379(11)</div>
    <div>Authors: Kasemsuppakorn, Piyawan; Karimi, Hassan A.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A pedestrian network is a topological map that contains the geometric relationship between pedestrian path segments (e.g., sidewalk, crosswalk, footpath), which is needed in a variety of applications, such as pedestrian navigation services. However, current pedestrian networks are not widely available. In an effort to provide an automatic means for creating pedestrian networks, this paper presents a methodology for extracting pedestrian network from aerial and laser images. The methodology consists of data preparation and four steps: object filtering, pedestrian path region extraction, pedestrian network construction, and raster to vector conversion. An experiment, using ten images, was conducted to evaluate the performance of the methodology. Evaluation results indicate that the methodology can extract sidewalk, crosswalk, footpath, and building entrances; it collects pedestrian networks with 61 percent geometrical completeness, 67.35 percent geometrical correctness, 71 percent topological completeness and 51.38 percent topological correctness.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004/art00005;jsessionid=apagc42ht82c1.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Automatic Camera Calibration in Close Range Photogrammetry
        </a>
    </h3>
    <div style="font-style: italic;">201304, pp. 381-388(8)</div>
    <div>Authors: Fraser, Clive S.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Automatic camera calibration using self-calibration with the aid of coded targets is now very much the norm in close-range photogrammetry. This is irrespective of whether the cameras to be calibrated are high-end metric, or the digital SLRs and consumer-grade models that are increasingly being employed for image-based 3D measurement. Automation has greatly simplified the calibration task, but there are real prospects that important camera calibration issues may be overlooked in what has become an almost black-box operation. This paper discusses the impact of a number of such issues, some of which relate to the functional model adopted for self-calibration, and others to practical aspects which need to be taken into account when pursuing optimal calibration accuracy and integrity. Issues discussed include interior orientation stability, calibration reliability, focal plane distortion, image point distribution, variation in lens distortion with image scale, color imagery and chromatic aberration, and whether 3D object space control is warranted. By appreciating and accounting for these issues, users of automatic camera calibration will enhance the prospect of achieving an optimal recovery of scene-independent camera calibration parameters.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004/art00001;jsessionid=18acx42aom268.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Rational Function Model in Processing Historical Aerial Photographs
        </a>
    </h3>
    <div style="font-style: italic;">201304, nan</div>
    <div>Authors: Ma, Ruijin</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A large amount of aerial photographs were acquired early 1900s for various purposes. These aerial photographs provide valuable baseline data for a variety of environmental studies. To be used for analysis together with other data, these photographs need to be referenced to a ground coordinate system. Often large topographical variations in photo ground coverage demand these photographs be orthorectified using photogrammetric methods for acceptable mapping accuracy. However, camera models for these historical photographs are usually unavailable. Additionally, land-cover change over the years makes it a great challenge to collect accurate control points for calculating camera models. This study investigated the rational function model to orthorectify historical aerial photographs. The analysis results suggest that while the linear, the 2ndorder, and the 3rd order RFMs are all able to closely approximate the rigorous frame camera model, the linear RFM is optimal in representing imaging geometries of historical photographs..
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004/art00002;jsessionid=18acx42aom268.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Experimental Comparison of Semi-supervised Learning Algorithms for Multispectral Image Classification
        </a>
    </h3>
    <div style="font-style: italic;">201304, pp. 347-357(11)</div>
    <div>Authors: Tu, Enmei; Yang, Jie; Fang, Jiangxiong; Jia, Zhenghong; Kasabov, Nikola</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Semi-Supervised Learning (SSL) method has recently caught much attention in the fields of machine learning and computer vision owing to its superiority in classifying abundant unlabelled samples using a few labeled samples. The goal of this paper is to provide an experimental efficiency comparison between graph based SSL algorithms and traditional supervised learning algorithms (e.g., support vector machines) for multispectral image classification. This research shows that SSL algorithms generally outperform supervised learning algorithms in both classification accuracy and anti-noise ability. In the experiments carried out on two data sets (hyperspectral image and Landsat image), the mean overall accuracies (OAs) of supervised learning algorithms are 15 percent and 86 percent, while the mean OAs of SSL algorithms are 26 percent and 99 percent. To overcome the polynomial complexity of SSL algorithms, we also developed a linear-complexity algorithm by employing multivariate Taylor Series Expansion (TSE) and Woodbury Formula.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004/art00003;jsessionid=18acx42aom268.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Applicability of Multi-date Land Cover Mapping using Landsat-5 TM Imagery in the Northeastern US
        </a>
    </h3>
    <div style="font-style: italic;">201304, pp. 359-368(10)</div>
    <div>Authors: MacLean, Meghan Graham; Congalton, Russell G.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In many situations, multi-date image classification improves classification accuracies. However, with improved accuracies comes increased image processing time and effort. This work investigates the circumstances under which multi-date image classification is significantly better than single-date classification using Landsat-5 TM imagery for southeastern New Hampshire. Multiple Landsat images were processed for every three years from 1986 to 2010 and classified using an object-based image analysis approach (OBIA) with a classification and regression tree (CART) technique. Two maps were created for each of the mapping years, one using a single image, and another using multiple images from that year. The multi-date classification process generally performed better than the single-date process. However, the significance of the improvement was primarily dependent on the accuracy of the single-date map. Therefore, if the accuracy of the singledate classification is acceptable, it may not be necessary to perform the multi-date classification.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004/art00004;jsessionid=18acx42aom268.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Pedestrian Network Extraction from Fused Aerial Imagery (Orthoimages) and Laser Imagery (Lidar)
        </a>
    </h3>
    <div style="font-style: italic;">201304, pp. 369-379(11)</div>
    <div>Authors: Kasemsuppakorn, Piyawan; Karimi, Hassan A.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A pedestrian network is a topological map that contains the geometric relationship between pedestrian path segments (e.g., sidewalk, crosswalk, footpath), which is needed in a variety of applications, such as pedestrian navigation services. However, current pedestrian networks are not widely available. In an effort to provide an automatic means for creating pedestrian networks, this paper presents a methodology for extracting pedestrian network from aerial and laser images. The methodology consists of data preparation and four steps: object filtering, pedestrian path region extraction, pedestrian network construction, and raster to vector conversion. An experiment, using ten images, was conducted to evaluate the performance of the methodology. Evaluation results indicate that the methodology can extract sidewalk, crosswalk, footpath, and building entrances; it collects pedestrian networks with 61 percent geometrical completeness, 67.35 percent geometrical correctness, 71 percent topological completeness and 51.38 percent topological correctness.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000004/art00005;jsessionid=18acx42aom268.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Automatic Camera Calibration in Close Range Photogrammetry
        </a>
    </h3>
    <div style="font-style: italic;">201304, pp. 381-388(8)</div>
    <div>Authors: Fraser, Clive S.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Automatic camera calibration using self-calibration with the aid of coded targets is now very much the norm in close-range photogrammetry. This is irrespective of whether the cameras to be calibrated are high-end metric, or the digital SLRs and consumer-grade models that are increasingly being employed for image-based 3D measurement. Automation has greatly simplified the calibration task, but there are real prospects that important camera calibration issues may be overlooked in what has become an almost black-box operation. This paper discusses the impact of a number of such issues, some of which relate to the functional model adopted for self-calibration, and others to practical aspects which need to be taken into account when pursuing optimal calibration accuracy and integrity. Issues discussed include interior orientation stability, calibration reliability, focal plane distortion, image point distribution, variation in lens distortion with image scale, color imagery and chromatic aberration, and whether 3D object space control is warranted. By appreciating and accounting for these issues, users of automatic camera calibration will enhance the prospect of achieving an optimal recovery of scene-independent camera calibration parameters.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 04 - Year 2013</p>
        </footer>
    </body>
    </html>
    