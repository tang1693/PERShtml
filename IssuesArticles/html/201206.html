<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2012/00000078/00000006/art00001;jsessionid=6mv783v2g663c.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Registration of Multisource Satellite Images by Thin-plate Splines with Highly Reliable Conjugate Points
        </a>
    </h3>
    <div style="font-style: italic;">201206, nan</div>
    <div>Authors: Wu, Joz; Chang, Chi; Tsai, Hsien-Yu; Liu, Ming-Che</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Image registration is essential for geospatial information systems analysis, which usually involves integrating multitemporal and multispectral datasets from remote optical and radar sensors. An algorithm that deals with feature extraction, keypoint matching, outlier detection, and image warping is experimented in this study. The methods currently available in the literature rely on techniques, such as the scale-invariant feature transform, between-edge cost minimization, normalized cross correlation, least-squares image matching, random sample consensus, iterated data snooping, and thin-plate splines. Their basics are highlighted and encoded into a computer program. The test images are excerpts from digital files created by the multispectral SPOT-5 and Formosat-2 sensors, side-looking ERS-2, and Envisat synthetic aperture radars, and by the panchromatic Ikonos and QuickBird sensors. Suburban areas, housing rooftops, the countryside and hilly plantations are studied. The co-registered images are displayed with block sub-images in a criss-cross pattern. Besides the imagery, the registration accuracy is expressed by the root mean square error. Toward the end, this paper also includes a few opinions on issues that are believed to hinder a correct correspondence between diverse images.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2012/00000078/00000006/art00002;jsessionid=6mv783v2g663c.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Framework for Supervised Image Classification with Incomplete Training Samples
        </a>
    </h3>
    <div style="font-style: italic;">201206, pp. 595-604(10)</div>
    <div>Authors: Guo, Qinghua; Li, Wenkai; Liu, Desheng; Chen, Jin</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                For traditional supervised classification methods, all landcover types need to be exhaustively labeled to train the classifier. However, there are situations where the training sample classes are incomplete due to a lack of understanding of ground cover types in the image. In this study we propose a one-by-one (OBO) classification framework to address this incomplete training sample problem. The OBO approach is based on a one-class classifier (positive and unlabeled learning algorithm), and it extracts the land-cover type from the image one at a time. The performance of the proposed method was compared with a traditional supervised classifier using a high spatial resolution image. The average accuracy of the new method is 76.34 percent across different training sample sizes, whereas the accuracy of the classical approach is 66.46 percent, with an increase of 9.88 percent. The results demonstrate that the proposed new framework provides significantly higher classification accuracy than the classical approach at the 95 percent confidence level, and shows promise in dealing with the incomplete training sample problem for supervised image classification.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2012/00000078/00000006/art00003;jsessionid=6mv783v2g663c.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Neuro-fuzzy Classification of Submarine Lava Flow Morphology
        </a>
    </h3>
    <div style="font-style: italic;">201206, pp. 605-616(12)</div>
    <div>Authors: McClinton, J. Timothy; White, Scott M.; Sinton, John M.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper presents a novel approach to semi-automated classification of volcanic morphology on the seafloor using high-resolution multibeam sonar bathymetry and side-scan sonar backscatter imagery. The classification methodology combines a fuzzy inference system and neural network theory in an adaptive neuro-fuzzy inference system (ANFIS) and is capable of rapidly classifying submarine lava morphology based on bathymetry-derived surface geometry and backscatter-derived attributes of acoustics and texture. The system has been applied to a study area on a seafloor spreading ridge, the Gal√°pagos Spreading Center (GSC), in order to quantify the distribution and relative abundance of lava flow types, which can be used to indicate variations in eruption and emplacement dynamics. A detailed assessment shows the classification has an overall accuracy of almost 90 percent with a kappa coefficient of 0.84. The neuro-fuzzy method described here is shown to be an efficient and reliable tool for classification of submarine lava morphology.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2012/00000078/00000006/art00004;jsessionid=6mv783v2g663c.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Error Propagation in Raster Data Integration
        </a>
    </h3>
    <div style="font-style: italic;">201206, pp. 617-624(8)</div>
    <div>Authors: Christman, Zachary J.; Rogan, John</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Integrating raster-based categorical maps from multiple sources necessitates the transformation of geometric characteristics to compare maps, as in land change analyses. By projecting maps to a new geographic reference framework and scaling pixel values to a new size, distortions of map information are introduced that can affect the proportion and arrangement of thematic classes across the landscape. Using a sample land cover dataset depicting a heterogeneous landscape, this paper examines these impacts using three common raster-based transformation methods and introduces a new vector-based method that minimizes error propagation. While relative class area was best preserved by a nearest-neighbor resampling method, distortions to the contiguity of thematic classes and the overall fragmentation of the landscape were minimized when using the vectorbased projection and resampling method. Results demonstrate that more than a third of pixel values of a categorical map may be affected by common projection and scaling methods and reinforce the need for careful attention to impacts of error propagation in categorical data transformations.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2012/00000078/00000006/art00005;jsessionid=6mv783v2g663c.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatial Resolution Imagery Requirements for Identifying Structure Damage in a Hurricane Disaster
        </a>
    </h3>
    <div style="font-style: italic;">201206, pp. 625-635(11)</div>
    <div>Authors: Battersby, Sarah E.; Hodgson, Michael E.; Wang, Jiayu</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In disaster response, timely collection and exploitation of remotely sensed imagery is of increasing importance. Image exploitation approaches during the immediate (first few days) aftermath of a disaster are predominantly through visual analysis rather than automated classification methods. While the temporal needs for obtaining the imagery are fairly clear (within a one- to three-day window), there have only been educated guesses about the spatial resolution requirements necessary for the imagery for visual analysis. In this paper, we report results from an empirical study to identify the coarsest spatial resolution that is adequate for tasks required immediately following a major disaster. The study was conducted using cognitive science experimental methods and evaluated the performance of individuals with varying image interpretation skills in the task of mapping hurricane-related residential structural damage. Through this study, we found 1.5 m as a threshold for the coarsest spatial resolution imagery that can successfully be used for this task. The results of the study are discussed in terms of the likelihood of collection of this type of imagery within the temporal window required for emergency management operations.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2012/00000078/00000006/art00006;jsessionid=6mv783v2g663c.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Towards Operational Automatic Flood Detection Using EOS/MODIS Data
        </a>
    </h3>
    <div style="font-style: italic;">201206, pp. 637-646(10)</div>
    <div>Authors: Sun, Donglian; Yu, Yunyue; Zhang, Rui; Li, Sanmei; Goldberg, Mitchell D.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This study investigates how to derive water fraction and flood map from the Moderate-Resolution Imaging Spectroradiometer (MODIS) using a Regression Tree (RT) approach, which can integrate all predictors. The New Orleans, Louisiana floods in August 2005 were selected as a case study. MODIS surface reflectance with matched water fraction data were used for training. The tree-based regression models were obtained automatically through learning process. The tree structure reveals that near-infrared reflectance is more important than the difference and ratio between near-infrared and visible channels for water fraction estimate. Flood distributions were generated using the differences in water fraction values between after and before the flooding. The derived water fractions were evaluated against 30 m Thematic Mapper (TM) data from Landsat observations. Water fractions derived from the MODIS and TM data agree well (R20.94, bias 0.38 percent, and RMSE 4.35 percent). The results show that the RT approach in dynamic monitoring of floods is acceptable.
            </details>
        </div>
</article>
