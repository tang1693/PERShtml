
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 11 - Year 2024</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 11 - Year 2024</h1>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000011/art00010;jsessionid=vj4y1qllys3s.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Morphology-Based Feature Extraction Network for Arbitrary-Oriented SAR Vehicle Detection
        </a>
    </h3>
    <div style="font-style: italic;">202411, pp. 665-673(9)</div>
    <div>Authors: Chen, Ting; Huang, Xiaohong</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In recent years, synthetic aperture radar (SAR) vehicle detection has become a research hotspot. However, algorithms using horizontal bounding boxes can lead to redundant detection areas due to the varying aspect ratio and arbitrary orientation of vehicle targets. This paper proposes a morphology-based feature extraction network (MFE-Net), which fully uses the prior shape knowledge of the vehicle targets. Specifically, we adopt rotatable bounding boxes to predict the targets, and a novel rectangular rotation-invariant coordinate convolution (RRICC) is proposed to extract the feature, which can determine more accurately the convolutional sampling location of the vehicles. The adaptive thresholding denoising module (ATDM) is designed to suppress background clutter. Furthermore, inspired by the convolutional neural networks (CNNs) and self-attention, we propose the hybrid representation enhancement module (HREM) to highlight the vehicle target features. The experiment results show that the proposed model obtains an average precision (AP) of 93.1% on the SAR vehicle detection data set (SVDD).
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000011/art00011;jsessionid=vj4y1qllys3s.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatial-Spectral Middle Cross-Attention Fusion Network for Hyperspectral Image Superresolution
        </a>
    </h3>
    <div style="font-style: italic;">202411, pp. 675-686(12)</div>
    <div>Authors: Lang, Xiujuan; Lu, Tao; Zhang, Yanduo; Jiang, Junjun; Xiong, Zixiang</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The spatial and spectral features of hyperspectral images exhibit complementarity, and neglecting them prevents the full exploitation of useful information for superresolution. This article proposes a spatial-spectral middle cross-attention fusion network to explore the spatial-spectral structure correlation. Initially, we learn spatial and spectral features through spatial and spectral branches instead of single ones to reduce information compression. Then, a novel middle-cross attention fusion block that includes middle features fusion strategy and cross-attention is proposed to fuse spatial-spectral features to enhance their mutual effects, which aims to explore the spatial-spectral structural correlations. Finally, we propose a spectral feature compensation mechanism to provide complementary information for adjacent band groups. The experimental results show that the proposed method outperforms state-of-the-art algorithms in object values and visual quality.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000011/art00012;jsessionid=vj4y1qllys3s.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Machine Learning and New-Generation Spaceborne Hyperspectral Data Advance Crop Type Mapping
        </a>
    </h3>
    <div style="font-style: italic;">202411, pp. 687-698(12)</div>
    <div>Authors: Aneece, Itiya; Thenkabail, Prasad S.; McCormick, Richard; Alifu, Haireti; Foley, Daniel; Oliphant, Adam J.; Teluguntla, Pardhasaradhi</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Hyperspectral sensors provide near-continuous spectral data that can facilitate advancements in agricultural crop classification and characterization, which are important for addressing global food and water security issues. We investigated two new-generation hyperspectral sensors, Germany’s Deutsches Zentrum für Luft‐ und Raumfahrt Earth Sensing Imaging Spectrometer (DESIS) and Italy’s PRecursore IperSpettrale della Missione Applicativa (PRISMA), within California???s Central Valley in August 2021 focusing on five irrigated agricultural crops (alfalfa, almonds, corn, grapes, and pistachios). With reference data from the U.S. Department of Agriculture Cropland Data Layer, we developed a spectral library of the crops and classified them using three machine learning algorithms (support vector machines [SVM], random forest [RF], and spectral angle mapper [SAM]) and two philosophies: 1. Full spectral analysis (FSA) and 2. Optimal hyperspectral narrowband (OHNB) analysis. For FSA, we used 59 DESIS four-bin product bands and 207 of 238 PRISMA bands. For OHNB analysis, 9 DESIS and 16 PRISMA nonredundant OHNBs for studying crops were selected. FSA achieved only 1% to 3% higher accuracies relative to OHNB analysis in most cases. SVM provided the best results, closely followed by RF. Using both DESIS and PRISMA image OHNBs in SVM for classification led to higher accuracy than using either image alone, with an overall accuracy of 99%, producer’s accuracies of 94% to 100%, and user???s accuracies of 95% to 100%.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2024/00000090/00000011/art00014;jsessionid=vj4y1qllys3s.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Variable-Iterative Fully Convolutional Neural Network for Sparse Unmixing
        </a>
    </h3>
    <div style="font-style: italic;">202411, pp. 699-706(8)</div>
    <div>Authors: Kong, Fanqiang; Lv, Zhijie; Wang, Kun; Fang, Xu; Zheng, Yuhan; Yu, Shengjie</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Neural networks have greatly promoted the development of hyperspectral unmixing (HU). Most data-driven deep networks extract features of hyperspectral images (HSIs) by stacking convolutional layers to achieve endmember extraction and abundance estimation. Some model-driven networks have strong interpretability but fail to mine the deep feature. We propose a variable-iterative fully convolutional neural network (VIFCNN) for sparse unmixing, combining the characteristics of these two networks. Under the model-driven iterative framework guided by sparse unmixing by variable splitting and augmented lagrangian (SUnSAL), a data-driven spatial-spectral feature learning module and a spatial information updating module are introduced to enhance the learning of data information. Experimental results on synthetic and real datasets show that VIFCNN significantly outperforms several traditional unmixing methods and two deep learning???based methods. On real datasets, our method improves signal-to-reconstruction error by 17.38%, reduces abundance root-mean-square error by 25.24%, and reduces abundance spectral angle distance by 31.40% compared with U-ADMM-ßUNet.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 11 - Year 2024</p>
        </footer>
    </body>
    </html>
    