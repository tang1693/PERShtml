
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 03 - Year 2020</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 03 - Year 2020</h1>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2020/00000086/00000003/art00006;jsessionid=muvqg9d5i3t2.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Edge-Reinforced Convolutional Neural Network for Road Detection in Very-High-Resolution Remote Sensing Imagery
        </a>
    </h3>
    <div style="font-style: italic;">202003, pp. 153-160(8)</div>
    <div>Authors: Lu, Xiaoyan; Zhong, Yanfei; Zheng, Zhuo; Zhao, Ji; Zhang, Liangpei</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Road detection in very-high-resolution remote sensing imagery is a hot research topic. However, the high resolution results in highly complex data distributions, which lead to much noise for road detectionâ€”for example, shadows and occlusions caused by disturbance on the roadside make it difficult to accurately recognize road. In this article, a novel edge-reinforced convolutional neural network, combined with multiscale feature extraction and edge reinforcement, is proposed to alleviate this problem. First, multiscale feature extraction is used in the center part of the proposed network to extract multiscale context information. Then edge reinforcement, applying a simplified U-Net to learn additional edge information, is used to restore the road information. The two operations can be used with different convolutional neural networks. Finally, two public road data sets are adopted to verify the effectiveness of the proposed approach, with experimental results demonstrating its superiority.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2020/00000086/00000003/art00007;jsessionid=muvqg9d5i3t2.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            The Application of Bidirectional Reflectance Distribution Function Data to Recognize the Spatial Heterogeneity of Mixed Pixels in Vegetation Remote Sensing: A Simulation Study
        </a>
    </h3>
    <div style="font-style: italic;">202003, pp. 161-167(7)</div>
    <div>Authors: Yan, Yanan; Deng, Lei; Liu, XianLin</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Spectral decomposition of mixed pixels can provide information about the abundance of end members but fails to indicate the spatial distribution of end members in vegetation remote sensing. This work is a significant attempt to use the bidirectional reflectance distribution function (BRDF) characteristics of mixed pixels in the prediction of spatial-heterogeneity metrics. Data sets from this function with different spatial distributions were constructed by the discrete anisotropic radiative transfer model, and three spatial aggregation and dispersion metrics were calculated: percentage of like adjacencies, spatial division index, and aggregation index. A simple linear regression method was used to construct the prediction model of spatial aggregation and dispersion metrics. The potential of multiangle remote sensing model for identifying spatial patterns well was demonstrated, and its importance was found to differ for different spatial aggregation and dispersion metrics. Specifically, the precision of the model based on multiangle reflectance used for predicting the spatial division index could meet a minimum root mean square of 5.95%. The reflectance features from backward observation on the principal plane play the leading role in recognizing the spatial heterogeneity of mixed pixels. The prediction model is sufficiently robust to distinguish the same vegetation with different growth trends, but also performs well when the ground objects have a smaller reflectance difference in the mixed pixels in a certain band. This study is expected to offer a new thought for spatial-heterogeneity identification of ground objects and thus promote the development of remote sensing technology in assessing spatial distribution.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2020/00000086/00000003/art00008;jsessionid=muvqg9d5i3t2.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Self-Calibration of the Stereo Vision System of the Chang'e-4 Lunar Rover Based on the Points and Lines Combined Adjustment
        </a>
    </h3>
    <div style="font-style: italic;">202003, pp. 169-176(8)</div>
    <div>Authors: Zhang, Shuo; Jia, Yang; Peng, Song; Wen, Bo; Ma, Youqing; Qi, Chen; Sima, Bing; Liu, Shaochuang</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The stereo vision system is the special engineering measurement instrument of the Chang'e-4 lunar rover. It is composed of the Navigation Camera (NavCam) and the Mast Mechanism (MasMec). An improved self-calibration method for the stereo vision system of the Chang'e-4 lunar rover is proposed. The method consists of two parts: the NavCam's self-calibration and the MasMec's self-calibration. A combined adjustment based on the points and lines is proposed. The baseline constraint of the NavCam is considered. The self-calibration model of the MasMec is established based on the product-of-exponentials formula. Finally, the premission laboratory calibration and the on-site calibration are carried out. The laboratory calibration shows that the proposed approach has high accuracy. The checkpoint with a distance of about 2.7 m to the left NavCam has a point error of about 4 mm. Finally, the proposed approach is applied in the on-site calibration.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2020/00000086/00000003/art00009;jsessionid=muvqg9d5i3t2.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Reducing Shadow Effects on the Co-Registration of Aerial Image Pairs
        </a>
    </h3>
    <div style="font-style: italic;">202003, pp. 177-186(10)</div>
    <div>Authors: Plummer, Matthew; Stow, Douglas; Storey, Emanuel; Coulter, Lloyd; Zamora, Nicholas; Loerch, Andrew</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Image registration is an important preprocessing step prior to detecting changes using multi-temporal image data, which is increasingly accomplished using automated methods. In high spatial resolution imagery, shadows represent a major source of illumination variation, which can reduce the performance of automated registration routines. This study evaluates the statistical relationship between shadow presence and image registration accuracy, and whether masking and normalizing shadows leads to improved automatic registration results. Eighty-eight bitemporal aerial image pairs were co-registered using software called Scale Invariant Features Transform (SIFT) and Random Sample Consensus (RANSAC) Alignment (SARA). Co-registration accuracy was assessed at different levels of shadow coverage and shadow movement within the images. The primary outcomes of this study are (1) the amount of shadow in a multi-temporal image pair is correlated with the accuracy/success of automatic co-registration; (2) masking out shadows prior to match point select does not improve the success of image-to-image co-registration; and (3) normalizing or brightening shadows can help match point routines find more match points and therefore improve performance of automatic co-registration. Normalizing shadows via a standard linear correction provided the most reliable co-registration results in image pairs containing substantial amounts of relative shadow movement, but had minimal effect for pairs with stationary shadows.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2020/00000086/00000003/art00010;jsessionid=muvqg9d5i3t2.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Assessment of Salt Marsh Change on Assateague Island National Seashore Between 1962 and 2016
        </a>
    </h3>
    <div style="font-style: italic;">202003, pp. 187-194(8)</div>
    <div>Authors: Campbell, Anthony; Wang, Yeqiao</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Salt marshes provide extensive ecosystem services, including high biodiversity, denitrification, and wave attenuation. In the mid-Atlantic, sea level rise is predicted to affect salt marsh ecosystems severely. This study mapped the entirety of Assateague Island with Very High Resolution satellite imagery and object-based methods to determine an accurate salt marsh baseline for change analysis. Topobathy-metric light detection and ranging was used to map the salt marsh and model expected tidal effects. The satellite imagery, collected in 2016 and classified at two hierarchical thematic schemes, were compared to determine appropriate thematic richness. Change analysis between this 2016 map and both a manually delineated 1962 salt marsh extent and image classification of the island from 1994 determined rates off change. The study found that from 1962 to 1994, salt marsh expanded by 4.01 ha/year, and from 1994 to 2016 salt marsh was lost at a rate of -3.4 ha/ year. The study found that salt marsh composition, (percent vegetated salt marsh) was significantly influenced by elevation, the length of mosquito ditches, and starting salt marsh composition. The study illustrates the importance of remote sensing monitoring for understanding site-specific changes to salt marsh environments and the barrier island system.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 03 - Year 2020</p>
        </footer>
    </body>
    </html>
    