
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 12 - Year 2025</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 12 - Year 2025</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000012" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000012/art00011;jsessionid=1ipqro8bok43i.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Novel Multi-level Feature Collaborative Matching Network for Optical and Synthetic Aperture Radar Image Registration
        </a>
    </h3>
    <div style="font-style: italic;">202512, pp. 753-761(9)</div>
    <div>Authors: Pang, Bo; Wang, Lei; Wei, Bo; Zhu, Wenlei; Gao, Haiyun</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/12/A Novel Multi-level Feature Collaborative Matching Network for Optical and Synthetic Aperture Radar Image Registration.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Due to the complementary characteristics of synthetic aperture radar (SAR) and optical images, image registration as a prerequisite for their information fusion has received increasing attention. Currently, learning-based methods can better handle the significant radiometric and geometric differences between optical and SAR images compared to traditional registration approaches, but they still have limitations in distinguishing difficult samples, making high-precision registration a remaining challenge. To address these challenges, this paper proposes a multi-level feature collaborative matching network (MFC-Net) that effectively integrates high-level abstract features and low-level spatial features for precise registration. Furthermore, a novel dual-dimension joint attention module (DDJA) is designed to dynamically capture feature dependencies across both channel and spatial dimensions, enhancing cross-modal feature consistency and improving matching performance. Additionally, to address the problem of similarity between hard positive and negative samples caused by high-precision registration requirements, a dynamic differentiation factor is introduced at the loss function level, enabling the model to better distinguish between these similar samples in training. Extensive experiments conducted on the WHU-OPT-SAR data set and WHU-SENCity data set demonstrate that the proposed MFC-Net outperforms state-of-the-art methods in both matching accuracy and precision, validating its superiority in cross-modal image registration tasks.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000012/art00013;jsessionid=1ipqro8bok43i.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Joint Detection and Parameters Regression of Detailed Windows based on Facade Textures via an Adaptive Soft Teacher
        </a>
    </h3>
    <div style="font-style: italic;">202512, pp. 763-775(13)</div>
    <div>Authors: Hu, Han; Li, Chenwei; Xu, Bo; Chen, Min; Zhu, Qing; Han, Zujie; Ning, Xinwen; Fu, Xuesong</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/12/Joint Detection and Parameters Regression of Detailed Windows based on Facade Textures via an Adaptive Soft Teacher.jpg" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The detailed and precise reconstruction of building facades with semantic information conforming to the level-of-detail protocol has drawn increasing attention in recent years. However, despite windows being a major component of building facades, current research often oversimplifies them in the modeling process, primarily focusing on their location and size. This study proposes a joint approach to simultaneously address window detection and detailed parametric modeling. All required window information, including type, location, and interior structure parameters, is obtained via an end-to-end network. This information is then consolidated into predefined window syntax accessible via the SketchUp Ruby application programming interface (API), resulting in the construction of detailed three-dimensional window models. To resolve the issue of training the network with limited labeled data, an adaptive threshold method based on Soft Teacher is proposed, leveraging the pseudolabel technique and consistency regularization to enhance detection precision. Experiments performed on multiple datasets demonstrate that the proposed methods achieve improved window-detection precision with reduced cell estimation errors compared with existing Faster R-CNN???based methods. The mean average precision and average precision at the intersection of union = 0.5 (AP50) metrics increased from 47.9% and 61.9% to 69.0% and 83.1%, respectively. The mean absolute error metric for cell estimation improved from 0.103 to 0.076.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000012/art00015;jsessionid=1ipqro8bok43i.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Evaluating Three-Dimensional Elevation Program Lidar Consistency and Accuracy at Scale Using Cloud-Native, Open-Source Methods
        </a>
    </h3>
    <div style="font-style: italic;">202512, pp. 777-785(9)</div>
    <div>Authors: Sampath, Aparajithan; Irwin, Jeffrey R.; Stoker, Jason M.</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/12/Evaluating Three-Dimensional Elevation Program Lidar Consistency and Accuracy at Scale Using Cloud-Native, Open-Source Methods.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The U.S. Geological Survey three-dimensional elevation program (3DEP) has significantly expanded national lidar coverage, necessitating scalable, reproducible methods for assessing data quality across diverse terrains and acquisition conditions. This study introduces a cloud-native, open-source workflow designed to evaluate the geometric accuracy and consistency of 3DEP lidar data sets at a national scale. Leveraging tools such as the Point Data Abstraction Library, Open3D, and Amazon Web Services infrastructure, the workflow integrates global navigation satellite system???surveyed ground control points and terrestrial laser scanning data to validate airborne lidar collections. Two case studies demonstrate the application of this process. In Puerto Rico, the process identified vertical biases and inconsistencies in vegetated areas, while in Iowa and Arizona, the process confirmed high vertical accuracy with minimal bias. The results underscore the effectiveness of combining cloud computing with open-source tools to perform large-scale lidar data quality assessments. This process offers a reproducible, efficient solution for nationwide validation of 3DEP data sets, supporting enhanced decision-making in geospatial applications.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000012/art00017;jsessionid=1ipqro8bok43i.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            ETF-MNet:A Multi-Scale Fusion Network for Enhancing Target Features for Remote Sensing Images
        </a>
    </h3>
    <div style="font-style: italic;">202512, pp. 787-797(11)</div>
    <div>Authors: Shan, Huilin; Wang, Shuoyang; Hu, Yuxiang; Chen, Xin; Wu, Xinyue; Zhang, Yinsheng</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/12/ETF-MNet A Multi-Scale Fusion Network for Enhancing Target Features for Remote Sensing Images.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In response to the challenges of dense target distribution, significant scale variations, and limited feature information for small objects in remote sensing images, this paper introduces a multi-scale fusion network with enhanced target features. Initially, a multi-layer feature aggregation module is constructed within the backbone network to enhance the capability of feature extraction. Subsequently, a multi-channel feature fusion module is implemented in the neck portion of the network to effectively capture cross-channel information and further enhance the expressive power of features at different scales. Moreover, a bi-directional multi-scale feature fusion module is proposed as a mechanism for feature fusion, using top-down and bottom-up fusion strategies to facilitate information interaction among features at different levels. Finally, in the detection layer, a fractional Fourier transform is applied to the image to extract additional feature information, which, combined with convolutional operations, improves the accuracy of small object detection. To validate the effectiveness of the proposed method, experiments were conducted on the data set for object detection in aerial images and Northwestern Polytechnical University very high resolution 10 data sets. The average detection accuracy achieved was 78.7% and 95.4%, respectively. Computational complexity was measured at 95.6 G, and the overall model size was 30.7 M. These results demonstrate that the proposed method excels at high detection accuracy, low computational complexity, and strong feature representation capability. It effectively improves the detection accuracy of small objects in remote sensing images, thereby enhancing the overall performance of small object detection in remote sensing imagery.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000012/art00019;jsessionid=1ipqro8bok43i.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Assessing the Accuracy of a 2021 Kansas‐Oklahoma Tallgrass Ecosystem Burn Area Map
        </a>
    </h3>
    <div style="font-style: italic;">202512, pp. 799-807(9)</div>
    <div>Authors: Iiames, John; Congalton, Russell G.; Avey, Lance; Miller, Steven; Ebert, Don; Grier, Gina</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/12/Assessing the Accuracy of a 2021 Kansas–Oklahoma Tallgrass Ecosystem Burn Area Map.jpg" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Systematic spring season burning of the tallgrass prairie in eastern Kansas and northeastern Oklahoma has been used by ranchers and ecologists for over 150 years to encourage forage production and reduce invasive species propagation and woody species encroachment in the Kansas Flint Hills and Oklahoma Osage Tallgrass ecoregion. On average, 8100 km2is burned yearly during the March to April burn window. Despite the beneficial effects resulting from spring burning, air resource managers must also account for the particulates emitted from the combustion of these fuels. The Kansas Department of Health and Environment (KDHE) has mapped spring burn area over the Flint Hills (Kansas) and Osage (Oklahoma) regions since 2011 by processing satellite data from the National Aeronautics and Space Administration (NASA) Moderate Resolution Imaging Spectroradiometer (MODIS) sensor onboard both the Aqua and Terra satellite platforms. Understanding the accuracy of these maps is useful for understanding PM2.5 impacts to the local communities and the surrounding states. We conducted both a sample unit???based and an area???based assessment of the 4 April 2021 burn area map to determine map accuracy and identified potential sources of error, both “commission” (i.e., mapped a burn area where there was no burn) and “omission” (i.e., did not map a burn area where a burn actually occurred). The results indicate that the KDHE 2021 map achieved high overall accuracies (>94%), yet these accuracies were not consistent over the entire area. Omission and commission errors were three to four times larger in areas that had higher burn area percentages, partially due to increased burn edge, promoting larger numbers of mixed 250-m cells. Another source of error was from the prior year (2020) fall burn scars that were occasionally identified as spring burns over the 2021 burn season. The area-based assessment method provided additional information from the sample unit???based method and was useful in identifying anomalous assessment results dependent on the geographical location within the tallgrass system.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 12 - Year 2025</p>
        </footer>
    </body>
    </html>
    