
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 05 - Year 2018</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 05 - Year 2018</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00013;jsessionid=tikv08egoq3e.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Unsupervised Source Selection for Domain Adaptation
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 249-261(13)</div>
    <div>Authors: Vogt, Karsten; Paul, Andreas; Ostermann, Jörn; Rottensteiner, Franz; Heipke, Christian</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The creation of training sets for supervised machine learning often incurs unsustainable manual costs. Transfer learning (TL) techniques have been proposed as a way to solve this issue by adapting training data from different, but related (source) datasets to the test (target) dataset. A problem inTLis how to quantify the relatedness of a source quickly and robustly. In this work, we present a fast domain similarity measure that captures the relatedness between datasets purely based on unlabeled data. Our method transfers knowledge from multiple sources by generating a weighted combination of domains. We show for multiple datasets that learning on such sources achieves an average overall accuracy closer than 2.5 percent to the results of the target classifier for semantic segmentation tasks. We further apply our method to the task of choosing informative patches from unlabeled datasets. Only labeling these patches enables a reduction in manual work of up to 85 percent.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00014;jsessionid=tikv08egoq3e.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Multitemporal Classification Under Label Noise Based on Outdated Maps
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 263-277(15)</div>
    <div>Authors: Maas, Alina E.; Rottensteiner, Franz; Alobeid, Abdalla; Heipke, Christian</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Supervised classification of remotely sensed images is a classical method for change detection. The task requires training data in the form of image data with known class labels. If the training labels are acquired from an outdated map, the classifier must cope with errors in the training labels. These errors (label noise) typically occur in clusters in object space, because they are caused by land cover changes over time. In this paper we adapt a label noise tolerant training technique for classification, so that the fact that changes affect larger clusters of pixels is considered. We also integrate the existing map into an iterative classification procedure to act as a priori in regions which are likely to contain changes. Additionally we expand the model for multitemporal data, making it applicable for time series. Our experiments are based on four test areas, including a multitemporal example. Our results show that this method helps to distinguish between real changes over time and false detections caused by misclassification and thus improve the accuracy of the classification results.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00015;jsessionid=tikv08egoq3e.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Archetypal Analysis for Sparse Representation-Based Hyperspectral Sub-pixel Quantification
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 279-286(8)</div>
    <div>Authors: Drees, Lukas; Roscher, Ribana; Wenzel, Susanne</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The estimation of land cover fractions from remote sensing images is a frequently used indicator of the environmental quality. This paper focuses on the quantification of land cover fractions in an urban area of Berlin, Germany, using simulated hyperspectralEnMAPdata with a spatial resolution of 30 m × 30 m. We use constrained sparse representation, where each pixel with unknown surface characteristics is expressed by a weighted linear combination of elementary spectra with known land cover class. We automatically determine the elementary spectra from image reference data using archetypal analysis by simplex volume maximization, and combine it with reversible jump Markov chain Monte Carlo method. In our experiments, the estimation of the automatically derived elementary spectra is compared to the estimation obtained by a manually designed spectral library by means of reconstruction error, mean absolute error of the fraction estimates, sum of fractions, R2, and the number of used elementary spectra. The experiments show that a collection of archetypes can be an adequate and efficient alternative to the manually designed spectral library with respect to the mentioned criteria.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00016;jsessionid=tikv08egoq3e.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Classification of Aerial Photogrammetric 3D Point Clouds
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 287-295(9)</div>
    <div>Authors: Becker, C.; Rosinskaya, E.; Häni, N.; D'Angelo, E.; Strecha, C.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                We present a powerful method to extract per-point semantic class labels from aerial photogrammetry data. Labeling this kind of data is important for tasks such as environmental modeling, object classification, and scene understanding. Unlike previous point cloud classification methods that rely exclusively on geometric features, we show that incorporating color information yields a significant increase in accuracy in detecting semantic classes. We test our classification method on four real-world photogrammetry datasets that were generated with Pix4Dmapper, and with varying point densities. We show that off-the-shelf machine learning techniques coupled with our new features allow us to train highly accurate classifiers that generalize well to unseen data, processing point clouds containing 10 million points in less than three minutes on a desktop computer. We also demonstrate that our approach can be used to generate accurate Digital Terrain Models, outperforming approaches based on more simple heuristics such as Maximally Stable Extremal Regions.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00017;jsessionid=tikv08egoq3e.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Large-Scale Supervised Learning For 3D Point Cloud Labeling: Semantic3d.Net
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 297-308(12)</div>
    <div>Authors: Hackel, Timo; Wegner, Jan D.; Savinov, Nikolay; Ladicky, Lubor; Schindler, Konrad; Pollefeys, Marc</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In this paper we review current state-of-the-art in 3D point cloud classification, present a new 3D point cloud classification benchmark data set of single scans with over four billion manually labeled points, and discuss first available results on the benchmark. Much of the stunning recent progress in 2D image interpretation can be attributed to the availability of large amounts of training data, which have enabled the (supervised) learning of deep neural networks. With the data set presented in this paper, we aim to boost the performance ofCNNsalso for 3D point cloud labeling. Our hope is that this will lead to a breakthrough of deep learning also for 3D (geo-) data. The semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains eight semantic classes and covers a wide range of urban outdoor scenes, including churches, streets, railroad tracks, squares, villages, soccer fields, and castles. We describe our labeling interface and show that, compared to those already available to the research community, our data set provides denser and more complete point clouds, with a much higher overall number of labeled points. We further provide descriptions of baseline methods and of the first independent submissions, which are indeed based onCNNs, and already show remarkable improvements over prior art. We hope that semantic3D.net will pave the way for deep learning in 3D point cloud analysis, and for 3D representation learning in general.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00018;jsessionid=tikv08egoq3e.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatiotemporal Change Detection Based on Persistent Scatterer Interferometry: A Case Study of Monitoring Building Changes
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 309-328(20)</div>
    <div>Authors: Yang, C. H.; Kenduiywo, B. K.; Soergel, U.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Persistent scatterer interferometry (PSI) detects and analyses PS points from multitemporal SAR images for scene deformation monitoring. We propose a novel technique to identify disappearing and emerging PS points, which are regarded as building changes in cities. A spatiotemporal analysis is implemented as both spatial position and occurrence time are obtained for each change point. We first estimate each pixel's temporal coherences in different image subsets. Computed from temporal coherences, a change index sequence is introduced to quantify each pixel's probabilities of being change points at different times. All pixels' change indices are then utilized to extract change points by a global, automatic, and statistical-based scheme. We then eliminate blunders by a spatial filtering. Finally, the occurrence times of the change points are detected based on the temporal variation in their change index sequences. We implement a simulated data test to validate and assess our method. Using TerraSAR-X images, our real data test successfully recognizes steady, disappearing, and emerging buildings in Berlin, Germany within 2013.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00013;jsessionid=11h45u0kbczdi.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Unsupervised Source Selection for Domain Adaptation
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 249-261(13)</div>
    <div>Authors: Vogt, Karsten; Paul, Andreas; Ostermann, Jörn; Rottensteiner, Franz; Heipke, Christian</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The creation of training sets for supervised machine learning often incurs unsustainable manual costs. Transfer learning (TL) techniques have been proposed as a way to solve this issue by adapting training data from different, but related (source) datasets to the test (target) dataset. A problem inTLis how to quantify the relatedness of a source quickly and robustly. In this work, we present a fast domain similarity measure that captures the relatedness between datasets purely based on unlabeled data. Our method transfers knowledge from multiple sources by generating a weighted combination of domains. We show for multiple datasets that learning on such sources achieves an average overall accuracy closer than 2.5 percent to the results of the target classifier for semantic segmentation tasks. We further apply our method to the task of choosing informative patches from unlabeled datasets. Only labeling these patches enables a reduction in manual work of up to 85 percent.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00014;jsessionid=11h45u0kbczdi.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Multitemporal Classification Under Label Noise Based on Outdated Maps
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 263-277(15)</div>
    <div>Authors: Maas, Alina E.; Rottensteiner, Franz; Alobeid, Abdalla; Heipke, Christian</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Supervised classification of remotely sensed images is a classical method for change detection. The task requires training data in the form of image data with known class labels. If the training labels are acquired from an outdated map, the classifier must cope with errors in the training labels. These errors (label noise) typically occur in clusters in object space, because they are caused by land cover changes over time. In this paper we adapt a label noise tolerant training technique for classification, so that the fact that changes affect larger clusters of pixels is considered. We also integrate the existing map into an iterative classification procedure to act as a priori in regions which are likely to contain changes. Additionally we expand the model for multitemporal data, making it applicable for time series. Our experiments are based on four test areas, including a multitemporal example. Our results show that this method helps to distinguish between real changes over time and false detections caused by misclassification and thus improve the accuracy of the classification results.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00015;jsessionid=11h45u0kbczdi.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Archetypal Analysis for Sparse Representation-Based Hyperspectral Sub-pixel Quantification
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 279-286(8)</div>
    <div>Authors: Drees, Lukas; Roscher, Ribana; Wenzel, Susanne</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The estimation of land cover fractions from remote sensing images is a frequently used indicator of the environmental quality. This paper focuses on the quantification of land cover fractions in an urban area of Berlin, Germany, using simulated hyperspectralEnMAPdata with a spatial resolution of 30 m × 30 m. We use constrained sparse representation, where each pixel with unknown surface characteristics is expressed by a weighted linear combination of elementary spectra with known land cover class. We automatically determine the elementary spectra from image reference data using archetypal analysis by simplex volume maximization, and combine it with reversible jump Markov chain Monte Carlo method. In our experiments, the estimation of the automatically derived elementary spectra is compared to the estimation obtained by a manually designed spectral library by means of reconstruction error, mean absolute error of the fraction estimates, sum of fractions, R2, and the number of used elementary spectra. The experiments show that a collection of archetypes can be an adequate and efficient alternative to the manually designed spectral library with respect to the mentioned criteria.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00016;jsessionid=11h45u0kbczdi.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Classification of Aerial Photogrammetric 3D Point Clouds
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 287-295(9)</div>
    <div>Authors: Becker, C.; Rosinskaya, E.; Häni, N.; D'Angelo, E.; Strecha, C.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                We present a powerful method to extract per-point semantic class labels from aerial photogrammetry data. Labeling this kind of data is important for tasks such as environmental modeling, object classification, and scene understanding. Unlike previous point cloud classification methods that rely exclusively on geometric features, we show that incorporating color information yields a significant increase in accuracy in detecting semantic classes. We test our classification method on four real-world photogrammetry datasets that were generated with Pix4Dmapper, and with varying point densities. We show that off-the-shelf machine learning techniques coupled with our new features allow us to train highly accurate classifiers that generalize well to unseen data, processing point clouds containing 10 million points in less than three minutes on a desktop computer. We also demonstrate that our approach can be used to generate accurate Digital Terrain Models, outperforming approaches based on more simple heuristics such as Maximally Stable Extremal Regions.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00017;jsessionid=11h45u0kbczdi.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Large-Scale Supervised Learning For 3D Point Cloud Labeling: Semantic3d.Net
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 297-308(12)</div>
    <div>Authors: Hackel, Timo; Wegner, Jan D.; Savinov, Nikolay; Ladicky, Lubor; Schindler, Konrad; Pollefeys, Marc</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In this paper we review current state-of-the-art in 3D point cloud classification, present a new 3D point cloud classification benchmark data set of single scans with over four billion manually labeled points, and discuss first available results on the benchmark. Much of the stunning recent progress in 2D image interpretation can be attributed to the availability of large amounts of training data, which have enabled the (supervised) learning of deep neural networks. With the data set presented in this paper, we aim to boost the performance ofCNNsalso for 3D point cloud labeling. Our hope is that this will lead to a breakthrough of deep learning also for 3D (geo-) data. The semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains eight semantic classes and covers a wide range of urban outdoor scenes, including churches, streets, railroad tracks, squares, villages, soccer fields, and castles. We describe our labeling interface and show that, compared to those already available to the research community, our data set provides denser and more complete point clouds, with a much higher overall number of labeled points. We further provide descriptions of baseline methods and of the first independent submissions, which are indeed based onCNNs, and already show remarkable improvements over prior art. We hope that semantic3D.net will pave the way for deep learning in 3D point cloud analysis, and for 3D representation learning in general.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2018/00000084/00000005/art00018;jsessionid=11h45u0kbczdi.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatiotemporal Change Detection Based on Persistent Scatterer Interferometry: A Case Study of Monitoring Building Changes
        </a>
    </h3>
    <div style="font-style: italic;">201805, pp. 309-328(20)</div>
    <div>Authors: Yang, C. H.; Kenduiywo, B. K.; Soergel, U.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Persistent scatterer interferometry (PSI) detects and analyses PS points from multitemporal SAR images for scene deformation monitoring. We propose a novel technique to identify disappearing and emerging PS points, which are regarded as building changes in cities. A spatiotemporal analysis is implemented as both spatial position and occurrence time are obtained for each change point. We first estimate each pixel's temporal coherences in different image subsets. Computed from temporal coherences, a change index sequence is introduced to quantify each pixel's probabilities of being change points at different times. All pixels' change indices are then utilized to extract change points by a global, automatic, and statistical-based scheme. We then eliminate blunders by a spatial filtering. Finally, the occurrence times of the change points are detected based on the temporal variation in their change index sequences. We implement a simulated data test to validate and assess our method. Using TerraSAR-X images, our real data test successfully recognizes steady, disappearing, and emerging buildings in Berlin, Germany within 2013.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 05 - Year 2018</p>
        </footer>
    </body>
    </html>
    