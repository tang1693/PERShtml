<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000008/art00001;jsessionid=2j0il3bts2s9k.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Range Image Techniques for Fast Detection and Quantification of Changes in Repeatedly Scanned Buildings
        </a>
    </h3>
    <div style="font-style: italic;">201308, nan</div>
    <div>Authors: Kang, Zhizhong; Zhang, Liqiang; Yue, Huanyin; Lindenbergh, Roderik</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper proposes a new method for the detection and quantification of changes in buildings using terrestrial laser scanning data from different epochs. A refined registration process is implemented that utilizes an optimized version of the Iterative Closest Point (ICP) algorithm, which implements the search of adjacent points in terms of their scanning angles. For detecting changes, a novel 2D angular difference histogram is proposed to first determine point segments representing building parts from the raw scattered scans. Afterwards, Hausdorff distance-based change detection is innovatively integrated into the optimized ICP process to improve the efficiency of the entire algorithm. The detected changes are quantified in the final step by determining the total planar surface area of the changed facade. This approach is tested and illustrated on two real datasets. The change quantification results show that the accuracy of the changed area quantification is in the order of square centimeters.
            </details>
        </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000008/art00002;jsessionid=2j0il3bts2s9k.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Methodology to Characterize Vertical Accuracies in Lidar-derived Products at Landscape Scales
        </a>
    </h3>
    <div style="font-style: italic;">201308, pp. 709-716(8)</div>
    <div>Authors: Tinkham, Wade T.; Hoffman, Chad M.; Falkowski, Michael J.; Smith, Alistair M.S.; Marshall, Hans-Peter; Link, Timothy E.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Light detection and ranging (lidar) is the premier technology for high-resolution elevation measurements in complex landscapes. Lidar error assessments allow for objective interpretation of Digital Elevation Models (DEMs) and products reliant on these layers. The purpose of this study is to spatially estimate the vertical error of a lidar-derived DEM across seven cover types through modeling of field survey data. We use thirty-four variables and ground-based field survey data in a Random Forest regression to predict elevation error. Four variables captured the variability within the lidar errors, with three variables relevant to the distribution of returns within the vegetation and one relating to the terrain form. Good agreement was observed when comparing the survey against the model predictions (Î¼ = .0.02 m, s = 0.13 m, and RMSE = 0.14 m). With most lidar products reliant upon accurate production of DEMs, providing spatially explicit assessments of uncertainty at the landscape level will increase user confidence in lidar products.
            </details>
        </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000008/art00003;jsessionid=2j0il3bts2s9k.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Coarse Elevation Map-based Registration Method for Super-resolution of Three-line Scanner Images
        </a>
    </h3>
    <div style="font-style: italic;">201308, pp. 717-730(14)</div>
    <div>Authors: Qin, Rongjun; Gong, Jianya; Li, Hongli; Huang, Xianfeng</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Three-line scanner imagery provides three overlapped images in an along-base direction, and creates a possible avenue to obtain higher quality images through the application of the super-resolution. Accurate co-registration of the three images is a key step for super-resolution. However, discontinuities and occlusions resulting from the 3D-to-2D projection cause mis-registration in traditional 2D-image-level co-registration methods. In this paper, we address this problem by introducing 3D information extracted from image triplets by using GPS/IMU data as an approximation. The core of the proposed method is to use a number of height layers derived from feature points and image partitions, in the form of a coarse elevation map (CEM), as a 3D constraint to restrict registration on the corresponding height. In terms of super-resolution, we also propose a tree-based fast adaptive template matching method for Knife-edge detection to fully automate the SRKE super-resolution algorithm. Experimental results show that the proposed method produces improved registered images and accordingly yields significant resolution enhancement as compared to other methods.
            </details>
        </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000008/art00004;jsessionid=2j0il3bts2s9k.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Registration of Optical Images with Lidar Data and Its Accuracy Assessment
        </a>
    </h3>
    <div style="font-style: italic;">201308, pp. 731-741(11)</div>
    <div>Authors: Zheng, Shunyi; Huang, Rongyong; Zhou, Yang</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Photogrammetry and lidar are two technologies complementary for 3D reconstruction. However, the problem is that the current registration methods of optical images with lidar data cannot satisfy all the requirements for the fusion of the above two technologies, especially for close-range photogrammetry and terrestrial lidar. In this paper, we propose a novel method for registration of optical images with terrestrial lidar data, which is implemented by minimizing the distances from the photogrammetric matching points to terrestrial lidar data surface, with the collinearity equation as the basic mathematical model. One advantage of this method is that it requires no feature extraction and segmentation from the lidar data. Another advantage is that non-rigid deformation caused by lens distortion can be eliminated through the use of bundle adjustment similar to self-calibration. In addition, experiments with two different data sets show that this method cannot only eliminate the influence of certain gross errors, but also offer a high accuracy of 3 mm to 5 mm. Therefore, the proposed registration method is proved to be more effective, accurate, and reliable.
            </details>
        </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000008/art00005;jsessionid=2j0il3bts2s9k.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Registration of Aerial Imagery and Lidar Data in Desert Areas Using the Centroids of Bushes as Control Information
        </a>
    </h3>
    <div style="font-style: italic;">201308, pp. 743-752(10)</div>
    <div>Authors: Li, Na; Huang, Xianfeng; Zhang, Fan; Wang, Le</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Geometric registration of multiple-source data is of great value for fusion processing and is very beneficial for the research of desert ecosystems. A lidar point cloud and optical image are two typical data that need to be integrated for data assimilation and information retrieval. This paper aims to solve the registration problem of aerial imagery and airborne lidar data in desert areas where traditional registration methods have difficulties in identifying registration primitives. In many deserts, such as the Sahara in Africa and Gobi in China, we observe that there are unevenly distributed desert bushes, which can be used as cues for registration. In this paper, we propose a registration approach using the centroids of bushes as registration primitives. This approach employs similar triangles created from both centroids as the evidence for matching and verifies the registration by the RANSAC algorithm. Experiments using data taken from the Dunhuang Gobi Desert in China show the registration surface model visually, and at the same time quantifies the deviation error, which corroborates that the proposed registration method is effective and feasible in desert areas.
            </details>
        </div>
</article>
<article style="border-top: 1px solid #000; padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000008/art00006;jsessionid=2j0il3bts2s9k.x-ic-live-02" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Robust Affine-Invariant Line Matching for High Resolution Remote Sensing Images
        </a>
    </h3>
    <div style="font-style: italic;">201308, pp. 753-760(8)</div>
    <div>Authors: Chen, Min; Shao, Zhenfeng</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Point-based matching methods usually hold limitation in dealing with low texture scenes. In this paper, a robust affineinvariant lines matching method is proposed. The method commences with line segments extraction. All the extracted line segments are grouped into salient lines and general lines. Accordingly, the matching procedure includes salient lines matching and general lines matching. In salient lines matching, affine invariants are calculated and the matched salient line correspondences are the basis of the general lines matching. Each general line is clustered into a matched salient line according to a certain rule. Taking each salient line as the root, together with all the general lines clustered to it, a control network is constructed. Finally, the general lines matching procedure is performed between the two subnetworks whose roots are correspondences. Experimental results show that our proposed method can successfully process local distortion and improve the matching performance in low texture areas.
            </details>
        </div>
</article>
