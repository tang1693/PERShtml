
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 05 - Year 2019</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 05 - Year 2019</h1>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000005/art00012;jsessionid=157qoba9z2mwa.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Robust Structure From Motion Based on Relative Rotations and Tie Points
        </a>
    </h3>
    <div style="font-style: italic;">201905, pp. 347-359(13)</div>
    <div>Authors: Wang, X.; Rottensteiner, F.; Heipke, C.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In this article, we present two new approaches for image orientation with a focus on robustness, starting with relative orientations of available image pairs, an incremental and a global one, and compare their performance. For the incremental approach, we first choose a suitable initial image pair, and we then iteratively extend the image cluster by adding new images. The rotations of these newly added images are estimated from relative rotations by single rotation averaging. In the next step, a linear equation system is set up for each new image to solve the translation parameters with triangulated tie points that can be viewed in that new image, followed by a resection for refinement. Finally, we refine the orientation parameters of the images by a local bundle adjustment. We also present a global method that consists of two parts: global rotation averaging, followed by setting up a large linear equation system to solve for all image translation parameters simultaneously; a final bundle adjustment is carried out to refine the results. We compare these two methods by analyzing results on different benchmark sets, including ordered and unordered image data sets from the Internet and two other challenging data sets to demonstrate the performance of our two approaches. We conclude that while the incremental method typically yields results of higher accuracy and performs better on the challenging data sets, our global method runs significantly faster.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000005/art00013;jsessionid=157qoba9z2mwa.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Flexible Photogrammetric Computations Using Modular Bundle Adjustment: The Chain Rule and the Collinearity Equations
        </a>
    </h3>
    <div style="font-style: italic;">201905, pp. 361-368(8)</div>
    <div>Authors: BÃ¶rlin, Niclas; Murtiyoso, Arnadi; Grussenmeyer, Pierre; Menna, Fabio; Nocerino, Erica</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The main purpose of this article is to show that photogram-metric bundle-adjustment computations can be sequentially organized into modules. Furthermore, the chain rule can be used to simplify the computation of the analytical Jacobians needed for the adjustment. Novel projection models can be flexibly evaluated by inserting, modifying, or swapping the order of selected modules. As a proof of concept, two variants of the pinhole projection model with Brown lens distortion were implemented in the open-source Damped Bundle Adjustment Toolbox and applied to simulated and calibration data for a nonconventional lens system. The results show a significant difference for the simulated, error-free, data but not for the real calibration data. The current flexible implementation incurs a performance loss. However, in cases where flexibility is more important, the modular formulation should be a useful tool to investigate novel sensors, data-processing techniques, and refractive models.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000005/art00014;jsessionid=157qoba9z2mwa.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Aiding Indoor Photogrammetry with UWB Sensors
        </a>
    </h3>
    <div style="font-style: italic;">201905, pp. 369-378(10)</div>
    <div>Authors: Masiero, Andrea; Fissore, Francesca; Guarnieri, Alberto; Pirotti, Francesco; Vettore, Antonio</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Given the worldwide spread of smartphones, photogram-metric surveying with mobile devices is becoming of significant interest in the research community for providing low cost three-dimensional (3D) models. Since the photogram-metric procedure applied to images produces a projective model, some external information is needed in order to obtain a 3D metric model. To this aim, Global Navigation Satellite System (GNSS) measurements are typically aided to the photogrammetric reconstruction procedure. How- ever, the quality of the obtained reconstruction is related to the GNSS positioning accuracy, which is typically at meterlevel for cheap receivers as those embedded in most of the consumer mobile devices, e.g.smartphones. Furthermore, this approach cannot be used in GNSS-denied environments (e.g.indoors). To overcome these issues, this paper investigates the integration of information provided by an Ultra-Wideband (UWB) positioning system with image-based reconstruction to produce a metric reconstruction. Further-more, the orientation (with respect to North-East directions) of the model is assessed thanks to the use of inertial sensors included in the UWB devices. Results of this integration are shown on two case studies in indoor environments.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000005/art00015;jsessionid=157qoba9z2mwa.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            DSM Generation from High Resolution Multi-View Stereo Satellite Imagery
        </a>
    </h3>
    <div style="font-style: italic;">201905, pp. 379-387(9)</div>
    <div>Authors: Gong, K.; Fritsch, D.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Along with improvements to spatial resolution, multiple-view stereo satellite imagery has become a valuable datasource for digital surface model generation. In 2016, a public multi-view stereo benchmark of commercial satellite imag- ery was released by the John Hopkins University Applied Physics Laboratory, USA. Motivated by this well-organized benchmark, we propose a pipeline to process multi-view satellite imagery into digital surface models. Input images are selected based on view angles and capture dates. We apply the relative bias-compensated model for orientation, and then generate the epipolar image pairs. The images are matched by the modified tube-based SemiGlobal Matching method (tSGM). Within the triangulation step, very dense point clouds are produced, and are fused by a median filter to generate the Digital Surface Model (DSM). A comparison with the reference data shows that the fused DSM generated by our pipeline is accurate and robust.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000005/art00016;jsessionid=157qoba9z2mwa.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Self-Supervised Convolutional Neural Networks for Plant Reconstruction Using Stereo Imagery
        </a>
    </h3>
    <div style="font-style: italic;">201905, pp. 389-399(11)</div>
    <div>Authors: Xia, Yuanxin; D'Angelo, Pablo; Tian, Jiaojiao; Fraundorfer, Friedrich; Reinartz, Peter</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Stereo matching can provide complete and dense three-dimensional reconstruction to study plant growth. Recently, high-quality stereo matching results were achieved combining Semi-Global Matching (SGM) with deep learning. However, due to a lack of suitable training data, this technique is not readily applicable for plant reconstruction. We propose a self-supervised Matching Cost with a Convolutional Neural Network (MC-CNN) scheme to calculate matching cost and test it for plant reconstruction. The MC-CNN network is retrained using the initial matching results obtained from the standard MC-CNN weights. For the experiment, closerange photogrammetric imagery of an in-house plant is used. The results show that the performance of self-supervised MC-CNN is superior to the Census algorithm and comparable to MC-CNN trained by a Light Detection and Ranging point cloud. Another experiment is performed using stereo imagery of a field beech tree. The proposed self-training strategy is tested and has proved capable of identifying the drought condition of trees from the reconstructed leaves.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 05 - Year 2019</p>
        </footer>
    </body>
    </html>
    