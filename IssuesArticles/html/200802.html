<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2008/00000074/00000002/art00001;jsessionid=wrtfev7ivfpl.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Automatic Registration and Mosaicking for Airborne Multispectral Image Sequences
        </a>
    </h3>
    <div style="font-style: italic;">200802, nan</div>
    <div>Authors: Du, Qian; Raksuntorn, Nareenart; Orduyilmaz, Adnan; Bruce, Lori M.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Airborne remote sensing has important applications in agriculture monitoring because of the flexibility of system deployment. The major obstacle in practical use is its high cost. To reduce the cost, a multispectral system can be assembled by using individual cameras onboard a small aerial platform, such as a miniature unmanned aerial vehicle (mini-UAV). In such a case, the cameras may have shifting and rotational misalignment, even after careful adjustment. Contiguous frames are captured as the platform flies. So multi-band registration within a single frame and frame-to-frame mosaicking are necessary to obtain a co-registered multispectral image for the entire monitoring area before any commercial product can be generated to support practical decision-making. In this paper, we present automatic algorithms to achieve this goal. These algorithms are particularly useful to the image scenes where no distinctive features are available. Both automatic and manual evaluations confirm the effectiveness of the developed algorithms in multi-sensor data fusion for overall flat terrain without distinctive features.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2008/00000074/00000002/art00002;jsessionid=wrtfev7ivfpl.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Pixel Level Fusion of Panchromatic and Multispectral Images Based on Correspondence Analysis
        </a>
    </h3>
    <div style="font-style: italic;">200802, pp. 183-192(10)</div>
    <div>Authors: Cakir, Halil I.; Khorram, Siamak</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A pixel level data fusion approach based on correspondence analysis (CA) is introduced for high spatial and spectral resolution satellite data. Principal component analysis (PCA) is a well-known multivariate data analysis and fusion technique in the remote sensing community. Related to PCA but a more recent multivariate technique, correspondence analysis, is applied to fuse panchromatic data with multi-spectral data in order to improve the quality of the final fused image. In the CA-based fusion approach, fusion takes place in the last component as opposed to the first component of the PCA-based approach. This new approach is then quantitatively compared to the PCA fusion approach using Landsat ETM∂, QuickBird, and two Ikonos (with and without dynamic range adjustment) test imagery. The new approach provided an excellent spectral accuracy when synthesizing images from multispectral and high spatial resolution panchromatic imagery.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2008/00000074/00000002/art00003;jsessionid=wrtfev7ivfpl.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Multispectral and Panchromatic Data Fusion Assessment Without Reference
        </a>
    </h3>
    <div style="font-style: italic;">200802, pp. 193-200(8)</div>
    <div>Authors: Alparone, Luciano; Aiazzi, Bruno; Baronti, Stefano; Garzelli, Andrea; Nencini, Filippo; Selva, Massimo</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper introduces a novel approach for evaluating the quality of pansharpened multispectral (MS) imagery without resorting to reference originals. Hence, evaluations are feasible at the highest spatial resolution of the panchromatic (PAN) sensor. Wang and Bovik’s image quality index (QI) provides a statistical similarity measurement between two monochrome images. The QI values between any couple of MS bands are calculated before and after fusion and used to define a measurement ofspectraldistortion. Analogously, QI values between each MS band and the PAN image are calculated before and after fusion to yield a measurement ofspatialdistortion. The rationale is that such QI values should be unchanged after fusion, i.e., when the spectral information is translated from the coarse scale of the MS data to the fine scale of the PAN image. Experimental results, carried out on very high-resolution Ikonos data and simulated Pléiades data, demonstrate that the results provided by the proposed approach are consistent and in trend with analysis performed on spatially degraded data. However, the proposed method requires no reference originals and is therefore usable in all practical cases.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2008/00000074/00000002/art00004;jsessionid=wrtfev7ivfpl.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Anomaly Detection in Hyperspectral Imagery by Fuzzy Integral Fusion of Band-subsets
        </a>
    </h3>
    <div style="font-style: italic;">200802, pp. 201-213(13)</div>
    <div>Authors: Di, Wei; Pan, Quan; He, Lin; Cheng, Yongmei</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Anomaly detection in hyperspectral imagery is gaining increasing interest. However, most covariance matrix-based detectors are applied directly to all the hyperspectral data bands without considering the spectral variation and their possible different contribution to detection. Besides, the limited sample number as compared with the high dimensionality of the data can lead to the imprecise estimation of the covariance matrix and even the singularity problem. In this paper, a band-subset fuzzy integral fusion (BS-FI) detection method is presented to solve these problems. The complete set of hyperspectral data bands is first partitioned into several lower dimensional band-subsets, whose detection results are obtained separately and finally merged by a fuzzy integral fusion method. We adopt a non-parametric fuzzy support function which can utilize more statistical information and avoid the model discrepancy that might be brought in by a fixed distribution model. In addition, the fuzzy density is assigned by the ratio between the target signal and noise, which is the key to the target detection problem through an adaptive eigenvalue-based approach. In the experiments on real OMIS-I hyperspectral imagery, the proposed method outperforms the RX detector both on the complete set of bands and on each band-subset, and other band-subset fusion detectors.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2008/00000074/00000002/art00005;jsessionid=wrtfev7ivfpl.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Fusion of Lidar and Imagery for Reliable Building Extraction
        </a>
    </h3>
    <div style="font-style: italic;">200802, pp. 215-225(11)</div>
    <div>Authors: Lee, Dong Hyuk; Lee, Kyoung Mu; Lee, Sang Uk</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                We propose a new building detection and description algorithm for lidar data and photogrammetric imagery using directional histograms, splitting and merging segments, and line segments matching. Our algorithm consists of three steps. In the first step, we extract initial building regions from lidar data. Here, we apply a modified local maxima technique coupled with directional histograms and the entropies of these histograms. In the second step, given the color segmentation results from the photogrammetric imagery, we extract coarse building boundaries based on the lidar results with region segmentation and merging from aerial imagery. In the third step, we extract precise building boundaries based on the coarse building boundaries using line segments matching and perceptual grouping. Experimental results on multi-sensor data demonstrate that the proposed algorithm produces accurate and reliable results.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2008/00000074/00000002/art00006;jsessionid=wrtfev7ivfpl.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Extracting Urban Road Networks from High-resolution True Orthoimage and Lidar
        </a>
    </h3>
    <div style="font-style: italic;">200802, pp. 227-237(11)</div>
    <div>Authors: Youn, Junhee; Bethel, James S.; Mikhail, Edward M.; Lee, Changno</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Automated or semi-automated feature extraction from remotely collected, large scale image data has been a challenging issue in digital photogrammetry for many years. In the feature extraction field, fusing different types of data to provide complementary information about the objects is becoming increasingly important. In this paper, we present a newly developed approach for the automatic extraction of urban area road networks from a true orthoimage and lidar assuming the road network to be a semi-grid pattern. The proposed approach starts from the subdivision of a study area into small regions based on homogeneity of the dominant road directions from the true orthoimage. Each region’s road candidates are selected with a proposed free passage measure. This process is called the “acupuncture” method. Features around the road candidates are used as key factors for an advanced “acupuncture method” called the region-based acupuncture method. Extracted road candidates are edited to avoid collocation with non-road features such as buildings and grass fields. In order to produce a building map for the prior step, a first-last return analysis and morphological filter are used with the lidar point cloud. A grass area thematic map is generated by supervised classification techniques from a synthetic image, which contains the three color bands from the true orthoimage and the lidar intensity value. Those non-road feature maps are used as a blocking mask for the roads. The accuracy of the result is evaluated quantitatively with respect to manually compiled road vectors, and a completeness of 80 percent and a correctness of 79 percent are obtained with the proposed algorithm on an area of 1,081,600 square meters.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2008/00000074/00000002/art00007;jsessionid=wrtfev7ivfpl.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Multisource Classification Using Support Vector Machines
        </a>
    </h3>
    <div style="font-style: italic;">200802, pp. 239-246(8)</div>
    <div>Authors: Watanachaturaporn, Pakorn; Arora, Manoj K.; Varshney, Pramod K.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Remote sensing image classification has proven to be attractive for extracting useful thematic information such as land-cover. However, often for a given application, spectral information acquired by a remote sensing sensor may not be sufficient to derive accurate information. Incorporation of data from other sources such as a digital elevation model (DEM), and geophysical and geological data may assist in achieving more accurate land-cover classification from remote sensing images. Recently, support vector machines (SVM) have been proposed as an alternative for classification of remote sensing data, and the results are promising. In this paper, we employ the SVM algorithm to perform multi-source classification. An IRS–1C LISS III image along with normalized differenced vegetation index (NDVI) image andDEMare used to produce a land-cover classification for a region in the Himalayas. The accuracy of SVM-based multi-source classification is compared with several other nonparametric algorithms namely a decision tree classifier, and back propagation and radial basis function neural network classifiers. The well-known kappa coefficient of agreement is used to assess classification accuracy. The differences in the kappa coefficient of classifiers have been statistically evaluated using a pairwise Z-test. The results show a significant increase in the accuracy of the SVM based classifier on incorporation of ancillary data over classification performed solely on the basis of spectral data from remote sensing sensors.
            </details>
        </div>
</article>
<article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2008/00000074/00000002/art00008;jsessionid=wrtfev7ivfpl.x-ic-live-01" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Mapping Vegetation Communities Using Statistical Data Fusion in the Ozark National Scenic Riverways, Missouri, USA
        </a>
    </h3>
    <div style="font-style: italic;">200802, pp. 247-264(18)</div>
    <div>Authors: Chastain, Robert A.; Struckhoff, Matthew A.; He, Hong S.; Larsen, David R.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A vegetation community map was produced for the Ozark National Scenic Riverways consistent with the association level of the National Vegetation Classification System. Vegetation communities were differentiated using a large array of variables derived from remote sensing and topographic data, which were fused into independent mathematical functions using a discriminant analysis classification approach. Remote sensing data provided variables that discriminated vegetation communities based on differences in color, spectral reflectance, greenness, brightness, and texture. Topographic data facilitated differentiation of vegetation communities based on indirect gradients (e.g., landform position, slope, aspect), which relate to variations in resource and disturbance gradients. Variables derived from these data sources represent both actual and potential vegetation community patterns on the landscape. A hybrid combination of statistical and photointerpretation methods was used to obtain an overall accuracy of 63 percent for a map with 49 vegetation community and land-cover classes, and 78 percent for a 33-class map of the study area.
            </details>
        </div>
</article>
