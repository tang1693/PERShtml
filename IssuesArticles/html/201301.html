
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 01 - Year 2013</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 01 - Year 2013</h1>
            <p>Displaying articles from Issue 01 - Year 2013.</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000001/art00001;jsessionid=ryzut6qgslyq.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Spatio-statistical Predictions of Vernal Pool Locations in Massachusetts
        </a>
    </h3>
    <div style="font-style: italic;">201301, nan</div>
    <div>Authors: Cormier, Tina A.; Congalton, Russell G.; Babbitt, Kimberly J.</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Vernal pools are small, isolated, depressions that experience cyclical periods of inundation and drying. Many species have evolved strategies to utilize the unique characteristics of vernal pools; however, their small size, seasonal nature, and isolation from other, larger water bodies, suggest increased risk of damage or loss by development. The objectives of this research were to statistically determine physical predictors of vernal pool presence, and subsequently, to represent the output cartographically for use as a conservation tool. Logistic regression and Classification and Regression Tree (CART) methods were used to identify important predictors of 405 known vernal pools across northeastern Massachusetts. The CART models performed most favorably, achieving map accuracies as high as 97 percent and providing a set of rules for vernal pool prediction. It is important to note that we observed significant discrepancies between model accuracy and map accuracy, illustrating the pitfall of relying on statistical metrics alone (e.g., R2values) to assess the quality of spatial analyses.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000001/art00002;jsessionid=ryzut6qgslyq.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Predicting Surface Fuel Models and Fuel Metrics Using Lidar and CIR Imagery in a Dense, Mountainous Forest
        </a>
    </h3>
    <div style="font-style: italic;">201301, pp. 37-49(13)</div>
    <div>Authors: Jakubowksi, Marek K.; Guo, Qinghua; Collins, Brandon; Stephens, Scott; Kelly, Maggi</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                We compared the ability of several classification and regression algorithms to predict forest stand structure metrics and standard surface fuel models. Our study area spans a dense, topographically complex Sierra Nevada mixed-conifer forest. We used clustering, regression trees, and support vector machine algorithms to analyze high density (average 9 pulses/m2), discrete return, small-footprint lidar data, along with multispectral imagery. Stand structure metric predictions generally decreased with increased canopy penetration. For example, from the top of canopy, we predicted canopy height (r2=0.87), canopy cover (r2=0.83), basal area (r2=0.82), shrub cover (r2=0.62), shrub height (r2=0.59), combined fuel loads (r2=0.48), and fuel bed depth (r2=0.35). While the general fuel types were predicted accurately, specific surface fuel model predictions were poor (76 percent and<50 percent correct classification, respectively) using all algorithms. These fuel components are critical inputs for wildfire behavior modeling, which ultimately support forest management decisions. This comprehensive examination of the relative utility of lidar and optical imagery will be useful for forest science and management.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000001/art00003;jsessionid=ryzut6qgslyq.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Improved Nonsubsampled Contourlet Transform for Multi-sensor Image Registration
        </a>
    </h3>
    <div style="font-style: italic;">201301, pp. 51-66(16)</div>
    <div>Authors: Wang, Ruirui; Ma, Jianwen; Huang, Huaguo; Shi, Wei</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Homologous feature point extraction is a key problem in the multi-sensor image registration. In this paper, a new feature point extraction method using nonsubsampled contourlet transform (NSCT) and an adaptive shrink operator (ASO_NSCT) for multi-sensor image registration is proposed. Moreover, this proposed feature matching is different from the traditional feature matching strategies and is performed using a similarity measure computed from neighborhood circles in low-frequency bands. Then, a number of reliable matched couples with even distributions are obtained, which assures the accuracy of the registration. Applications of the proposed algorithm to different optical images as well as optical and synthetic aperture radar images show that, in each case, a large number of accurate matched couples could be identified. Additionally, the RMSE patterns are analyzed and comparisons of the parameters are carried out between the registration models and the actual ground structures, which further demonstrates the effectiveness of the proposed algorithm.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000001/art00004;jsessionid=ryzut6qgslyq.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Modeling Image Motion in Airborne Three-Line-Array (TLA) Push-broom Cameras
        </a>
    </h3>
    <div style="font-style: italic;">201301, pp. 67-78(12)</div>
    <div>Authors: Jia, Guimin; Wang, Xiangjun; Wei, Hong; Zhang, Zhaocai</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper presents an image motion model for airborne three-line-array (TLA) push-broom cameras. Both aircraft velocity and attitude instability are taken into account in modeling image motion. Effects of aircraft pitch, roll, and yaw on image motion are analyzed based on geometric relations in designated coordinate systems. The image motion is mathematically modeled by image motion velocity multiplied by exposure time. Quantitative analysis to image motion velocity is then conducted in simulation experiments. The results have shown that image motion caused by aircraft velocity is space invariant while image motion caused by aircraft attitude instability is more complicated. Pitch, roll, and yaw all contribute to image motion to different extents. Pitch dominates the along-track image motion and both roll and yaw greatly contribute to the cross-track image motion. These results provide a valuable base for image motion compensation to ensure high accuracy imagery in aerial photogrammetry.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000001/art00005;jsessionid=ryzut6qgslyq.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Using Optical Projection in Close-range Photogrammetry for 6DOF Sensor Positioning
        </a>
    </h3>
    <div style="font-style: italic;">201301, pp. 79-86(8)</div>
    <div>Authors: Zheng, Benrui; Dong, Yue; Davies, Angela; Mullany, Brigid; Morse, Edward</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                A novel, low cost, non-contact, six degrees of freedom (DOF) measurement technique is proposed that enables real-time measurement of a small lightweight module’s location. Straight forward applications of the proposed technique include robot calibration by installing the module to the end effector of a robot arm, and head-tracking in a typical virtual reality environment by attaching the module to a human head. The technique is based on a combination of photogrammetry and optical pattern projection. The module generates an optical pattern that is observable on the surrounding walls, and photogrammetry is used to measure the absolute coordinates of features in the projected optical pattern with respect to a defined global coordinate system. By combining these absolute coordinates with the known angular information of the optical projection beams, a minimization algorithm can be used to extract the absolute coordinates and angular orientation of the module itself. Experimental agreement of 1 to 5 parts in 103was obtained by translating the module over 0.9 m and by rotating it through 60°. Numerical simulations were conducted to demonstrate that optimum design of the projected pattern gives a lower associated measurement uncertainty than is possible by direct photogrammetric measurement with traditional tie points. This paper documents the proof of principle and describes how the measurement can be further improved.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2013/00000079/00000001/art00006;jsessionid=ryzut6qgslyq.x-ic-live-01" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            An Automatic Approach to UAV Flight Planning and Control for Photogrammetric Applications
        </a>
    </h3>
    <div style="font-style: italic;">201301, pp. 87-98(12)</div>
    <div>Authors: Hernandez-Lopez, David; Felipe-Garcia, Beatriz; Gonzalez-Aguilera, Diego; Arias-Perez, Benjamin</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This paper deals with the implementation of an automatic and novel approach for UAV flight planning and control based on photogrammetric principles. Software which performs multiple tasks related to aerial photogrammetry and particularized to UAV systems was developed. Specifically, the flight planning and control framework incorporates a robust geometric control that guarantees the effectiveness, precision, and reliability of image acquisition. A real case study was generated to test and validate the flight planning and control strategies developed here.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>Generated automatically for Issue 01 - Year 2013</p>
        </footer>
    </body>
    </html>
    