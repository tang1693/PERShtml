
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 09 - Year 2025</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 09 - Year 2025</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000009" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000009/art00008;jsessionid=7089s3elc130c.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Crack Parameter Determination Based on Multiple-Window Matching Strategy and Crack Grid Geometry Analysis
        </a>
    </h3>
    <div style="font-style: italic;">202509, pp. 551-562(12)</div>
    <div>Authors: Tong, Xiaohua; Gao, Sa; Ye, Zhen; Chen, Peng; Liu, Shijie; Xie, Huan; Liu, Xianglei; Hong, Zhonghua; Zhang, Dezhi; Yang, Jun; Song, Yanfeng; Lv, Hongpeng; Cao, Dong</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/09/Crack Parameter Determination Based on Multiple-Window Matching Strategy and Crack Grid Geometry Analysis.jpg" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Accurate deformation measurement of building materials has consistently been a focal point of research within the field of material testing. This paper proposes a robust 3D crack parameter estimation approach to measure the crack propagation characteristics of concrete pillars under compression testing. Through the use of an improved multiple-window matching strategy and other advanced image processing algorithms, the precise positional information of speckle target points can be calculated in speckle image sequences. 3D point cloud data and full-field deformation of target points on the concrete pillar surface can be further calculated through photogrammetric analysis and spatiotemporal analysis, respectively. Finally, a robust crack estimation algorithm based on grid geometry analysis is proposed to extract accurate crack parameters in the presence of complex speckle texture interference. To verify the superiority and reliability of the proposed approach, both a simulation test and an empirical test were conducted. The experimental results, including positional comparisons with previous matching strategies and displacement comparisons with third-party equipment, corroborate the effectiveness of our method.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000009/art00009;jsessionid=7089s3elc130c.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            LFSA‐YOLOv7: A Novel Method for Ship Detection in Remote Sensing Images with Complex Backgrounds
        </a>
    </h3>
    <div style="font-style: italic;">202509, pp. 563-571(9)</div>
    <div>Authors: Gu, Heng; Li, Wei; Zhang, Linlin; Meng, Qingyan; Wei, Lianhuan; Wu, Hantian; Ma, Jian</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/09/LFSA‐YOLOv7 A Novel Method for Ship Detection in Remote Sensing Images with Complex Backgrounds.jpg" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Ship target detection is a critical component for marine environmental monitoring and the protection of maritime rights and interests. However, existing methods for ship detection in remote sensing imagery often face challenges such as false positives and missed detections, particularly in the presence of complex backgrounds and multi-scale targets. To address these issues, we propose a novel method called LFSA‐YOLOv7, (LDConv {Linear Deformable Convolution}; SPPFCSPC‐G {Spatial Pyramid Pooling Fast Cross Stage Partial Channel ‐ Group}, SimAM{Simple Attention Module}, and Alpha‐CoIoU {Alpha‐Coordinate Point Orienta tion ‐ Intersection over Union}) which is designed to enhance the accuracy and robustness of ship detection in complex backgrounds: (a) a novel network architecture (LDConv + SPPFCSPC‐G + SimAM) to enhance the model’s ship detection capabilities, and (b) a novel Alpha‐CoIoU loss function to improve the model’s ship localization accuracy. To evaluate the performance of our algorithm in complex backgrounds, we have developed a novel ship classification detection (SCD) data set and conducted comprehensive experiments. Furthermore, we have validated the generalization of our algorithm across various remote sensing ship data sets. The experimental results demonstrate that our algorithm outperforms previous techniques, exhibiting strong generalization and robustness, making it effectively suitable for ship detection in complex backgrounds. The SCD data set and code will be available at https://github.com/wionn/LFSA-YOLOv7.git.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000009/art00011;jsessionid=7089s3elc130c.x-ic-live-03" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Lightweight and Accurate Video Synthetic Aperture Radar Target Detection Network
        </a>
    </h3>
    <div style="font-style: italic;">202509, pp. 573-581(9)</div>
    <div>Authors: Ma, Huilian; Li, Yinwei; Li, Weisong; Zhu, Yiming</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/09/Lightweight and Accurate Video Synthetic Aperture Radar Target Detection Network.jpg" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Video synthetic aperture radar (SAR) target detection has seen rapid development in recent years, but current methods generally suffer from high computational complexity as well as frequent false alarms and missed alarms. Additionally, in video SAR scenarios, small targets become more difficult to identify due to low resolution and complex backgrounds. Recently, leveraging shadows for video SAR moving target detection has proven advantageous as it provides accurate location information and additional boundary details. To address the challenges of target detection in video SAR caused by low resolution and complex backgrounds, as well as the high computational cost of current methods, this paper proposes a lightweight algorithm for video SAR shadow detection based on the YOLOv5 architecture, named Lavs-DeNet. First, a lightweight feature extraction backbone network combining both global and local information called CGLLNet is proposed. This backbone improves the detection performance by mitigating false alarms and missed alarms caused by defocus in video SAR images. Next, a slanted ladder bidirectional feature pyramid network (SL‐BiFPN) is designed. By using tilted upsampling with multiple stacked layers, this network efficiently extracts multi-scale features, further reducing missed alarms. Finally, a lightweight information interaction module (LII‐C3) has been developed, significantly reducing computational complexity. To validate the effectiveness of the proposed method, experiments were conducted using the data set released by Sandia National Laboratories. The experimental results show that the proposed Lavs-DeNet can achieve 97.88% detection accuracy, requiring only 21.02G floating points and 0.88M parameters, which is superior to the current classical video SAR target detection networks.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 09 - Year 2025</p>
        </footer>
    </body>
    </html>
    