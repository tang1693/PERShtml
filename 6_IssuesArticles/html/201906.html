
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 06 - Year 2019</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 06 - Year 2019</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000006" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000006/art00009;jsessionid=2hfjhc01ja924.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            CNN-Based Dense Image Matching for Aerial Remote Sensing Images
        </a>
    </h3>
    <div style="font-style: italic;">201906, pp. 415-424(10)</div>
    <div>Authors: Ji, Shunping; Liu, Jin; Lu, Meng</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Dense stereo matching plays a key role in 3D reconstruction. The capability of using deep learning in the stereo matching of remote sensing data is currently uncertain. This article investigated the application of deep learning–based stereo methods in aerial image series and proposed a deep learning–based multi-view dense matching framework. First, we applied three typical convolutional neural network models, MC-CNN, GC-Net, and DispNet, to aerial stereo pairs and compared the results with those of the SGM and a commercial software, SURE. Second, on different data sets, the generalization ability of each network is evaluated by using direct transfer learning with models pretrained on other data sets and by fine-tuning with a small number of target training data. Third, we present a deep learning–based multi-view dense matching framework where the multi-view geometry is introduced to further refine matching results. Three sets of aerial images as the main data sets and two open-source sets of street images as auxiliary data sets are used for testing. Experiments show that, first, the performance of deep learning–based stereo methods is slightly better than traditional methods. Second, both the GC-Net and the MC-CNN have demonstrated good generalization ability and can obtain satisfactory results on aerial images using a pretrained model on several available stereo benchmarks. Third, multi-view geometry constraints can further improve the performance of deep learning–based methods, which is better than that of the multi-view–based SGM and SURE.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000006/art00010;jsessionid=2hfjhc01ja924.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Semantic Façade Segmentation from Airborne Oblique Images
        </a>
    </h3>
    <div style="font-style: italic;">201906, pp. 425-433(9)</div>
    <div>Authors: Lin, Yaping; Nex, Francesco; Yang, Michael Ying</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                In this paper, oblique airborne images with very high resolution are used to address the problem from aerial views in urban areas. Traditional classification method (i.e., random forests) is compared with state-of-the-art fully convolutional networks (FCNs). Random forests use hand-craft image features including red, green, blue (RGB), scale-invariant feature transform (SIFT), and Texton, and point cloud features consisting of normal vector and planarity extracted from different scales. In contrast, the inputs of FCNs are the RGB bands and the third components of normal vectors. In both cases, three-dimensional (3D) features are projected back into the image space to support the facade interpretation. Fully connected conditional random field (CRF) is finally taken as a post-processing of the FCN to refine the segmentation results. Several tests have been performed and the achieved results show that the models embedding the 3D component outperform the solution using only images. FCNs significantly outperformed random forests, especially for the balcony delineation.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000006/art00011;jsessionid=2hfjhc01ja924.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            RoofN3D: A Database for 3D Building Reconstruction with Deep Learning
        </a>
    </h3>
    <div style="font-style: italic;">201906, pp. 435-443(9)</div>
    <div>Authors: Wichmann, Andreas; Agoub, Amgad; Schmidt, Valentina; Kada, Martin</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Machine learning methods, in particular those based on deep learning, have gained in importance through the latest development of artificial intelligence and computer hardware. However, the direct application of deep learning methods to improve the results of 3D building reconstruction is often not possible due, for example, to the lack of suitable training data. To address this issue, we present RoofN3D which provides a three-dimensional (3D) point cloud training dataset that can be used to train machine learning models for different tasks in the context of 3D building reconstruction. The details about RoofN3D and the developed framework to automatically derive such training data are described in this paper. Furthermore, we provide an overview of other available 3D point cloud training data and approaches from current literature in which solutions for the application of deep learning to 3D point cloud data are presented. Finally, we exemplarily demonstrate how the provided data can be used to classify building roofs with the PointNet framework.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000006/art00012;jsessionid=2hfjhc01ja924.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Through-Water Dense Image Matching for Shallow Water Bathymetry
        </a>
    </h3>
    <div style="font-style: italic;">201906, pp. 445-455(11)</div>
    <div>Authors: Mandlburger, Gottfried</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                The introduction of Dense Image Matching (DIM) has reactivated the interest in photogrammetric surface mapping, as it allows the derivation of Digital Elevation Models with a spatial resolution in the range of the ground sampling distance of the aerial images. While the primary field of application is wide-area mapping of topography and urban scenes, charting bathymetry of clear and shallow water areas is equally feasible via application of multimedia photogrammetry. The article specifically investigates the potential of through-water DIM for high resolution mapping of generally low textured shallow water areas using modern techniques like semiglobal matching and off-the-shelf software. In a case study, the DIM-derived underwater surfaces of coastal and inland water bodies are compared to concurrently acquired laser bathymetry data. With an achieved penetration depth of more than 5 m and deviations in the dm-range compared to the laser data as reference, the results confirm the general feasibility of through-water DIM. However, sufficient bottom texture and favorable environmental conditions are a precondition for achieving accurate results.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2019/00000085/00000006/art00013;jsessionid=2hfjhc01ja924.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Object-Based Point Cloud Analysis for Landslide and Erosion Monitoring
        </a>
    </h3>
    <div style="font-style: italic;">201906, pp. 455-462(8)</div>
    <div>Authors: Mayr, Andreas; Rutzinger, Martin; Geitner, Clemens</div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Today point clouds from close-range sensing are used for operational erosion and landslide monitoring. Distances between points from multi-temporal acquisitions can indicate surface deformation, while a designation of the underlying geomorphological processes is often handicapped by complex terrain structures and vegetation. We present an approach to landslide monitoring that integrates semantic information and three-dimensional deformation detection automatically. Surface changes are assigned to (i) semantic object classes (landslide scarp, eroded area, deposit) and (ii) to spatially contiguous, individual objects (like parts of the landslide scarp and moving clods of turf and soil). We demonstrate this object-based approach with a time series of 13 topographic Light Detection and Ranging point clouds, covering a site affected by shallow landsliding. The results of this case study illustrate how the presented methods translate the unstructured point clouds into information on geomorphological process dynamics to support erosion and landslide assessment.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 06 - Year 2019</p>
        </footer>
    </body>
    </html>
    