
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Issue 03 - Year 2025</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 0;
                background-color: #f9f9f9;
                color: #333;
            }
            header {
                background-color: #1b5faa;
                color: white;
                padding: 20px;
                text-align: center;
            }
            article {
                background-color: #fff;
                margin: 20px auto;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 5px;
                max-width: 800px;
            }
            h1 {
                font-size: 1.8em;
                margin-bottom: 0.5em;
            }
            h3 {
                font-size: 1.4em;
                margin: 10px 0;
            }
            .separator {
                border-bottom: 1px solid #ddd;
                margin: 20px 0;
            }
            footer {
                text-align: center;
                margin-top: 40px;
                font-size: 0.9em;
                color: #666;
            }
            .ga-image img {
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
                margin: 10px 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>Issue 03 - Year 2025</h1>
            <p><a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000003" target="_blank" style="color: white;">View Full Issue</a></p>
            <p>Photogrammetric Engineering and Remote Sensing</p>
        </header>
        <main>
    <article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000003/art00010;jsessionid=jm2wajzzvth3.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Accuracy Assessment of Dense Point Cloud Generated by Deep Learning and Semiglobal Matching
        </a>
    </h3>
    <div style="font-style: italic;">202503, pp. 153-162(10)</div>
    <div>Authors: Sadeq, Haval Abduljabbar</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/03/Accuracy Assessment of Dense Point Cloud Generated by Deep Learning and Semi-Global Matching.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                This study assesses two techniques for generating point clouds based on dense image matching (DIM): semiglobal matching (SGM) implemented in Trimble INPHO MATCH-3DX software, and a deep-learning algorithm Pyramid Stereo Matching Network (PSMNet). The PSMNet was trained using three datasets with both automated driving and aerial scenes. Two other distinctive sites were selected to assess the accuracy against LiDAR data. The study found inaccuracies in the PSMNet point clouds and suggested that SGM could potentially result in a better outcome. However, for flat slab or ground surface, its root-mean-square error is better than SGM. The analysis showed that the SGM analyses favorably remove points on vertical surfaces due to occlusion, while PSMNet incorrectly extrapolate them with slopes. Furthermore, the assessment identified the potential to improve PSMNet by using more or in-distribution training dataset for test (unseen) areas.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles <span style="color: rgb(0, 191, 255);">Open Access</span></div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000003/art00011;jsessionid=jm2wajzzvth3.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            A Comparative Study of Deep Learning Methods for Automated Road Network Extraction from High-Spatial-Resolution Remotely Sensed Imagery
        </a>
    </h3>
    <div style="font-style: italic;">202503, pp. 163-174(12)</div>
    <div>Authors: Zhou, Haochen; He, Hongjie; Xu, Linlin; Ma, Lingfei; Zhang, Dedong; Chen, Nan; Chapman, Michael A.; Li, Jonathan</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/03/A Comparative study of deep learning methods for automated road network extraction from high spatial resolution remotely sensed imagery.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Road network data are crucial for various applications, such as road network planning, traffic control, map navigation, autonomous driving, and smart city construction. Automated road network extraction from high-spatial-resolution remotely sensed imagery has shown promise in road network data construction. In recent years, the advent of deep learning algorithms has pushed road network extraction towards auto - mation, achieving very high accuracy. However, the latest deep learning models are often less applied in the field of road network extraction and lack comparative experiments for guidance. Therefore, this research selected three recent deep learning algorithms, including dense prediction transformer (DPT), SegFormer, SEgmentation TRansformer (SETR), and the classic model fully convolutional network-8s (FCN-8s) for a comparative study. Additionally, this research paper compares three different decoder structures within the SETR model (SETR_naive, SETR_mla, SETR_pup) to investigate the effect of different decoders on the road network extraction task. The experiment is conducted on three commonly used datasets: the DeepGlobe Dataset, the Massachusetts Dataset, and Road Datasets in Complex Mountain Environments (RDCME). The DPT model outperforms other models on the Massachusetts dataset with superior reliability, achieving a high accuracy of 96.31% and excelling with a precision of 81.78% and recall of 32.50%, leading to an F1 score of 46.51%. While SegFormer has a slightly higher F1 score, DPT's precision is particularly valuable for minimizing false positives, making it the most balanced and reliable choice. Similarly, for the DeepGlobe Dataset, DPT achieves an accuracy of 96.76%, precision of 66.12%, recall of 41.37%, and F1 score of 50.89%, and for RDCME, DPT achieves an accuracy of 98.94%, precision of 99.07%, recall of 99.84%, and F1 score of 99.46%, confirming its consistent performance across datasets. This paper provides valuable guidance for future studies on road network extraction techniques using deep learning algorithms.
            </details>
        </div>
</article>
<div class="separator"></div><article style="padding: 15px;">
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <div style="font-weight: bold; color: gray;">Research Articles </div>
    </div>
    <h3 style="margin: 5px 0;">
        <a href="https://www.ingentaconnect.com/contentone/asprs/pers/2025/00000091/00000003/art00013;jsessionid=jm2wajzzvth3.x-ic-live-02" target="_blank" rel="noreferrer" style="text-decoration: none; color: #1b5faa;">
            Real-time Vanishing Point Tracking in Manhattan World Using Improved BaySAC
        </a>
    </h3>
    <div style="font-style: italic;">202503, pp. 175-186(12)</div>
    <div>Authors: Ye, Chenming; Kang, Zhizhong; Cai, Jinhao; Zhu, Longze</div>
    <div class="ga-image">
        <img src="https://raw.githubusercontent.com/tang1693/PERShtml/refs/heads/main/IssuesArticles/html/img/2025/03/Real-time Vanishing Point Tracking in Manhattan World Using Improved BaySAC.png" alt="Graphical Abstract">
    </div>
    <div>
            Abstract: 
            <details>
                <summary style="color: #1b5faa;">Read more...</summary>
                Vanishing points provide geometric information about a scene and assist in camera calibration, scene understanding, and 3D reconstruction. The random sample consensus (RANSAC) algorithm faces challenges of low efficiency and insufficient robustness. With prior information, the Bayesian sample consensus (BaySAC) algorithm can efficiently derive the correct parameters and compensate for the deficiencies of the RANSAC algorithm. This study proposes an improved BaySAC vanish - ing point detection algorithm, which uses a linear grouping strategy to enhance the distribution of inliers across groups and accelerate convergence. In the continuous frame vanishing point tracking problem, the detection result of the previous frame is employed as a priori infor - mation for the subsequent frames, facilitating efficient convergence for vanishing point detection and tracking in videos. Experimental results on both benchmark datasets and real-world image sets demonstrate that the proposed method achieves remarkable accuracy and efficiency, enabling real-time performance for vanishing point detection and track- ing. The code is available at https://github. com/CHEMYE/BaySAC.
            </details>
        </div>
</article>
<div class="separator"></div>
        </main>
        <footer>
            <p>PE&RS Issue 03 - Year 2025</p>
        </footer>
    </body>
    </html>
    